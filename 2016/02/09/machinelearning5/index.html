<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>机器学习笔记（Chapter 05 - Logistic回归） | Forec&#39;s Notes</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Logistic回归根据现有数据对边界回归线建立回归公式，以此进行分类。训练分类器时的做法就是寻找最佳拟合参数，使用的是最优化算法。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记（Chapter 05 - Logistic回归）">
<meta property="og:url" content="http://forec.github.io/2016/02/09/machinelearning5/index.html">
<meta property="og:site_name" content="Forec's Notes">
<meta property="og:description" content="Logistic回归根据现有数据对边界回归线建立回归公式，以此进行分类。训练分类器时的做法就是寻找最佳拟合参数，使用的是最优化算法。">
<meta property="og:image" content="http://7xktmz.com1.z0.glb.clouddn.com/logistic%E5%9B%9E%E5%BD%92-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87.png">
<meta property="og:image" content="http://7xktmz.com1.z0.glb.clouddn.com/Logistic%E5%9B%9E%E5%BD%92-plotGD-%E7%AD%89%E9%AB%98%E7%BA%BF.png">
<meta property="og:image" content="http://7xktmz.com1.z0.glb.clouddn.com/Logistic%E5%9B%9E%E5%BD%92-plotSDerror-%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B.png">
<meta property="og:image" content="http://7xktmz.com1.z0.glb.clouddn.com/logistic%E5%9B%9E%E5%BD%92-sigmoid%E5%87%BD%E6%95%B0.png">
<meta property="og:updated_time" content="2016-10-03T15:37:18.638Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习笔记（Chapter 05 - Logistic回归）">
<meta name="twitter:description" content="Logistic回归根据现有数据对边界回归线建立回归公式，以此进行分类。训练分类器时的做法就是寻找最佳拟合参数，使用的是最优化算法。">
  
    <link rel="alternative" href="/atom.xml" title="Forec&#39;s Notes" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Forec&#39;s Notes</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://forec.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-machinelearning5" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/02/09/machinelearning5/" class="article-date">
  <time datetime="2016-02-09T10:57:05.000Z" itemprop="datePublished">2016-02-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      机器学习笔记（Chapter 05 - Logistic回归）
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>Logistic回归根据现有数据对边界回归线建立回归公式，以此进行分类。训练分类器时的做法就是寻找最佳拟合参数，使用的是最优化算法。</p>
</blockquote>
<a id="more"></a>
<h1 id="Logistic回归和Sigmoid函数"><a href="#Logistic回归和Sigmoid函数" class="headerlink" title="Logistic回归和Sigmoid函数"></a>Logistic回归和Sigmoid函数</h1><ul>
<li>Logistic回归过程<ul>
<li>准备数据：需要进行距离运算，数据类型为数值型，结构化数据格式最佳。</li>
<li>分析数据：任意方法。</li>
<li>训练算法：大部分时间用于训练，训练目的为了找到最佳的分类回归系统。</li>
<li>测试算法：训练步骤完成后分类将会很快。</li>
<li>使用算法：输入数据并将其转换为对应的结构化数值，之后基于训练好的回归系数可以对这些数值进行简单的回归计算，判定其属于哪个类别。</li>
</ul>
</li>
<li>Logistic回归优缺点<ul>
<li>优点：计算代价不高，易于理解和实现</li>
<li>缺点：容易欠拟合，分类精度不高</li>
<li>使用数据类型：数值型和标称型</li>
</ul>
</li>
<li>Sigmoid函数：是近似海维塞德阶跃函数（单位阶跃函数），<code>σ(z)=1/(1+e^(-z))</code>。当x为0时，Sigmoid(0)=0.5，随着x的增大减小，σ(x)将逼近1和0。当横坐标刻度足够大，Sigmoid看起来类似阶跃函数。我们将输入数据的每个特征乘以对应的回归系数，得到的结果相加，作为Sigmoid函数的参数，得到一个范围在0-1之间的数值，若大于0.5则归入1，小于0.5则归入0。因此Logistic回归可以被看成概率估计。</li>
</ul>
<h1 id="最佳回归系数确定"><a href="#最佳回归系数确定" class="headerlink" title="最佳回归系数确定"></a>最佳回归系数确定</h1><ul>
<li>梯度上升法与梯度下降法类似，梯度上升算法用来求函数的最大值，梯度下降算法用来求函数的最小值。思想为要找到某函数的最大值，则沿着该函数的梯度方向探寻。梯度上升法到达每个点后会重新估计移动方向，循环迭代直至满足停止条件。对于线性回归系数，初始状态均为1，每次迭代的计算公式为<code>w:=w+α▽f(w)</code>，▽f(w)是在w处的梯度，α是沿梯度方向移动量大小，记为步长。该公式一直迭代执行，直到停止条件，比如迭代次数达到某个指定值，或误差达到指定精度。</li>
<li><p>使用梯度上升找到最佳参数，R为迭代次数，流程如下：</p>
<ul>
<li>每个回归系数初始化为1</li>
<li>重复以下步骤R次：计算整个数据集的梯度，使用alpha*gradient更新回归系数的向量，返回回归系数</li>
</ul>
</li>
<li><p>Code - gradAscent - logRegres.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(inX)</span>:</span>	<span class="comment"># the array from numpy can be used as a single parameter</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1</span>+exp(-inX))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span><span class="params">(dataMatIn, classLabels)</span>:</span></span><br><span class="line">	dataMatrix = mat(dataMatIn)    <span class="comment"># m*n</span></span><br><span class="line">	labelMat = mat(classLabels).transpose()    <span class="comment"># m*1</span></span><br><span class="line">	m, n = shape(dataMatrix)</span><br><span class="line">	alpha = <span class="number">0.001</span></span><br><span class="line">	maxCycles = <span class="number">500</span></span><br><span class="line">	weights = ones((n,<span class="number">1</span>))    <span class="comment"># n*1</span></span><br><span class="line">	<span class="keyword">for</span> k <span class="keyword">in</span> range(maxCycles):</span><br><span class="line">		h = sigmoid(dataMatrix*weights)	<span class="comment"># m*1</span></span><br><span class="line">		error = (labelMat - h)	<span class="comment"># counting error direction, m*1</span></span><br><span class="line">		weights = weights + alpha * dataMatrix.transpose() * error</span><br><span class="line">	<span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>
</li>
<li><p>Code - loadDataSet - logRegres.py ，loadDataSet函数导入testSet.txt，返回数据矩阵和标签。gradAscent接收数据矩阵和标签，并返回生成的回归系数向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span></span><br><span class="line">	dataMat = []; labelMat = []</span><br><span class="line">	fr = open(<span class="string">'testSet.txt'</span>)</span><br><span class="line">	<span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">		lineArr = line.strip().split()</span><br><span class="line">		dataMat.append([<span class="number">1.0</span>, float(lineArr[<span class="number">0</span>]), float(lineArr[<span class="number">1</span>])])</span><br><span class="line">		labelMat.append(int(lineArr[<span class="number">2</span>]))</span><br><span class="line">	<span class="keyword">return</span> dataMat, labelMat</span><br></pre></td></tr></table></figure>
</li>
<li><p>Code - plotBestFit - logRegres.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotBestFit</span><span class="params">(weights)</span>:</span></span><br><span class="line">	<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">	dataMat, labelMat = loadDataSet()</span><br><span class="line">	dataArr = array(dataMat)</span><br><span class="line">	n = shape(dataArr)[<span class="number">0</span>]</span><br><span class="line">	xcord1 = []; ycord1 = []</span><br><span class="line">	xcord2 = []; ycord2 = []</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">		<span class="keyword">if</span> int(labelMat[i]) == <span class="number">1</span>:</span><br><span class="line">			xcord1.append(dataArr[i,<span class="number">1</span>]); ycord1.append(dataArr[i,<span class="number">2</span>])</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			xcord2.append(dataArr[i,<span class="number">1</span>]); ycord2.append(dataArr[i,<span class="number">2</span>])</span><br><span class="line">	fig = plt.figure()</span><br><span class="line">	ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">	ax.scatter(xcord1, ycord1, s = <span class="number">30</span>, c = <span class="string">'red'</span>, marker = <span class="string">'s'</span>)</span><br><span class="line">	ax.scatter(xcord2, ycord2, s = <span class="number">30</span>, c = <span class="string">'green'</span>)</span><br><span class="line">	x = arange(-<span class="number">3.0</span>, <span class="number">3.0</span>, <span class="number">0.1</span>)</span><br><span class="line">	y = (-weights[<span class="number">0</span>]-weights[<span class="number">1</span>]*x)/weights[<span class="number">2</span>]</span><br><span class="line">	ax.plot(x,y)</span><br><span class="line">	plt.xlabel(<span class="string">'x1'</span>); plt.ylabel(<span class="string">'x2'</span>);</span><br><span class="line">	plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#&gt;&gt;&gt; import logRegres</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt; dataArr, labelMat = logRegres.loadDataSet()</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt; weights = logRegres.gradAscent(dataArr, labelMat)</span></span><br><span class="line"><span class="comment">#&gt;&gt;&gt; logRegres.plotBestFit(weights.getA())</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="随机梯度上升"><a href="#随机梯度上升" class="headerlink" title="随机梯度上升"></a>随机梯度上升</h1><ul>
<li><p>gradAscent函数迭代五百次，并且每次计算都要遍历整个数据集，对于大规模数据复杂度过高。改进方法为每次仅用一个样本点来更新回归系数，只在新样本到来时对分类器进行增量式更新，是在线学习算法。流程如下。</p>
<ul>
<li>所有回归系数初始化为1</li>
<li>对数据集中的每个样本：计算该样本的梯度，使用alpha*gradient更新回归系数值</li>
<li>返回回归系数值</li>
</ul>
</li>
<li><p>Code - stocGradAscent0 - logRegres.py，随机上升算法在200次迭代时的系数变化过程在《机器学习实战》82页，其中系数X2经过50次迭代后达到稳定，而系数1和0则需要更多次迭代。并且，在大的波动停止后，还有一些小的周期性波动，这源于数据中存在一些不能正确分类的样本点（数据集非线性可分），在每次迭代时会引发系统的剧烈震荡。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent0</span><span class="params">(dataMatrix, classLabels)</span>:</span></span><br><span class="line">	m, n = shape(dataMatrix)</span><br><span class="line">	weights = ones(n)</span><br><span class="line">	alpha = <span class="number">0.01</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">		h = sigmoid(sum(dataMatrix[i]*weights))</span><br><span class="line">		error = classLabels[i] - h</span><br><span class="line">		weights = weights + alpha * error * dataMatrix[i]</span><br><span class="line">	<span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>
</li>
<li><p>Improve - Code -stocGradAscent1 - logRegres.py，改进后的代码中，alpha每次迭代都会调整，这可以缓解高频波动，并且虽然alpha随着迭代次数减小，但永远不会减小到0（常数项存在），这样保证多次迭代之后新数据仍然对系数有影响。同样，这也避免了alpha的严格下降，避免参数的严格下降也常见于模拟退火算法等其他优化算法中。改进后的代码通过随机选取样本的方式更新回归系数，这样可以减少周期性波动。改进后的代码收敛速度更快，默认迭代次数150。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent1</span><span class="params">(dataMatrix, classLabels, numIter = <span class="number">150</span>)</span>:</span></span><br><span class="line">	m, n = shape(dataMatrix)</span><br><span class="line">	weights = ones(n)</span><br><span class="line">	<span class="keyword">for</span> j <span class="keyword">in</span> range(numIter):</span><br><span class="line">		dataIndex = range(m)</span><br><span class="line">		<span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">			alpha = <span class="number">4</span>/(<span class="number">1.0</span>+j+i)+<span class="number">0.0001</span></span><br><span class="line">			randIndex = int(random.uniform(<span class="number">0</span>, len(dataIndex)))</span><br><span class="line">			h = sigmoid(sum(dataMatrix[randIndex]*weights))</span><br><span class="line">			error = classLabels[randIndex] - h</span><br><span class="line">			weights = weights + alpha * error * dataMatrix[randIndex]</span><br><span class="line">			<span class="keyword">del</span>(dataIndex[randIndex])</span><br><span class="line">	<span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>
</li>
<li><p>改进后的回归系数<img src="http://7xktmz.com1.z0.glb.clouddn.com/logistic%E5%9B%9E%E5%BD%92-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87.png" width="500px"></p>
</li>
</ul>
<h1 id="处理数据中的缺失值"><a href="#处理数据中的缺失值" class="headerlink" title="处理数据中的缺失值"></a>处理数据中的缺失值</h1><ul>
<li>假设有1000个样本和20个特征，若某传感器损坏导致一个特征无效，其余数据仍可用。<ul>
<li>使用可用特征的均值填补缺失值</li>
<li>使用特殊值来填补确实值，如-1</li>
<li>忽略有缺失值的样本</li>
<li>使用相似样本的均值填补缺失值</li>
<li>使用另外的机器学习算法预测缺失值</li>
</ul>
</li>
<li><p>对于Logistic回归，确实只用0代替可以保留现有数据，并且无需对算法进行修改。如果在测试数据集中发现某一条数据的类别标签已经缺失，Logistic回归的简单做法是将该数据丢弃，但如果采用类似kNN的方法则不太可行。</p>
</li>
<li><p>Code 用logistic回归从疝气病症预测病马死亡率 - logRegres.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyVector</span><span class="params">(inX, weights)</span>:</span></span><br><span class="line">	prob = sigmoid(sum(inX*weights))</span><br><span class="line">	<span class="keyword">if</span> prob &gt; <span class="number">0.5</span> : <span class="keyword">return</span> <span class="number">1.0</span></span><br><span class="line">	<span class="keyword">else</span>: <span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">colicTest</span><span class="params">()</span>:</span></span><br><span class="line">	frTrain = open(<span class="string">'horseColicTraining.txt'</span>)</span><br><span class="line">	frTest = open(<span class="string">'horseColicTest.txt'</span>)</span><br><span class="line">	trainingSet = []; trainingLabels = []</span><br><span class="line">	<span class="keyword">for</span> line <span class="keyword">in</span> frTrain.readlines():</span><br><span class="line">		currLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">		lineArr = []</span><br><span class="line">		<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">			lineArr.append(float(currLine[i]))</span><br><span class="line">		trainingSet.append(lineArr)</span><br><span class="line">		trainingLabels.append(float(currLine[<span class="number">21</span>]))</span><br><span class="line">	trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, <span class="number">1000</span>)</span><br><span class="line">	errorCount = <span class="number">0</span>; numTestVec = <span class="number">0.0</span></span><br><span class="line">	<span class="keyword">for</span> line <span class="keyword">in</span> frTest.readlines():</span><br><span class="line">		numTestVec += <span class="number">1.0</span></span><br><span class="line">		currLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">		lineArr = []</span><br><span class="line">		<span class="keyword">for</span> i  <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">			lineArr.append(float(currLine[i]))</span><br><span class="line">		<span class="keyword">if</span> int(classifyVector(array(lineArr), trainWeights)) != int(currLine[<span class="number">21</span>]) :</span><br><span class="line">			errorCount += <span class="number">1</span></span><br><span class="line">	errorRate = (float(errorCount)/numTestVec)</span><br><span class="line">	<span class="keyword">print</span> <span class="string">"the error rate of this test is %f"</span> % errorRate</span><br><span class="line">	<span class="keyword">return</span> errorRate</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiTest</span><span class="params">()</span>:</span></span><br><span class="line">	numTests = <span class="number">10</span>; errorSum = <span class="number">0.0</span></span><br><span class="line">	<span class="keyword">for</span> k <span class="keyword">in</span> range(numTests):</span><br><span class="line">		errorSum += colicTest()</span><br><span class="line">	<span class="keyword">print</span> <span class="string">"after %d iterations the average error rate is: %f"</span> % (numTests, errorSum/float(numTests))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="Matplotlib绘制"><a href="#Matplotlib绘制" class="headerlink" title="Matplotlib绘制"></a>Matplotlib绘制</h1><blockquote>
<p>备份下列几份代码（来自《机器学习实战》的github），大致了解matplotlib绘制的基本方法。</p>
</blockquote>
<ul>
<li><p>绘制等高线 - plotGD.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.cm <span class="keyword">as</span> cm</span><br><span class="line"><span class="keyword">import</span> matplotlib.mlab <span class="keyword">as</span> mlab</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">leafNode = dict(boxstyle=<span class="string">"round4"</span>, fc=<span class="string">"0.8"</span>)</span><br><span class="line">arrow_args = dict(arrowstyle=<span class="string">"&lt;-"</span>)</span><br><span class="line"></span><br><span class="line">matplotlib.rcParams[<span class="string">'xtick.direction'</span>] = <span class="string">'out'</span></span><br><span class="line">matplotlib.rcParams[<span class="string">'ytick.direction'</span>] = <span class="string">'out'</span></span><br><span class="line"></span><br><span class="line">delta = <span class="number">0.025</span></span><br><span class="line">x = np.arange(-<span class="number">2.0</span>, <span class="number">2.0</span>, delta)</span><br><span class="line">y = np.arange(-<span class="number">2.0</span>, <span class="number">2.0</span>, delta)</span><br><span class="line">X, Y = np.meshgrid(x, y)</span><br><span class="line">Z1 = -((X-<span class="number">1</span>)**<span class="number">2</span>)</span><br><span class="line">Z2 = -(Y**<span class="number">2</span>)</span><br><span class="line"><span class="comment">#Z1 = mlab.bivariate_normal(X, Y, 1.0, 1.0, 0.0, 0.0)</span></span><br><span class="line"><span class="comment">#Z2 = mlab.bivariate_normal(X, Y, 1.5, 0.5, 1, 1)</span></span><br><span class="line"><span class="comment"># difference of Gaussians</span></span><br><span class="line">Z = <span class="number">1.0</span> * (Z2 + Z1)+<span class="number">5.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a simple contour plot with labels using default colors.  The</span></span><br><span class="line"><span class="comment"># inline argument to clabel will control whether the labels are draw</span></span><br><span class="line"><span class="comment"># over the line segments of the contour, removing the lines beneath</span></span><br><span class="line"><span class="comment"># the label</span></span><br><span class="line">plt.figure()</span><br><span class="line">CS = plt.contour(X, Y, Z)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=(<span class="number">0.05</span>, <span class="number">0.05</span>),  xycoords=<span class="string">'axes fraction'</span>,</span><br><span class="line">             xytext=(<span class="number">0.2</span>,<span class="number">0.2</span>), textcoords=<span class="string">'axes fraction'</span>,</span><br><span class="line">             va=<span class="string">"center"</span>, ha=<span class="string">"center"</span>, bbox=leafNode, arrowprops=arrow_args )</span><br><span class="line">plt.text(-<span class="number">1.9</span>, -<span class="number">1.8</span>, <span class="string">'P0'</span>)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=(<span class="number">0.2</span>,<span class="number">0.2</span>),  xycoords=<span class="string">'axes fraction'</span>,</span><br><span class="line">             xytext=(<span class="number">0.35</span>,<span class="number">0.3</span>), textcoords=<span class="string">'axes fraction'</span>,</span><br><span class="line">             va=<span class="string">"center"</span>, ha=<span class="string">"center"</span>, bbox=leafNode, arrowprops=arrow_args )</span><br><span class="line">plt.text(-<span class="number">1.35</span>, -<span class="number">1.23</span>, <span class="string">'P1'</span>)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=(<span class="number">0.35</span>,<span class="number">0.3</span>),  xycoords=<span class="string">'axes fraction'</span>,</span><br><span class="line">             xytext=(<span class="number">0.45</span>,<span class="number">0.35</span>), textcoords=<span class="string">'axes fraction'</span>,</span><br><span class="line">             va=<span class="string">"center"</span>, ha=<span class="string">"center"</span>, bbox=leafNode, arrowprops=arrow_args )</span><br><span class="line">plt.text(-<span class="number">0.7</span>, -<span class="number">0.8</span>, <span class="string">'P2'</span>)</span><br><span class="line">plt.text(-<span class="number">0.3</span>, -<span class="number">0.6</span>, <span class="string">'P3'</span>)</span><br><span class="line">plt.clabel(CS, inline=<span class="number">1</span>, fontsize=<span class="number">10</span>)</span><br><span class="line">plt.title(<span class="string">'Gradient Ascent'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'y'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>生成图像如下<img src="http://7xktmz.com1.z0.glb.clouddn.com/Logistic%E5%9B%9E%E5%BD%92-plotGD-%E7%AD%89%E9%AB%98%E7%BA%BF.png" width="500px"></p>
</li>
<li><p>随机梯度上升过程中回归系数的变化 - plotGD.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.patches <span class="keyword">import</span> Rectangle</span><br><span class="line"><span class="keyword">import</span> logRegres</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent0</span><span class="params">(dataMatrix, classLabels)</span>:</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    alpha = <span class="number">0.5</span></span><br><span class="line">    weights = ones(n)   <span class="comment">#initialize to all ones</span></span><br><span class="line">    weightsHistory=zeros((<span class="number">500</span>*m,n))</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            h = logRegres.sigmoid(sum(dataMatrix[i]*weights))</span><br><span class="line">            error = classLabels[i] - h</span><br><span class="line">            weights = weights + alpha * error * dataMatrix[i]</span><br><span class="line">            weightsHistory[j*m + i,:] = weights</span><br><span class="line">    <span class="keyword">return</span> weightsHistory</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent1</span><span class="params">(dataMatrix, classLabels)</span>:</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    alpha = <span class="number">0.4</span></span><br><span class="line">    weights = ones(n)   <span class="comment">#initialize to all ones</span></span><br><span class="line">    weightsHistory=zeros((<span class="number">40</span>*m,n))</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">40</span>):</span><br><span class="line">        dataIndex = range(m)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            alpha = <span class="number">4</span>/(<span class="number">1.0</span>+j+i)+<span class="number">0.01</span></span><br><span class="line">            randIndex = int(random.uniform(<span class="number">0</span>,len(dataIndex)))</span><br><span class="line">            h = logRegres.sigmoid(sum(dataMatrix[randIndex]*weights))</span><br><span class="line">            error = classLabels[randIndex] - h</span><br><span class="line">            <span class="comment">#print error</span></span><br><span class="line">            weights = weights + alpha * error * dataMatrix[randIndex]</span><br><span class="line">            weightsHistory[j*m + i,:] = weights</span><br><span class="line">            <span class="keyword">del</span>(dataIndex[randIndex])</span><br><span class="line">    <span class="keyword">print</span> weights</span><br><span class="line">    <span class="keyword">return</span> weightsHistory</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">dataMat,labelMat=logRegres.loadDataSet()</span><br><span class="line">dataArr = array(dataMat)</span><br><span class="line">myHist = stocGradAscent1(dataArr,labelMat)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">n = shape(dataArr)[<span class="number">0</span>] <span class="comment">#number of points to create</span></span><br><span class="line">xcord1 = []; ycord1 = []</span><br><span class="line">xcord2 = []; ycord2 = []</span><br><span class="line"></span><br><span class="line">markers =[]</span><br><span class="line">colors =[]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">311</span>)    <span class="comment"># take care</span></span><br><span class="line">type1 = ax.plot(myHist[:,<span class="number">0</span>])</span><br><span class="line">plt.ylabel(<span class="string">'X0'</span>)</span><br><span class="line">ax = fig.add_subplot(<span class="number">312</span>)</span><br><span class="line">type1 = ax.plot(myHist[:,<span class="number">1</span>])</span><br><span class="line">plt.ylabel(<span class="string">'X1'</span>)</span><br><span class="line">ax = fig.add_subplot(<span class="number">313</span>)</span><br><span class="line">type1 = ax.plot(myHist[:,<span class="number">2</span>])</span><br><span class="line">plt.xlabel(<span class="string">'iteration'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'X2'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>生成图像如下<img src="http://7xktmz.com1.z0.glb.clouddn.com/Logistic%E5%9B%9E%E5%BD%92-plotSDerror-%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B.png" width="500px"></p>
</li>
<li><p>生成sigmoid函数 - sigmoidPlot.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">t = arange(-<span class="number">60.0</span>, <span class="number">60.3</span>, <span class="number">0.1</span>)</span><br><span class="line">s = <span class="number">1</span>/(<span class="number">1</span> + exp(-t))</span><br><span class="line">ax = subplot(<span class="number">211</span>)</span><br><span class="line">ax.plot(t,s)</span><br><span class="line"><span class="comment">#ax.axis([-5,5,0,1])</span></span><br><span class="line">plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Sigmoid(x)'</span>)</span><br><span class="line">ax = subplot(<span class="number">212</span>)    <span class="comment"># x-y-index</span></span><br><span class="line">ax.plot(t,s)</span><br><span class="line">ax.axis([-<span class="number">60</span>,<span class="number">60</span>,<span class="number">0</span>,<span class="number">1</span>])    <span class="comment"># x-index width</span></span><br><span class="line">plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Sigmoid(x)'</span>)</span><br><span class="line">show()</span><br></pre></td></tr></table></figure>
</li>
<li><p>生成图像如下<img src="http://7xktmz.com1.z0.glb.clouddn.com/logistic%E5%9B%9E%E5%BD%92-sigmoid%E5%87%BD%E6%95%B0.png" width="500px"></p>
</li>
</ul>
<h1 id="Logistic回归总结"><a href="#Logistic回归总结" class="headerlink" title="Logistic回归总结"></a>Logistic回归总结</h1><blockquote>
<p>Logistic回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化算法完成。随机梯度上升算法可以简化梯度上升算法，获取几乎相同的效果，并且占用更少的计算资源，在新数据到来时完成在线更新，而不需要重新读取整个数据集。</p>
</blockquote>
<hr>
<p>参考文献： 《机器学习实战 - 美Peter Harrington》</p>
<p>原创作品，允许转载，转载时无需告知，但请务必以超链接形式标明文章<a href="https://forec.github.io/2016/02/09/machinelearning5/">原始出处</a>(<a href="https://forec.github.io/2016/02/09/machinelearning5/">https://forec.github.io/2016/02/09/machinelearning5/</a>) 、作者信息（<a href="https://forec.github.io/">Forec</a>）和本声明。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://forec.github.io/2016/02/09/machinelearning5/" data-id="civ2lny340020v8ewfnj7v81r" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithms/">Algorithms</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/02/11/machinelearning6/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          机器学习笔记（Chapter 06 - 支持向量机）
        
      </div>
    </a>
  
  
    <a href="/2016/02/04/machinelearning1-4/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">机器学习笔记（Chapter 01-04）</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Code/">Code</a><span class="category-list-count">17</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/DataBase/">DataBase</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hadoop/">Hadoop</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Language/">Language</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/OS/">OS</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Test/">Test</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据/">大数据</a><span class="category-list-count">15</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/计算机理论基础/">计算机理论基础</a><span class="category-list-count">6</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Access/">Access</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithms/">Algorithms</a><span class="tag-list-count">25</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CVM/">CVM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Structures/">Data-Structures</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker/">Docker</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Emacs/">Emacs</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Golang/">Golang</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/">Hadoop</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mistakes/">Mistakes</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/OS/">OS</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Qt/">Qt</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Raspberry/">Raspberry</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/">Spark</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sicp/">sicp</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/函数式编程/">函数式编程</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/图分割/">图分割</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/字符编码/">字符编码</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a><span class="tag-list-count">14</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/线程/">线程</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/计算机组成与体系结构/">计算机组成与体系结构</a><span class="tag-list-count">6</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Access/" style="font-size: 10px;">Access</a> <a href="/tags/Algorithms/" style="font-size: 20px;">Algorithms</a> <a href="/tags/CVM/" style="font-size: 10px;">CVM</a> <a href="/tags/Data-Structures/" style="font-size: 14.29px;">Data-Structures</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Emacs/" style="font-size: 10px;">Emacs</a> <a href="/tags/Golang/" style="font-size: 10px;">Golang</a> <a href="/tags/Hadoop/" style="font-size: 11.43px;">Hadoop</a> <a href="/tags/Mistakes/" style="font-size: 15.71px;">Mistakes</a> <a href="/tags/OS/" style="font-size: 15.71px;">OS</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/Qt/" style="font-size: 10px;">Qt</a> <a href="/tags/Raspberry/" style="font-size: 10px;">Raspberry</a> <a href="/tags/Spark/" style="font-size: 10px;">Spark</a> <a href="/tags/sicp/" style="font-size: 10px;">sicp</a> <a href="/tags/函数式编程/" style="font-size: 12.86px;">函数式编程</a> <a href="/tags/图分割/" style="font-size: 11.43px;">图分割</a> <a href="/tags/字符编码/" style="font-size: 11.43px;">字符编码</a> <a href="/tags/机器学习/" style="font-size: 18.57px;">机器学习</a> <a href="/tags/线程/" style="font-size: 11.43px;">线程</a> <a href="/tags/计算机组成与体系结构/" style="font-size: 17.14px;">计算机组成与体系结构</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">4</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/11/02/cloud-virtual-machine-config/">CVM 操作记录</a>
          </li>
        
          <li>
            <a href="/2016/10/29/raspberry-settings/">Raspberry Pi 3 配置索引</a>
          </li>
        
          <li>
            <a href="/2016/10/08/haskell-fixit/">Fix in Haskell</a>
          </li>
        
          <li>
            <a href="/2016/10/03/co-occurrence-structure-capture/">Network Mining Based On Co-occurrence</a>
          </li>
        
          <li>
            <a href="/2016/09/27/duplicate-cantor/">Cantor Expansion With Duplicate Elements</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 Forec<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>